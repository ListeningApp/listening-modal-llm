You will receive raw text from an OCR scan of a document or web page. Each line of input represents one textbox from the document.

Your task is to label each textbox with exactly one of the following labels:
  title: the primary document title, no more than one per document
  heading: the name of the following chapter or section (should never start in the middle of the previous paragraph)
  subheading: a secondary heading, often following another heading (should never start in the middle of the previous paragraph)
  body: main paragraph content: may start or end in the middle of a word, use context to follow paragraphs that are split across textboxes (if a textbox contains both heading and body text, label it "body")
  math: textbox primarily containing math
  imageDescription: standalone callout that only describes an image, chart, table, or similar (typically begins with "Table...", "Figure...", "Fig. ...", "Above...", "Left...", etc.)
  authors: names of contributors to this document
  institutions: list of universities, business, and other institutions affiliated with this document
  publisher: info about the publisher, or provided by the publisher, including copyright info
  pagenum: the page number
  headerOrFooter: boilerplate text page number, or redundant headings at the top/bottom of the page
  toc: the table of contents
  references: bibliographic references and resources
  acknowledgements: thanks and credits to contributors
  appendix: index, appendix, or any other end matter not part of the main thread (including related headings)
  table: tabular data, label, title, or subheading for a grid or list of data
  datum: numeric data or data label from a chart or list
  advertisement: promotional ads and marketing
  layout: non-content related to the layout of the page, e.g. sidebar navigation
  callout: pull quote or long block text that stands apart from the main text
  footnote: footnotes and endnotes near the bottom of the page
  keywords: list of keywords or tags
  url: web address or email
  other: any other text that does not belong to the main thread

Each line of input is prefixed with a textbox ID in the format `id | Text content`. The output must be in the format `id | label`.

Note that body text may be split across multiple textboxes, and textboxes may start or end in the middle of a sentence or word. Because the text was extracted from a web page or document, paragraphs of body text may be interrupted by multiple textboxes of redundant headers, footnotes, page numbers, tables, images, etc. For example, a real heading will never interrupt the middle of a sentence. Use context clues like this to carefully label each textbox.

EXAMPLE INPUT:
1pjs | Neuroscience Applied 1 (2022) 101012
o2kr | Contents lists available at ScienceDirect
v6sk | Neuroscience Applied
1wj6 | journal homepage: www.journals.elsevier.com/neuroscience-applied
936l | Research Articles
06yq | Microdosing with psychedelics to self-medicate for ADHD symptoms in adults: A naturalistic study
dpv6 | ARTICLE INFO
8r5j | ABSTRACT
2taz | Keywords: ADHD Microdosing Psychedelics Self-medication Well-being Time perception
o3ya | ADHD in adulthood is often overlooked, which negatively affects the individual's well-being. First-line phar- macological interventions are effective in many ADHD patients, relieving symptoms rapidly. However, there seems to be a proportion of individuals who discontinue, or fail to respond to these treatments. For these in- dividuals, alternative treatment options should be explored.
r5il | 1. Introduction
5gui | Attention deficit hyperactivity disorder (ADHD) is one of the most common developmental disorders worldwide. Prevalence research indicates that 2.6% of the adult population has persistent ADHD. ADHD in adults is often overlooked because of the high comorbidity rate and lack of knowledge about how ADHD is expressed in adulthood (Kooij et al.,
9d4o | Fig. 1. Flowchart of included participants for each time point.
82qc | 2019). In addition, ADHD is associated with deficits in various domains of cogni- tive functioning. Twenty-five percent of ADHD cases suffered from ADHD symptoms purely because of de- ficiencies in temporal processing (Sonuga-Barke et al., 2010).
ls7d | First-line ADHD treatments in adults mainly include pharmacological interventions to enhance dopaminergic and noradrenergic neurotrans- mission with stimulants. Overall, they have been proven to work effectively in adults with ADHD, inducing fast symptom relief and thereby enhancing the person's quality of life. In the
5esx | Table 1 Demographic information from respondents at baseline and the two and four- week time points.
t8vv | longer term, approximately twenty percent of ADHD patients discontinue their prescribed medication after six to nine months, thirty percent after one year, and half of them after two years.
xrd1 | 2. Material and methods
5746 | 2.1. Study design and participants
7toz | The study employed a naturalistic design, assessing the experiences of participants at baseline,
j346 | Neuroscience Applied 1 (2022)
m5ka | E.C.H.M. Haijen et al.
kx2c | REVIEW
v8uq | before they start MD on their own initiative, and at two and four weeks after MD initiation. The target population included adults diagnosed with ADHD and individuals who experienced ADHD symptoms to the extent that these interfered with their daily lives and who had not been diag- nosed with ADHD before. To be included in the analyses, participants needed to score above a cut-off point on at least one of the subscales of the Conner's Adult ADHD Rating Scale (CAARS-S:SV).
2zmw | Fig. 2. Mean scores of the CAARS-S:SV DSM-IV total symptoms T-scores at baseline (0W) and two (2W) and four weeks (4W) after MD (A) of the whole sample, and (B) per conventional ADHD medication use. Error bars represent mean error.
sdk3 | <LATEX>\lim _ { x ightarrow \infty } rac { 6 x ^ { 2 } + 1 0 0 } { 7 x ^ { 2 } - 1 0 } =</LATEX>
x972 | 2.2. Study procedure
k221 | Mean performance measures of the CAARS-

EXAMPLE OUTPUT:
1pjs | headerOrFooter
o2kr | publisher
v6sk | publisher
1wj6 | publisher
936l | headerOrFooter
06yq | title
dpv6 | other
8r5j | heading
2taz | keywords
o3ya | body
r5il | heading
5gui | body
9d4o | imageDescription
82qc | body
ls7d | body
5esx | imageDescription
t8vv | body
xrd1 | heading
5746 | subheading
7toz | body
j346 | headerOrFooter
m5ka | authors
kx2c | headerOrFooter
v8uq | body
2zmw | imageDescription
sdk3 | math
x972 | heading
k221 | body




aki1 | The second case is weird and interesting. It's named after Rolf Hagedorn, who in 1964 noticed that this was a possibility in nuclear physics. He considered a model of nuclear matter where the energies <LATEX>E _ { n }</LATEX> approach <LATEX>+ \infty</LATEX> in a roughly logarithmic way. As you heat it, its expected energy keeps increasing, but its temperature can never exceed <LATEX>T _ { \mathrm { H } } .</LATEX> This model turned out to be incorrect, but <LATEX>\mathrm { i t } ^ { \gamma } \varsigma</LATEX> interesting anyway.
vf5h | Now let's solve some puzzles on systems with a countable infinity of states. Some of these show up in quantum mechanics, but you don't need to know quantum mechanics to do these puzzles.
l51p | Puzzle 27. Show that for a system with a countable infinity of states, if thermal equi- librium is possible for some negative temperature, it is impossible for positive or infinite temperatures.
s9zz | Puzzle 28. Work out the Boltzmann distribution when <LATEX>E _ { n } = n E</LATEX> for some energy <LATEX>E ,</LATEX> and show that it is well-defined for all temperatures
x97k | <LATEX>0 < T < + \infty .</LATEX> The next puzzle is a lot like the previous one a bit more messy, but worthwhile because of its great importance in physics.
qwix | Puzzle 29. For a system called the quantum harmonic oscillator of frequency <LATEX>\omega</LATEX> we have <LATEX>E _ { n } = \left( n + \frac { 1 } { 2 } \right) \hbar \omega ,</LATEX> where <LATEX>\hbar</LATEX> is the reduced Planck's constant. Work out the Boltzmann distribution in this case, and show it is well-defined for all temperatures
6lnq | <LATEX>0 < T < + \infty .</LATEX> Puzzle 30. For a system called the primon gas we have <LATEX>E _ { n } = E \ln n</LATEX> for some energy <LATEX>E .</LATEX> Show that the Boltzmann distribution is well-defined for small enough positive temperatures, but there is a Hagedorn temperature. Give a formula for the Boltzmann distribution in terms of the Riemann zeta function:
nxb1 | <LATEX>\zeta \left( s \right) = \sum _ { n = 1 } ^ { \infty } n ^ { - s } .</LATEX> You can show that for the primon gas the sum <LATEX>\sum _ { n = 1 } ^ { \infty } \exp \left( - E _ { n } / k T \right)</LATEX> diverges at the Hagedorn temperature. But it can go the other way, too:
kwpw | Puzzle 31. Find energies <LATEX>E _ { n }</LATEX> with a Hagedorn temperature such that <LATEX>\sum _ { n = 1 } ^ { \infty } \exp \left( - E _ { n } / k T \right)</LATEX> converges at the Hagedorn temperature.
usfa | Various other strange things can happen, as you should expect when dealing with infinite series. For example, it's possible that the Boltzmann distribution is well-defined at some temperature but the expected value of the energy is infinite! But I'll resist the lure of these rabbit holes and turn to something much more important: systems with a continuum of states. We will need to get good at these to compute the entropy of hydrogen. Now our sums become integrals, and various new things happen.
dnjr | 38
qser | THE FINITE VERSUS THE CONTINUOUS
8gam | THE FINITE
z4db | THE CONTINUOUS
lw7l | p a probability distribution on {1, . .. , n} Gibbs entropy Gibbs entropy
2exi | p a probability distribution on R
16py | S(p) =- kpi In pi i=1 n = - k p(x) In p(x) dx
m4ov | S(p) always ≥ 0 S(p) not always ≥ 0
9uqd | S(p) always finite S(p) not always finite
8szx | S(p) invariant under permutations of {1, . . . , n}
iwdc | S(p) not invariant under reparametrizations of R
4vks | You can switch from finite sums to integrals in the definition of entropy, and we'll need to do this to compute the entropy of hydrogen. But be careful: a bunch of things change!
uyaf | We need to switch from finite sums to integrals when we switch from a finite set of states to a measure space of states. I'll illustrate the ideas with the real line, R. We define a probability distribution on R to be an integrable function p: R -> [0, 0) with
d1qh | Such a probability distribution has a Gibbs entropy given by
nyqt | S(p) = - k| 00 00 p(x) ln p(x) dx.
7odd | We can also define Shannon entropy, where we leave out Boltzmann's constant k and use whatever base we want for the logarithm:
0le2 | H (p) = - p(x) log p(x) dx. .00
5dw0 | I should warn you that many writers reserve the term 'Shannon entropy' only for a sum
i4yq | H (p) = - pi log pi. iEX
fj3g | While that convention has advantages, I want to use the term 'Shannon entropy' to signal that I'm leaving out the factor of k.
zuua | Unlike the entropy for a probability distribution on a finite set, the entropy of a probability distribution on R can be negative! This is disturbing. Earlier I said that the Shannon entropy of a probability distribution is the expected amount of information
elwn | 39
jnqv | you learn when an outcome is chosen according to that distribution. How can this be negative?
unp3 | The answer is that this interpretation of entropy, valid for probability distributions on a finite or even a countably infinite set, is not true in the continuous case! We have to adapt our intuitions.
v8vk | Look at an example. Let Pe be the probability distribution on R given by
sr40 | P(x) = 0 otherwise.
5zjj | 1
f56l | ϵ if 0 < x < E
ar2z | For small e it's a tall thin spike near 0. Let's work out its Shannon entropy: I(p) = - _ p(x) log p(x) dx
yad0 | = -1 Jo € € 1 1 ϵ
3g38 | - log - dx
vtbo | = log €.
q8pa | We're just integrating a constant here, so it's easy. When € = 1 the entropy is zero, and when e becomes smaller than 1 the entropy becomes negative!
uc9x | Why? We need a distance scale to define the entropy of a probability distribution on the real line. If I measure distance in centimeters, I'll think the entropy of a probability distribution is bigger than you, who measures it in meters. And if I measure it in kilometers, I'll think the entropy is smaller-and possibly even negative.
zy7o | Let's see how this works. If I measure distance in different units from you, my coordinate y on the real line will not equal your coordinate x: instead we'll have y = cx
kcar | for some c > 0. Then my probability distribution, say q, will have
00aw | q(y) dy = q(cx) d(x) = c q(x) dx -00 00 so we must have
351p | q(cx) = = p(x) 1
zzcb | to make this integral equal 1. In other words, stretching out a probability distribution must also flatten it out, making it less 'tall'-and its entropy increases. Indeed:
mxnx | Puzzle 32. Show that H(q) = H(p) + ln c.
11k8 | Thanks to this formula choosing 0 < c < 1 compresses a probability distribution and makes it taller, reducing its entropy. Inevitably, this can make the entropy negative if c is small enough.
twwy | In summary: in the continuous case, entropy is not invariant under reparametriza- tions: our choice of coordinates matters! And this can make entropy negative. This applies not only to R but many other measure spaces we'll be considering, like R". This issue will be very important.
fyp8 | After learning this, it should be less of a shock that the entropy of a probability distribution on R can be infinite, or even undefined:
z994 | Puzzle 33. Find three probability distributions p on the real line that have entropy +00, -0, and undefined because it's of the form +00 - 00.
vvqn | 40
p6xg | ENTROPY, ENERGY AND TEMPERATURE
bppc | Suppose a system has some measure space X of states with energy E : X -> R. In thermal equilibrium the probability distribution on states, p: X -> R, maximizes the Gibbs entropy S = - k |p(x) In p(x) dx
ilnw | subject to a constraint on the expected value of energy: (E) = p(x) E(x) dx
f3rc | Typically when this happens p is the Boltzmann distribution
d7bx | e-E(x)/kT
gx97 | p(x) =
ppkp | Je-E(2)/KT da
8br6 | where T is the temperature and k is Boltzmann's constant.
de3k | Then as we vary (E) we have
2kb3 | d(E) = TdS
lqcx | We can now generalize a lot of our work from a finite set of states to a general measure space. I won't redo all the arguments, just state the results and point out a couple of caveats.
3h7w | For any measure space X we say a function p: X -> [0, 0) is a probability distri- bution if it's measurable and Jy P(x) dx = 1.
th5p | We can define a version of Shannon entropy for p by
b7nk | H = - | p(x) log p(x) dx,
cbb5 | but physicists mainly use the Gibbs entropy, defined by
5qcf | S = - k _p(x) In p (x ) dx.
8u2c | As I warned you last time, this can take values in [-00, 0], though we are mainly interested in cases when it's finite. If we think of X as the space of states of some system, we can pick any measurable function E : X -> R and call it the 'energy'. Its expected value is then
c1e3 | (E) = |E(x)p(x) dx
7pez | at least when this integral converges.
qs5t | We say the probability distribution p describes thermal equilibrium if it maximizes S subject to a constraint (E) = c. Typically when this happens p is a Boltzmann distribution
en4s | p(x) = 1x e-BE(x) e-BE(x) dx
csb5 | where ß is called the coolness. I say 'typically' because even when X is a finite set, we saw in Puzzle 24 that there can be thermal equilibria that are not Boltzmann distributions, but only limits of Boltzmann distributions as 3 > + or > -00. This can also happen for other measure spaces X. I will not delve into this, because my goal now is to get to some physics.
i096 | As before, we can write B = 1/kT, at least if 8 / 0, and then write the Boltzmann distribution as
tlzt | X -E(x)/kT dx e-E(x)/kT
6iyf | p(x) =
o1nk | Also as before, the Boltzmann distributions obey the crucial relation
mnaf | dH = Bd(E).
0yxe | Rewriting this in terms of Gibbs entropy S = kH and temperature T = 1/kß, it becomes this famous relation between temperature, entropy and the expected energy: TdS = d(E).
erre | Notice that the units match here. The Shannon entropy H is dimensionless, but since k has units of energy/temperature, the Gibbs entropy S = kH has units of energy/temperature. Thus TdS has units of energy, as does d(E).
835g | 42
x3l9 | THE CHANGE IN ENTROPY
d2fj | As we change the temperature of a system from Ti to T2 while keeping it in thermal equilibrium, the change in its entropy is
geyi | S(T1) - S(To) = 1. T0 T1 d(E) T
2181 | where (E) is its expected energy at temperature T.
9ubl | Last time we saw that as we change the expected energy (E) of a system while keeping it in thermal equilibrium, this fundamental relation holds:
o8pj | TdS = d(E).
27b9 | We can rewrite this as
09xa | ds = d(E) T
yi97 | and then integrate this from one temperature to another-remember, as the expected energy changes, so does the temperature. We get
4zj7 | Tı d(E)
3wgh | / T = S(11) - S(To).
uvo3 | This is the main way people do experiments to 'measure entropy'. Slowly heat something up, keeping track of how much energy it takes to increase its temperature each little bit. Using this data you can approximately calculate the integral at left and that gives the change in entropy!
yjbv | But so far we're just measuring changes in entropy. How can you figure out the actual value of the entropy? One way is to assume the Third Law of Thermodynamics, which says that in thermal equilibrium the entropy approaches zero as the temperature approaches zero from above. This gives
l6it | Tı d(E) F = S(T)
h8x1 | This is how people often 'measure the entropy' of a system in thermal equibrium. They heat it up starting from absolute zero, very slowly so they hope it is close to thermal equilibrium at every moment-and they take data on how much energy is used, and approximately calculate the integral at left!
6qtq | But this relies on the Third Law of Thermodynamics. So where does that come from?
wm3n | 43
1anr | THE THIRD LAW OF THERMODYNAMICS
600w | If a system has countably many states, with just one of lowest energy, and thermal equilibrium is possible for this system for some temperature T > 0, then its entropy in thermal equilibrium approaches zero as T approaches zero from above:
60w7 | lim S(T) = 0 T-+0+
1a5g | Some people say the Third Law of Thermodynamics this way: "entropy is zero at absolute zero". But it's not really that simple indeed, other people say it's impossible to reach absolute zero. Above I've stated a version of the Third Law that's actually a theorem. Let's prove it!
0oof | Actually, let's prove it now for systems with only finitely many states. It'll be easier to handle systems with countably infinite number of states later, when we've developed more tools. And by the way, we'll see the Third Law isn't always true for systems with a continuum of states. It will fail for all three of the problems on our big to-do list: the classical harmonic oscillator, the classical particle in a box and the classical ideal gas. This is often taken as a failure of classical mechanics, since switching to quantum mechanics makes the Third Law hold for these systems.
p930 | Let's show that for a system with finitely many states i = 1, ... , n with energies Ei, as the temperature T approaches zero from above, the entropy of the system in thermal equilibrium approaches k ln N where N is the number of lowest-energy states. In thermal equilibrium
xhc7 | Pi xe -Ei/kT.
6a60 | Thus, all states with the lowest energy have the same probability, while as the temper- ature approaches zero from above, any higher-energy states have pi -> 0. So, as the temperature approaches zero from above, the probability of the system being in any one of its N lowest-energy states approaches 1/N, and we get
dfgj | 1 T->0+ lim S(T) = lim -kp; Inpi = - k T->0+ i=1 n i=1 N N 1
6ptp | ln N = kln N.
qjlj | In particular, if the system has just one lowest-energy state, we get the Third Law of Thermodynamics:
kva7 | lim S(T) = 0.
ssp0 | Here T -> 0+ means that T is approaching zero from above.