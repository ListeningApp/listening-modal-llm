{
  "id": "chatcmpl-A3U6K6zS351NCp5uLty1519GoK3fr",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "``` \n6pmo | We propose three predictive approaches towards optimally setting model size and training duration, based on the outcome of over four hundred training runs. All three approaches predict that Gopher is substantially oversized and estimate that for the same compute budget a smaller model trained on more data will perform better. We directly test this hypothesis by training Chinchilla, a seventy billion parameter model, and show that it outperforms Gopher and even larger models on nearly every measured evaluation task.\n195r | Whilst our method allows us to make predictions on how to scale large models when given additional compute, there are several limitations. Due to the cost of training large models, we only have two comparable training runs at large scale (Chinchilla and Gopher), and we do not have additional tests at intermediate scales. Furthermore, we assume that the efficient computational frontier can be described by a power-law relationship between the compute budget, model size, and number of training tokens. However, we observe some concavity in log at high compute budgets. This suggests that we may still be overestimating the optimal size of large models. Finally, the training runs for our analysis have all been trained on less than an epoch of data; future work may consider the multiple epoch regime. Despite these limitations, the comparison of Chinchilla to Gopher validates our performance predictions, that have thus enabled training a better (and more\nkqmi | lightweight) model at the same compute budget.\ng8rd | Though there has been significant recent work allowing larger and larger models to be trained, our analysis suggests an increased focus on dataset scaling is needed. Speculatively, we expect that scaling to larger and larger datasets is only beneficial when the data is high-quality. This calls for responsibly collecting larger datasets with a high focus on dataset quality. Larger datasets will require extra care to ensure train-test set overlap is properly accounted for, both in the language modelling loss but also with downstream tasks. Finally, training for trillions of tokens introduces many ethical and privacy concerns. Large datasets scraped from the web will contain toxic language, biases, and private information. With even larger datasets being used, the quantity (if not the frequency) of such information increases, which makes dataset introspection all the more important. Chinchilla does suffer from bias and toxicity but interestingly it seems less affected than Gopher. Better understanding how performance of large language models and toxicity interact is an important future research question.\nnns9 | While we have applied our methodology towards the training of auto-regressive language models, we expect that there is a similar trade-off between model size and the amount of data in other modalities. As training large models is very expensive, choosing the optimal model size and training steps beforehand is essential. The methods we propose are easy to reproduce in new settings.\n9uwy | Six. Acknowledgements\nsrly | A. Training dataset\ndl6i | In Table A one we show the training dataset makeup used for Chinchilla and all scaling runs. Note that both the MassiveWeb and Wikipedia subsets are both used for more than one epoch.\n89ub | One key assumption is made on the cosine cycle length and the corresponding learning rate drop. We find that setting the cosine cycle length too much longer than the target number of training steps results in sub-optimally trained models, as shown in Figure A one. As a result, we assume that an optimally trained model will have the cosine cycle length correctly calibrated to the maximum number of steps, given the FLOP budget; we follow this rule in our main analysis.\ndhle | We show scaling results from an IsoFLOP (Approach two) analysis after training on two different datasets: C four and GitHub code, results are shown in Table A two. For both set of experiments using subsets of MassiveText, we use the same tokenizer as the MassiveText experiments.\n2nib | We find that the scaling behaviour on these datasets is very similar to what we found on MassiveText, as shown in Figure A two and Table A two. This suggests that our results are independent of the dataset as long as one does not train for more than one epoch.\nnaev | D point one. Approach one: Fixing model sizes and varying training sequences\n7h0h | We use a maximum learning rate of two times ten to the negative four for the smallest models and one point two five times ten to the negative four for the largest models. In all cases, the learning rate drops by a factor of ten times during training, using a cosine schedule. We make the assumption that the cosine cycle length should be approximately matched to the number of training steps. We find that when the cosine cycle overshoots the number of training steps by more than twenty-five percent, performance is noticeably degraded-see Figure A one. We use Gaussian smoothing with a window length of ten steps to smooth the training curve.\na64c | D point two. Approach three: Parametric fitting of the loss\ngi7n | In this section, we first show how Equation two can be derived. We repeat the equation below for clarity,\n3pn1 | Î(N,D) ^ E + E+ M + B (five)\nwjuz | based on a decomposition of the expected risk between a function approximation term and an optimisation suboptimality term. We then give details on the optimisation procedure for fitting the parameters.\n3yxv | Loss decomposition. Formally, we consider the task of predicting the next token Y based on the previous tokens in a sequence X, with S varying from zero to S max the maximum sequence length. We consider a distribution P and D of tokens in Y and their past in X. A predictor F X to D(Y) computes the probability of each token given the past sequence. The Bayes classifier, F, minimizes the cross-entropy of F of X with the observed tokens Y, with expectation taken on the whole data distribution. We let L be the expected risk\nd8zj | L(F) E log F of X Y, and set F == argmin L(F).\nywxn | The set of all transformers of size N, that we denote HN, forms a subset of all functions that map sequences to distributions of tokens X to D(Y). Fitting a transformer of size N on the expected risk L(F) amounts to minimizing such risk on a restricted functional space\nuxhs | FN == argmin L(F).\ned6e | When we observe a dataset of size D, we do not have access to EP, but instead to the empirical expectation EP over the empirical distribution P. What happens when we are given D\n6qqm | datapoints that we can only see once, and when we constrain the size of the hypothesis space to be N-dimensional ? We are making steps toward minimizing the empirical risk within a finite-dimensional functional space HN:\nonum | ÎD(F)ªEP log F of X Y, setting ÎN, D == argmin LD(F).\n3iir | (eight)\n7a0a | We are never able to obtain FW, D as we typically perform a single epoch over the dataset of size D. Instead, be obtain FW, D, which is the result of applying a certain number of gradient steps based on the D datapoints the number of steps to perform depends on the gradient batch size, for which we use well-tested heuristics.\no1qp | Using the Bayes classifier F, the expected risk minimizer FW and the single epoch empirical risk minimizer FN, D, we can finally decompose the loss L(N, D) into\njrik | L(N, D) AL(JN, D) = L(F) plus L(FN) minus L(F) plus L(FN, D) minus L(FN).\n```",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725394056,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 1587,
    "prompt_tokens": 3374,
    "total_tokens": 4961
  }
}