You will receive raw text from an OCR scan of a document or web page. Each line of input represents one textbox from the document.

Your task is to label each textbox with exactly one of the following labels:
  title: the primary document title, no more than one per document
  heading: the name of the following chapter or section (should never start in the middle of the previous paragraph)
  subheading: a secondary heading, often following another heading (should never start in the middle of the previous paragraph)
  body: main paragraph content: may start or end in the middle of a word, use context to follow paragraphs that are split across textboxes (if a textbox contains both heading and body text, label it "body")
  math: textbox primarily containing math
  imageDescription: standalone callout that only describes an image, chart, table, or similar (typically begins with "Table...", "Figure...", "Fig. ...", "Above...", "Left...", etc.)
  authors: names of contributors to this document
  institutions: list of universities, business, and other institutions affiliated with this document
  publisher: info about the publisher, or provided by the publisher, including copyright info
  pagenum: the page number
  headerOrFooter: boilerplate text page number, or redundant headings at the top/bottom of the page
  toc: the table of contents
  references: bibliographic references and resources
  acknowledgements: thanks and credits to contributors
  appendix: index, appendix, or any other end matter not part of the main thread (including related headings)
  table: tabular data, label, title, or subheading for a grid or list of data
  datum: numeric data or data label from a chart or list
  advertisement: promotional ads and marketing
  layout: non-content related to the layout of the page, e.g. sidebar navigation
  callout: pull quote or long block text that stands apart from the main text
  footnote: footnotes and endnotes near the bottom of the page
  keywords: list of keywords or tags
  url: web address or email
  other: any other text that does not belong to the main thread

Each line of input is prefixed with a textbox ID in the format `id | Text content`. The output must be in the format `id | label`.

Note that body text may be split across multiple textboxes, and textboxes may start or end in the middle of a sentence or word. Because the text was extracted from a web page or document, paragraphs of body text may be interrupted by multiple textboxes of redundant headers, footnotes, page numbers, tables, images, etc. For example, a real heading will never interrupt the middle of a sentence. Use context clues like this to carefully label each textbox.

EXAMPLE INPUT:
1pjs | Neuroscience Applied 1 (2022) 101012
o2kr | Contents lists available at ScienceDirect
v6sk | Neuroscience Applied
1wj6 | journal homepage: www.journals.elsevier.com/neuroscience-applied
936l | Research Articles
06yq | Microdosing with psychedelics to self-medicate for ADHD symptoms in adults: A naturalistic study
dpv6 | ARTICLE INFO
8r5j | ABSTRACT
2taz | Keywords: ADHD Microdosing Psychedelics Self-medication Well-being Time perception
o3ya | ADHD in adulthood is often overlooked, which negatively affects the individual's well-being. First-line phar- macological interventions are effective in many ADHD patients, relieving symptoms rapidly. However, there seems to be a proportion of individuals who discontinue, or fail to respond to these treatments. For these in- dividuals, alternative treatment options should be explored.
r5il | 1. Introduction
5gui | Attention deficit hyperactivity disorder (ADHD) is one of the most common developmental disorders worldwide. Prevalence research indicates that 2.6% of the adult population has persistent ADHD. ADHD in adults is often overlooked because of the high comorbidity rate and lack of knowledge about how ADHD is expressed in adulthood (Kooij et al.,
9d4o | Fig. 1. Flowchart of included participants for each time point.
82qc | 2019). In addition, ADHD is associated with deficits in various domains of cogni- tive functioning. Twenty-five percent of ADHD cases suffered from ADHD symptoms purely because of de- ficiencies in temporal processing (Sonuga-Barke et al., 2010).
ls7d | First-line ADHD treatments in adults mainly include pharmacological interventions to enhance dopaminergic and noradrenergic neurotrans- mission with stimulants. Overall, they have been proven to work effectively in adults with ADHD, inducing fast symptom relief and thereby enhancing the person's quality of life. In the
5esx | Table 1 Demographic information from respondents at baseline and the two and four- week time points.
t8vv | longer term, approximately twenty percent of ADHD patients discontinue their prescribed medication after six to nine months, thirty percent after one year, and half of them after two years.
xrd1 | 2. Material and methods
5746 | 2.1. Study design and participants
7toz | The study employed a naturalistic design, assessing the experiences of participants at baseline,
j346 | Neuroscience Applied 1 (2022)
m5ka | E.C.H.M. Haijen et al.
kx2c | REVIEW
v8uq | before they start MD on their own initiative, and at two and four weeks after MD initiation. The target population included adults diagnosed with ADHD and individuals who experienced ADHD symptoms to the extent that these interfered with their daily lives and who had not been diag- nosed with ADHD before. To be included in the analyses, participants needed to score above a cut-off point on at least one of the subscales of the Conner's Adult ADHD Rating Scale (CAARS-S:SV).
2zmw | Fig. 2. Mean scores of the CAARS-S:SV DSM-IV total symptoms T-scores at baseline (0W) and two (2W) and four weeks (4W) after MD (A) of the whole sample, and (B) per conventional ADHD medication use. Error bars represent mean error.
sdk3 | <LATEX>\lim _ { x ightarrow \infty } rac { 6 x ^ { 2 } + 1 0 0 } { 7 x ^ { 2 } - 1 0 } =</LATEX>
x972 | 2.2. Study procedure
k221 | Mean performance measures of the CAARS-

EXAMPLE OUTPUT:
1pjs | headerOrFooter
o2kr | publisher
v6sk | publisher
1wj6 | publisher
936l | headerOrFooter
06yq | title
dpv6 | other
8r5j | heading
2taz | keywords
o3ya | body
r5il | heading
5gui | body
9d4o | imageDescription
82qc | body
ls7d | body
5esx | imageDescription
t8vv | body
xrd1 | heading
5746 | subheading
7toz | body
j346 | headerOrFooter
m5ka | authors
kx2c | headerOrFooter
v8uq | body
2zmw | imageDescription
sdk3 | math
x972 | heading
k221 | body




1pjs | LLMs, Turing Tests and Chinese Rooms: the prospects for meaning in Large Language Models
o2kr | Discussions of artificial intelligence and computational systems in philosophy of mind (and elsewhere) have been shaped to a large extent by two brilliant and highly influential thought- experiments: Alan Turing's Imitation Test for thinking systems and John Searle's Chinese Room Argument (these two thought experiments are universally well-known, but just for sake of completeness I'll sketch them very briefly in §1). As is extremely well-known, Turing argued for a behavioural test for intelligence. Searle, on the other hand, argued that passing a purely behavioural test, focused at the level of inputs and outputs, could never guarantee that meaning, understanding, or thinking was occurring in a system, since a purely formal, syntactic capacity could underpin the behaviour. In many ways, recent debates about large language models (LLMs) are struggling to move beyond the divide marked by these original, opposing thought- experiments. Thanks to the open access status of some large language models (systems like ChatGPT and Bard), we all now have, available on any suitable laptop or phone, an extraordinarily powerful example of a system which (at least within certain parameters) is capable of passing a Turing Test. Yet those who feel swayed by Searle's Chinese Room are likely to remain unconvinced that the incredible behavioural prowess displayed by these systems is sufficient to show that LLMs deserve the attribution of any richer predicates.
v6sk | One way to move this debate forward, stressed in several places already (xxx), is to 'look under the hood' of AI systems, asking exactly what (if anything) the elements they manipulate represent and what features are relevant to the computations they perform. While this is certainly an important strategy, and I'll touch on it below, in this paper I want to pursue a different route, exploring the features those sceptical about LLMs might take to ground meaning. In §1 I set out some common ground between Believers and Sceptics, concerning how LLMs work and the kind of features a Sceptic is likely to demand from an LLM before she would be willing to treat it as a semantic engine. In §§2-3, I explore the first potential ground of meaning, which requires a robust relation between lingsuistic signs and external objects or states of affairs. In section 3 I argue that concerns about worldly connections can be met, so that the outputs of LLMs should be viewed as genuinely meaningful. Then in (§4) I turn to the idea that LLMs require original intentionality before we admit them to the space of meaning. I suggest that this demand is not a prerequisite for meaning per se, but rather for agency or conscious understanding. I'll suggest
1wj6 | that the demands for original intentionality are not met by LLMs, but that nor should we want them to be.
936l | Turing Tests, Chinese Rooms and LLMs
06yq | Ideas generated in philosophy of mind usually remain within the confines of the ivory tower where they were born, but this is certainly not true of Turing Tests and possibly not true of Chinese Rooms. Still, just to ensure everything is on the table, in this section I'll recap very briefly on the two thought-experiments which have done so much to shape understanding in this field.
dpv6 | Alan Turing proposed what he called 'the imitation game' in his 1950 paper 'Computing machinery and intelligence' as an operationalised method to replace the question "Can machines think?". Turing's starting point was the recognition that terms like "thinking" and "intelligence" are difficult to define and are prone to yield divergent interpretations in different contexts. Instead, then, he proposed replacing questions framed in these vague terms with an investigation into a much more tractable question: "Can a machine pass the imitation test?". In the imitation test, a human subject poses questions to an unknown target, via text, and receives answers in the same way. If, given a reasonable length of conversation, the human subject is unable to tell whether they are interacting with another human or with a machine, then, if they are in fact interacting with a machine, that machine is judged to pass the imitation test. While Turing doesn't in fact say very much about the repercussions of passing such a test, the suggestion seems to be that, for all intents and purposes, a successful machine would qualify as a thinking thing. The only other option, Turing seems to think, would be adopting some kind of solipsistic condition on thinking, requiring first-personal conscious awareness, so that we judge thinking to be going on only when we 'know it from the inside'. However, as Turing 1950: ?? points out, this solipsistic perspective is not the one we adopt in everyday life, instead he notes that "it is usual to have the polite convention that everyone thinks". A similar courtesy, it seems, should be extended to behaviourally adept machines.
8r5j | In Searle's famous thought experiment from 1980, however, behavioural success of the kind envisaged by Turing is held to be insufficient for attribution of rich predicates like thinking or meaning to a system. For Searle objects that success at the level of outputs could come from a system which did nothing more than manipulate symbols in terms of their formal, syntactic properties, with no consideration of semantic content, or meaning, occurring within the system.
2taz | To see this, he asks us to consider a sealed room with a human subject in it. The subject receives what appear to her to be meaningless marks via an input slot. She then uses a giant look-up table to pair the incoming marks with a further set of marks and she outputs this further set of apparently meaningless marks via an output slot. Unbeknownst to the subject, however, the incoming symbols are questions in Chinese and the series of marks which she outputs constitute answers to these questions (again, in Chinese).1 Searle asks us to consider whether the individual in the room knows or understands Chinese and the intuitive answer, despite her impressive performance with Chinese questions, is that she does not. Yet, Searle contends, as for the individual in the Chinese Room, so for the computer. Both systems are purely syntactic engines with no semantic properties in play.
o3ya | So, what are we to say of LLMs? All parties agree that Large Language Models are trained for the task of minimizing prediction errors when deciding on the most probable next word in a sequence. Crudely, faced with the sentence "Emily paid in her cheques at the _", the task of an LLM is to fill in the blank with the most probable next word, e.g. completing this sentence with the expression "bank".2 In order to carry out this task, LLMs utilise what is known as transformer architecture (Vaswani et al 2017), which allows expressions of a natural language to be mapped into vectors within a multidimensional vector space. During pre-training, the system is provided with an almost unimaginably huge dataset (hundreds of billions to trillions of token words) which it uses to learn the statistical relationships between linguistic items in the database. The system does this by repeatedly masking words in sentences within its database and 'making guesses' (initially entirely randomly) about what the masked word is. Once the correct word is revealed the system then recalibrates the vectors it assigns to words in light of this information. The vector assigned to each word is thus determined by the statistical correlations between expressions across the entire training data. Following this unsupervised learning, there is then a period of human feedback which helps the system fine-tune its guesses for new input strings and then recalibrate resulting vector weights. Finally, vectors are also moderated by the correlations in the input strings themselves, so that, e.g., the vector assigned to a token of "dog" will differ dependent on whether the input string to the LLM is "The big dog ... " or "The barking dog ... ", etc. (see Jurafsky and Martin 2023 for discussion).
r5il | While the details get murky quite quickly (with part of the problem being that even computer scientists are unsure about exactly how to understand the internal states of LLMs), it seems that neither the input nor the design of LLMs directly target semantic content. The properties that LLMs are trained to recognise are simply statistical occurrence properties. Furthermore, the training data they receive is raw and unencoded (i.e. it has not been labelled in advance by humans using categories like "noun" or "verb", nor does it contain axioms that could plausibly be taken as assigning meaning to basic symbols, as in classical computational systems). The processing behind LLM outputs thus seems to bypass semantic properties. Yet the surprising result of this kind of processing is that it results in behaviour which gives the very strong impression of linguistic understanding. Try interacting with ChatGPT and its fellow travellers and it is hard, sometimes almost impossible, to resist the belief that one is conversing with another person, a thinking thing (the infamous case of Google engineer Blake Lemoine being a stark case in point).3 As [Sparks p. 8] suggest of GPT-4:
5gui | [Its] primary strength is its unparalleled mastery of natural language. It can not only generate fluent and coherent text, but also understand and manipulate it in various ways, such as summarizing, translating, or answering an extremely broad set of questions. Moreover, by translating we mean not only between different natural languages but also translations in tone and style, as well as across domains such as medicine, law, accounting, computer programming, music, and more.
82qc | Furthermore, this exemplary performance means that the role LLM outputs play in our lives is increasingly akin to those of an ordinary conversational partner: we take the systems to convey information, to answer questions, and to advance our understanding, in ways that are often indistinguishable from our use of the testimony of other humans.
ls7d | From the perspective of behavioural tests, then, it is very easy to believe that LLMs should be attributed rich, semantic properties: LLMs look like they respond to and manipulate genuine content, generating meaningful outputs through their understanding of language. In what follows, I'll label those who hold that LLMs deserve the attribution of rich predicates (like representing, meaning, and perhaps even understanding and thinking) 'Believers'. At the most extreme end of Believers are those who hold that by tracking and manipulating facts about the statistical occurrences of words these kinds of system develop "a form of general intelligence, indeed showing sparks of artificial general intelligence ... demonstrated by [GPT-4's] core mental capabilities (such as reasoning, creativity, and deduction), its range of topics on which it has
tzt3 | gained expertise (such as literature, medicine, and coding), and the variety of tasks it is able to perform (e.g., playing games, using tools, explaining itself)." [Sparks, p.91]
t8vv | 'Sceptics', on the other hand, insist that LLMs are gaming Turing's test - passing the behavioural criteria but by the use of strategies that do not, in fact, warrant ascription of richer properties (as Titus 2024: 101174 claims LLMs "do not themselves function to represent or produce meaningful text"). For LLMs to gain admittance to the space of meaning, we need to be sure that they are sensitive to the meaning properties of the signs they manipulate, providing answers which are driven by a grasp of semantic content not merely through sensitivity to common word distributions. Yet, Sceptics point out, LLMs lack the sorts of features we might prima facie expect are required for the emergence of genuine sensitivity to meaning: first, they are not embedded in the non-linguistic environment (they can't see, hear or touch things, and they can't manipulate objects in the real or virtual world), and, second, they lack the kind of long- term goals and aims which could drive their interactions with the world and with us. Furthermore, their responses lack the sort of consistency and stability indicative of having a point of view or perspective. As [Sparks, p.60] notes:
qvnu | The ability to explain one's own behavior is an important aspect of intelligence, as it allows for a system to communicate with humans and other agents. Self explanation is not only a form of communication, but also a form of reasoning, requiring a good theory of mind for both yourself (the explainer) and the listener. For GPT-4, this is complicated by the fact that it does not have a single or fixed "self" that persists across different executions (in contrast to humans). Rather, as a language model, GPT-4 simulates some process given the preceding input, and can produce vastly different outputs depending on the topic, details, and even formatting of the input.
kggw | Yet the ability to act in the world, in a consistent and principled way reflective of long-term goals and needs, seems central to our conception of agency and it is very tempting to think that agency is a prerequisite for getting meaning off the ground. All this leads to a suspicion that LLMs are nothing more than "stochastic parrots" (to borrow the now well-known phrase of computational linguist Emily Bender and colleagues, 2021) - that they are probability-based mechanisms which just parrot empty words back to us.
xrd1 | So, who is right here? Perhaps predictably, I'm going to argue that there is right on both sides: Believers are right to think that LLMs are special, that their outputs are meaningful, and perhaps even that the systems succeed in representing and manipulating genuinely semantic properties, even though they are trained on an entirely non-semantic task. Sceptics, however, are right to think that meaning in these systems is derivative - LLM outputs mean things because we mean things when we use language. For a system to have non-derivative meaning I suggest it
5746 | would have to qualify as an agent (having, in Searle's terms, original intentionality). LLMs don't have that, but neither, I conclude, should we want them to. For creating systems which met this demand would risk drawing artificial systems into the space of moral concern. To see this, in the next two sections I consider LLMs connection with the world, then in $4 I turn to the issue of original intentionality.
7toz | 2. Semantic content requires contact with things beyond the text
j346 | Perhaps the most obvious objection to the claim that LLMs deserve the attribution of rich predicates like meaning and representing is that such systems lack the kind of connections to the external world which seem necessary to ground semantic content. As Piantadosi and Hill 2022 note, there is a "prominent view ... that models trained only on text cannot acquire realistic meanings because they lack reference, or connection to objects in the real world" (see, e.g, Bender and Koller 2020. Lake & Murphy 2023: 411, although they argue against externalist semantics, agree that nothing in the vector assigned to a word in an LLM would allow a user to label an object or describe a scene). The assumption here is that words get their meaning from their connection to things outside the sphere of language, so that, e.g. "dog" means what it does because it refers to dogs or picks out the property of doghood (Fodor xxxx). While sentences and utterances are meaningful because they make claims about the world, so that their meaning can be analysed in terms of their conditions of truth (Davidson, Partee). As David Lewis famously put it (1970: 18), it seems that: "semantics with no treatment of truth conditions is not semantics". Yet, if all a system has available to it is intra-linguistic information, we might suspect that truth won't enter the picture, since the system is out of touch with the external states of affairs where (according to most theories) truth gets its footing.4