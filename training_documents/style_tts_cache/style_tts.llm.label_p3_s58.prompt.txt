You will receive raw text from an OCR scan of a document or web page. Each line of input represents one textbox from the document.

Your task is to label each textbox with exactly one of the following labels:
  title: the primary document title, no more than one per document
  heading: the name of the following chapter or section (should never start in the middle of the previous paragraph)
  subheading: a secondary heading, often following another heading (should never start in the middle of the previous paragraph)
  body: main paragraph content: may start or end in the middle of a word, use context to follow paragraphs that are split across textboxes (if a textbox contains both heading and body text, label it "body")
  math: textbox primarily containing math
  imageDescription: standalone callout that only describes an image, chart, table, or similar (typically begins with "Table...", "Figure...", "Fig. ...", "Above...", "Left...", etc.)
  authors: names of contributors to this document
  institutions: list of universities, business, and other institutions affiliated with this document
  publisher: info about the publisher, or provided by the publisher, including copyright info
  pagenum: the page number
  headerOrFooter: boilerplate text page number, or redundant headings at the top/bottom of the page
  toc: the table of contents
  references: bibliographic references and resources
  acknowledgements: thanks and credits to contributors
  appendix: index, appendix, or any other end matter not part of the main thread (including related headings)
  table: tabular data, label, title, or subheading for a grid or list of data
  datum: numeric data or data label from a chart or list
  advertisement: promotional ads and marketing
  layout: non-content related to the layout of the page, e.g. sidebar navigation
  callout: pull quote or long block text that stands apart from the main text
  footnote: footnotes and endnotes near the bottom of the page
  keywords: list of keywords or tags
  url: web address or email
  other: any other text that does not belong to the main thread

Each line of input is prefixed with a textbox ID in the format `id | Text content`. The output must be in the format `id | label`.

Note that body text may be split across multiple textboxes, and textboxes may start or end in the middle of a sentence or word. Because the text was extracted from a web page or document, paragraphs of body text may be interrupted by multiple textboxes of redundant headers, footnotes, page numbers, tables, images, etc. For example, a real heading will never interrupt the middle of a sentence. Use context clues like this to carefully label each textbox.

EXAMPLE INPUT:
```
1pjs | Neuroscience Applied 1 (2022) 101012
o2kr | Contents lists available at ScienceDirect
v6sk | Neuroscience Applied
1wj6 | journal homepage: www.journals.elsevier.com/neuroscience-applied
936l | Research Articles
06yq | Microdosing with psychedelics to self-medicate for ADHD symptoms in adults: A naturalistic study
dpv6 | ARTICLE INFO
8r5j | ABSTRACT
2taz | Keywords: ADHD Microdosing Psychedelics Self-medication Well-being Time perception
o3ya | ADHD in adulthood is often overlooked, which negatively affects the individual's well-being. First-line phar- macological interventions are effective in many ADHD patients, relieving symptoms rapidly. However, there seems to be a proportion of individuals who discontinue, or fail to respond to these treatments. For these in- dividuals, alternative treatment options should be explored.
r5il | 1. Introduction
5gui | Attention deficit hyperactivity disorder (ADHD) is one of the most common developmental disorders worldwide. Prevalence research indicates that 2.6% of the adult population has persistent ADHD. ADHD in adults is often overlooked because of the high comorbidity rate and lack of knowledge about how ADHD is expressed in adulthood (Kooij et al.,
9d4o | Fig. 1. Flowchart of included participants for each time point.
82qc | 2019). In addition, ADHD is associated with deficits in various domains of cogni- tive functioning. Twenty-five percent of ADHD cases suffered from ADHD symptoms purely because of de- ficiencies in temporal processing (Sonuga-Barke et al., 2010).
ls7d | First-line ADHD treatments in adults mainly include pharmacological interventions to enhance dopaminergic and noradrenergic neurotrans- mission with stimulants. Overall, they have been proven to work effectively in adults with ADHD, inducing fast symptom relief and thereby enhancing the person's quality of life. In the
5esx | Table 1 Demographic information from respondents at baseline and the two and four- week time points.
t8vv | longer term, approximately twenty percent of ADHD patients discontinue their prescribed medication after six to nine months, thirty percent after one year, and half of them after two years.
xrd1 | 2. Material and methods
5746 | 2.1. Study design and participants
7toz | The study employed a naturalistic design, assessing the experiences of participants at baseline,
j346 | Neuroscience Applied 1 (2022)
m5ka | E.C.H.M. Haijen et al.
kx2c | REVIEW
v8uq | before they start MD on their own initiative, and at two and four weeks after MD initiation. The target population included adults diagnosed with ADHD and individuals who experienced ADHD symptoms to the extent that these interfered with their daily lives and who had not been diag- nosed with ADHD before. To be included in the analyses, participants needed to score above a cut-off point on at least one of the subscales of the Conner's Adult ADHD Rating Scale (CAARS-S:SV).
2zmw | Fig. 2. Mean scores of the CAARS-S:SV DSM-IV total symptoms T-scores at baseline (0W) and two (2W) and four weeks (4W) after MD (A) of the whole sample, and (B) per conventional ADHD medication use. Error bars represent mean error.
sdk3 | <LATEX>\lim _ { x ightarrow \infty } rac { 6 x ^ { 2 } + 1 0 0 } { 7 x ^ { 2 } - 1 0 } =</LATEX>
x972 | 2.2. Study procedure
k221 | Mean performance measures of the CAARS-
```

EXAMPLE OUTPUT:
```
1pjs | headerOrFooter
o2kr | publisher
v6sk | publisher
1wj6 | publisher
936l | headerOrFooter
06yq | title
dpv6 | other
8r5j | heading
2taz | keywords
o3ya | body
r5il | heading
5gui | body
9d4o | imageDescription
82qc | body
ls7d | body
5esx | imageDescription
t8vv | body
xrd1 | heading
5746 | subheading
7toz | body
j346 | headerOrFooter
m5ka | authors
kx2c | headerOrFooter
v8uq | body
2zmw | imageDescription
sdk3 | math
x972 | heading
k221 | body
```

INPUT:
```
iwn5 | (3)
uooz | where N is the number of phonemes in t, ti is the i-th phoneme token of t, ti is the i-th predicted phoneme token, and CE(.) is the cross-entropy loss function.
pxbq | Since this alignment is not necessarily monotonic, we use a simple L-1 loss Lmono that forces the soft attention alignment to be close to its non-differentiable monotonic version:
jfsr | Lmono = Ex,t [|@align - @hard|1] , (4) where aalign = A(x, t) is the attention alignment and @hard is the monotonic hard alignment obtained through dynamic programming algorithms (see Appendix A-A for details).
u15r | (b) Duration and prosody prediction
lq35 | Adversarial objectives. We employ two adversarial loss functions, the original cross-entropy loss function Lady for adversarial training and the additional feature-matching loss [43] Lfm, to improve the sound quality of the reconstructed mel-spectrogram:
3g9w | Lady = Ex,t [log D(x) + log (1 - D(x))] ,
d09o | (5)
ylhz | 1 l=1 T NĮ | D'(x) - D'(x)|1| , (6)
a3td | Lfm = Ex,t
ex5k | where & is the reconstructed mel-spectrogram by G, T is the total number of layers in D and D' denotes the output feature map of l-th layer with Nį number of features. The feature matching loss can be seen as a reconstruction loss of hidden layer features of real and generated speech as judged by the discriminator.
9d4o | First stage full objectives. Our full objective functions in the first stage can be summarized as follows with hyperparameters As25, Amono, dadv and Afm:
cjq2 | min max Lmel + As25 Ls2s + Amono {mono
5esx | G,A,E,F,T D
m5ka | (7)
byu8 | + Xadv Lady + Afm Lfm
76b0 | 2) Second Stage Objectives:
zxz6 | Duration prediction. We employ the L-1 loss to train our duration predictor
xnth | Ldur = Ed [||d - dpred ||1]
vafm | (8)
2zmw | where d is the ground truth duration obtained by summing Calign along the mel frame axis. dpred = L(R(htext, s)) is the predicted duration under the style vector s.
bf9k | Prosody prediction. We train our prosody predictor via a unique data augmentation scheme. Since the duration predictor is trained separately from other modules (using only [dur), the duration predictions it produces may not always be optimal or compatible with the prosody predictor. To make the prosody predictor more robust to these potentially suboptimal duration predictions, we augment the data to introduce prosody invariance over the duration.
otkr | More specifically, instead of using the ground truth alignment, pitch, and energy of the original mel-spectrogram, we first apply a 1-D bilinear interpolation to stretch or compress the mel- spectrogram in time to obtain the augmented sample ã. As a result, the speech speed changes, yet the pitch and energy curves remain consistent. Accordingly, the prosody predictor learns to maintain pitch and energy prediction invariance, regardless of the duration of speech. This approach helps mitigate issues with unnatural prosody when the predicted duration is incorrect.
ibgs | We use Lf0 and Ln, which are F0 and energy reconstruction loss, respectively:
colo | Co = Epa [IlPa - Pp (S (htext, S) . @ align )|] (9)
d6r4 | Ln = Ex |na - Pn (S (htext, S) . @align ) |] (10)
9mrz | where pz, nz and @align are the pitch, energy and alignment of & E X the augmented dataset. Pp denotes the pitch output from the prosody predictor, and Pn denotes the energy output.
pk06 | Decoder reconstruction. Lastly, we aim to ensure that the predicted pitch and energy can be effectively used by the decoder. Given that the mel-spectrogram is stretched or compressed during data augmentation, using them as the ground truth may lead to unwanted artifacts in the predicted prosody. Instead, we train the prosody predictor to produce pitch and energy predictions that can effectively reconstruct the decoder's outputs
7wn9 | Lde = Eã,t | - G (htext · ãalign, s, p, î)|] , (11) where & = G (htext . @align, s, p, |x||) is the reconstruction of ã € Ñ, p = Pp (S (htext, S) · "align) the predicted pitch and ^ = Pn (S (htext, S) · Õalign) the predicted energy.
wcd0 | Second stage full objectives. Our full objective functions in the second stage can be summarized as follows with hyperparameters Àdur, Afo, and An:
caos | min Lde + 1 dur dur + AFOLF0 + AnCn S,L,P
qrvy | (12)
pn9j | III. EXPERIMENTS
1pmc | A. Datasets
5z6q | We conducted experiments on three datasets. We trained a single-speaker model on the LJSpeech dataset [44]. The LJSpeech dataset consists of 13,100 short audio clips with a total duration of approximately 24 hours. We used the same split as VITS where the training set contains 12,500 samples, the validation set 100 samples and the test set 500 samples. We also trained a multi-speaker model on the LibriTTS dataset [45]. The LibriTTS train-clean-460 subset consists of approximately
yeoh | Fig. 2. t-SNE visualization of style vectors. All styles are learned without explicit emotion or speaker labels. (a) Style vectors of reference audios in five different emotions of the speaker 0017 in ESD, computed by the multi-speaker model trained on ESD. (b) Style vectors of the same reference audios as in Fig. 2a, computed by the single-speaker model trained on the LJSpeech dataset. (c) Style vectors from the model trained on the LibriTTS data of 10 unseen speakers in the VCTK dataset.
b8hf | 245 hours of audio from 1,151 speakers. We removed utterances with a duration longer than 30 seconds and shorter than one second. We randomly split the train-clean-460 subset into a training (98%), a validation (1%), and a test (1%) set and use the test set for evaluation following [32]. We also used the VCTK [46] dataset to show that our model is capable of zero-shot speaker adaptation. We used the same training and test speaker split as in [47] for VCTK, where 88 speakers were used for training and the rest 20 were used for testing.
hntf | In addition, we trained a multi-speaker model on the emotional speech dataset (ESD) [48] to demonstrate the capacity to synthesize speech with diverse prosodic patterns. ESD consists of 10 Chinese and 10 English speakers reading the same 400 short sentences in five different emotions. We trained our model on 10 English speakers with all five emotions. We
99fo | TABLE I. COMPARISON OF EVALUATED MOS WITH 95% CONFIDENCE INTERVALS (CI) ON THE LJ SPEECH DATASET.
y5r7 | Fig. 3. Spectrograms of example reference audios and their corresponding generated speech reading "How much variation is there? Let's find it out." from the single-speaker model trained on LJSpeech. The estimated pitch contour is shown as white dots. Left top: Reference audio of a question, "Did England let nature take her course?". Note the pitch is mostly going up at the end of each word. Left bottom: Synthesized speech. The same pattern of pitch rising at the end of the words is present. Right top: Reference audio of surprised speech saying "It's true! I am shocked! My dreams!". Note the pitch goes up first and then down for each word. Right bottom: Synthesized speech with the same pattern of the pitch going up and down for most of the words.
lrry | upsampled training audios to 24 kHz to match the LibriTTS dataset. We converted text sequences into phoneme sequences using an open-source tool 1. We extracted mel-spectrograms with a FFT size of 2048, hop size of 300, and window length of 1200 in 80 mel bins using TorchAudio [49].
hvhm | B. Training
863b | For both stages, we trained all models for 200 epochs using the AdamW optimizer [50] with 31 = 0, 32 = 0.99, weight decay 1 = 10-4, learning rate y = 10-4 and batch size of 64 samples. We set s2s = 0.2, dadv = 1, mono = 5, 1fm = 0.2, Adur = 1, 40 = 0.1, and An = 1. This setting of hyperparameters makes sure that all loss values are on the same scale and that the training is not sensitive to these hyperparameters. The scale factor ranges from 0.75 to 1.25 for data augmentation. We randomly divided the mel-spectrograms into segments of the shortest length in the batch. The training was conducted on a single NVIDIA A40 GPU.
fj24 | 1https://github.com/Kyubyong/g2p
uz3c | C. Evaluations
d5bi | We performed two subjective evaluations: mean opinion score of naturalness (MOS-N) to measure the naturalness of synthesized speech, and mean opinion score of similarity (MOS- S) to evaluate the similarity between synthesized speech and reference for the multi-speaker model. We recruited native English speakers located in the U.S. to participate in the evaluations on Amazon Mechanical Turk2. In every experiment, we randomly selected 100 text samples from the test set. For each text, we synthesized speech using our model and the baseline models and included the ground truth for comparison. The baseline models include Tacorton 2 [51], FastSpeech 2 [11], and VITS [1]. For zero-shot speaker adapation experiments, we compared our model with StyleSpeech [32] and YourTTS [47]. All baseline models are pre-trained and publicly available (see Appendix B for details). The generated mel-spectrograms were converted into waveforms using the Hifi-GAN vocoder [52] for all models. Each set of speech was rated by 10 raters on a scale from 1 to 5 with 0.5 point increments. For a fair comparison, we downsampled our synthesized audio into 22 kHz to match those from baseline models. We used random references when synthesizing speech for the single-speaker and zero-shot speaker adaption experiments. For multi-speaker models, because our training did not require speaker labels, for a fair comparison with other models that use explicit speaker embeddings during training, we averaged the style vectors computed using all samples in the training set from the same speaker as the reference style.
ba4l | When evaluating each set, we randomly permuted the order of the models and instructed the subjects to listen and rate them without telling them the model labels. It is similar to multiple stimuli with hidden reference and anchor (MUSHRA), allowing the subjects to compare subtle differences among models. We used the ground truth as hidden attention checkers: raters were dropped from analysis if the MOS of the ground truth was not ranked top two among all the models.
ffx8 | We also conducted objective evaluations using ASR metrics. We evaluated the robustness of the models to different lengths of text input. We created four test sets with text length L < 10, 10 ≤ L < 50, 50 ≤ L < 100, and 100 ≤ L, respectively. Each set contains 100 texts sampled from the WSJ0 dataset [53]. We calculated the word error rate of the synthesized speech from both single-speaker and multi-speaker models using a pre-trained ASR model from ESPnet [54]. To measure the inference speed, we computed the real-time factor (RFT),
s6su | Fig. 4. Pearson correlation coefficients of six acoustic features associated with emotions between reference and synthesized speech on LJ Speech dataset.
y219 | TABLE III. COMPARISON OF EVALUATED MOS WITH 95% CONFIDENCE INTERVALS (CI) ON THE VCTK DATASET FOR UNSEEN SPEAKER ADAPTATION.
06ht | which denotes the time (in seconds) needed for the model to synthesize a one-second waveform. RFT was measured on a server with one NVIDIA 2080Ti GPU and a batch size of 1. In addition, we conducted the same analysis on the correlations of acoustic features associated with emotions between reference and synthesized speech using four multi-speaker models. Since there is no style in FastSpeech 2 and VITS, we used a pre- trained X-vector model [55] from Kaldi [56] to extract the speaker embedding as the reference vector.
v3r2 | IV. RESULTS
nv42 | A. Model Performance
xags | Tables I, II, and III showcase the results of human subjective evaluations on the LJSpeech and LibriTTS datasets. When assessed for naturalness (MOS-N) and similarity (MOS-S), StyleTTS clearly outperforms other models, demonstrating its superior performance under both single-speaker, multi-speaker, and zero-shot settings. Our models are more robust compared to other models (Table IV), especially for long input texts. Since we do not use generative flows that require inverse Jacobian
jc47 | computation, our model is faster than VITS [1], even though it was not trained end-to-end like VITS (Table VI).
ko3c | We do note that our evaluation results differ from those reported in the baseline models, particularly for VITS. The VITS model has been reported to yield results very close to the ground truth [1]. However, in our evaluation, VITS was found not to reach ground truth levels of performance. The primary factor leading to this discrepancy is the difference in evaluation methods. In VITS experiments, the traditional Mean Opinion Score (MOS) evaluation was used, where raters evaluated each module individually without any reference. The use of a reference point in our MUSHRA-like evaluation provides an anchor for rating, particularly the ground truth as the reference, which potentially lowers the scores of other models. A similar discrepancy has been reported in a very recent study that examines the effects of evaluation methods on the MOS results [57], and our evaluation of VITS is comparable to other studies that have tried to reproduce it on both LJSpeech and LibriTTS datasets [58], [59], [60], [61].
xek3 | B. Visualization of Style Vectors
bbpr | To verify that our model can learn meaningful style represen- tations, we projected the style vectors extracted from reference audios into a 2-D plane for visualization using t-SNE [62]. We selected 50 samples of each emotion from a single speaker in ESD and projected the style vectors of each audio into the 2-D space. It can be seen in Fig. 2(a) that our style vector distinctively encodes the emotional tones of reference sentences even though the training does not use emotion labels. We also computed the style vectors using speech samples from the same speaker with our single-speaker model. This model is only
5zyf | TABLE IV. ROBUSTNESS EVALUATION ON THE LJSPEECH AND LIBRITTS DATASET. WORD ERROR RATES (%) ARE REPORTED FOR DIFFERENT LENGTHS OF TEXT (L).
9own | TABLE V. COMPARISON OF PEARSON CORRELATION COEFFICIENTS OF ACOUSTIC FEATURES ASSOCIATED WITH EMOTIONS BETWEEN REFERENCE AND SYNTHESIZED SPEECH IN MULTI-SPEAKER EXPERIMENTS. FASTSPEECH 2 AND VITS EMPLOY THE X-VECTOR AS THE REFERENCE.
bqfb | TABLE VI. REAL TIME FACTOR (RTF) IN SECOND.
```

OUTPUT:
```
