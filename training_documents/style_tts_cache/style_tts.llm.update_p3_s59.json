{
  "id": "chatcmpl-A3UFmUV6A5pDNRMB6al1UfQIagesc",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "````\nolce | where N is the number of phonemes in t, ti is the i-th phoneme token of t, ti is the i-th predicted phoneme token, and C E is the cross-entropy loss function.\nerx1 | Since this alignment is not necessarily monotonic, we use a simple L one loss L mono that forces the soft attention alignment to be close to its non-differentiable monotonic version:\nb3bt | L mono equals expectation x t of the absolute value of a align minus a hard, where a align equals A of x t is the attention alignment and a hard is the monotonic hard alignment obtained through dynamic programming algorithms.\n0w8m | (b) Duration and prosody prediction\nvcu3 | Adversarial objectives. We employ two adversarial loss functions, the original cross-entropy loss function L adv for adversarial training and the additional feature-matching loss Lfm, to improve the sound quality of the reconstructed mel-spectrogram:\nhjzj | L adv equals expectation x t of the logarithm of D of x plus the logarithm of one minus D of x.\nj79f | L fm equals expectation x t of one over T N of the absolute value of D prime of x minus D prime of x.\n6tbv | L fm equals expectation x t.\nxb13 | where & is the reconstructed mel-spectrogram by G, T is the total number of layers in D and D prime denotes the output feature map of l-th layer with N number of features. The feature matching loss can be seen as a reconstruction loss of hidden layer features of real and generated speech as judged by the discriminator.\ncxqi | First stage full objectives. Our full objective functions in the first stage can be summarized as follows with hyperparameters As twenty-five, A mono, L adv and A fm.\nqfmx | minimize maximize L mel plus As twenty-five L s two s plus A mono L mono\nrut3 | G, A, E, F, T D\nm3cp | plus L adv L adv plus A fm L fm\nmq4s | Two) Second Stage Objectives:\n95el | Duration prediction. We employ the L one loss to train our duration predictor.\na0a5 | L dur equals expectation d of the absolute value of d minus d pred.\ngocl | where d is the ground truth duration obtained by summing C align along the mel frame axis. d pred equals L of R of h text s is the predicted duration under the style vector s.\nob1z | Prosody prediction. We train our prosody predictor via a unique data augmentation scheme. Since the duration predictor is trained separately from other modules (using only L dur), the duration predictions it produces may not always be optimal or compatible with the prosody predictor. To make the prosody predictor more robust to these potentially suboptimal duration predictions, we augment the data to introduce prosody invariance over the duration.\nm4cb | More specifically, instead of using the ground truth alignment, pitch, and energy of the original mel-spectrogram, we first apply a one-D bilinear interpolation to stretch or compress the mel-spectrogram in time to obtain the augmented sample. As a result, the speech speed changes, yet the pitch and energy curves remain consistent. Accordingly, the prosody predictor learns to maintain pitch and energy prediction invariance, regardless of the duration of speech. This approach helps mitigate issues with unnatural prosody when the predicted duration is incorrect.\nm61y | We use L f zero and L n, which are F zero and energy reconstruction loss, respectively.\nu2ob | L f zero equals expectation p a of the absolute value of P p of S of h text s times a align.\n2gnm | L n equals expectation x of the absolute value of na minus P n of S of h text s times a align.\nemgw | where p z, n z and a align are the pitch, energy and alignment of the augmented dataset. P p denotes the pitch output from the prosody predictor, and P n denotes the energy output.\nxaqt | Decoder reconstruction. Lastly, we aim to ensure that the predicted pitch and energy can be effectively used by the decoder. Given that the mel-spectrogram is stretched or compressed during data augmentation, using them as the ground truth may lead to unwanted artifacts in the predicted prosody. Instead, we train the prosody predictor to produce pitch and energy predictions that can effectively reconstruct the decoder's outputs.\n5rcb | L de equals expectation t of the absolute value of minus G of h text a align s p Ã®, where equals G of h text times a align s p absolute value of x is the reconstruction of a, p equals P p of S of h text s times a align the predicted pitch and ^ equals P n of S of h text s times a align the predicted energy.\n524j | Second stage full objectives. Our full objective functions in the second stage can be summarized as follows with hyperparameters A dur, A fo, and An.\nxd3m | minimize L de plus A dur L dur plus A f zero L f zero plus A n L n S, L, P.\n8qb6 | Three. EXPERIMENTS.\nwkbn | A. Datasets.\nftfg | We conducted experiments on three datasets. We trained a single-speaker model on the L J Speech dataset. The L J Speech dataset consists of thirteen thousand one hundred short audio clips with a total duration of approximately twenty-four hours. We used the same split as V I T S where the training set contains twelve thousand five hundred samples, the validation set one hundred samples and the test set five hundred samples. We also trained a multi-speaker model on the Libri T T S dataset. The Libri T T S train-clean-four hundred sixty subset consists of approximately\n7xi5 | two hundred forty-five hours of audio from one thousand one hundred fifty-one speakers. We removed utterances with a duration longer than thirty seconds and shorter than one second. We randomly split the train-clean-four hundred sixty subset into a training, ninety-eight percent, a validation, one percent, and a test, one percent, set and use the test set for evaluation. We also used the VCTK dataset to show that our model is capable of zero-shot speaker adaptation. We used the same training and test speaker split as in VCTK, where eighty-eight speakers were used for training and the rest twenty were used for testing.\nw8r5 | In addition, we trained a multi-speaker model on the emotional speech dataset, E S D, to demonstrate the capacity to synthesize speech with diverse prosodic patterns. E S D consists of ten Chinese and ten English speakers reading the same four hundred short sentences in five different emotions. We trained our model on ten English speakers with all five emotions. We\nybik | upsampled training audios to twenty-four kilohertz to match the Libri T T S dataset. We converted text sequences into phoneme sequences using an open-source tool. We extracted mel-spectrograms with an F F T size of two thousand forty-eight, hop size of three hundred, and window length of one thousand two hundred in eighty mel bins using Torch Audio.\ngii5 | B. Training.\nq5eu | For both stages, we trained all models for two hundred epochs using the Adam W optimizer with beta one equals zero, beta two equals zero point ninety-nine, weight decay lambda equals ten to the negative fourth, learning rate gamma equals ten to the negative fourth and batch size of sixty-four samples. We set s two s equals zero point two, L adv equals one, L mono equals five, L fm equals zero point two, A dur equals one, A f zero equals zero point one, and A n equals one. This setting of hyperparameters makes sure that all loss values are on the same scale and that the training is not sensitive to these hyperparameters. The scale factor ranges from zero point seventy-five to one point twenty-five for data augmentation. We randomly divided the mel-spectrograms into segments of the shortest length in the batch. The training was conducted on a single NVIDIA A forty GPU.\na7ao | C. Evaluations.\n```",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725394642,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 1691,
    "prompt_tokens": 3399,
    "total_tokens": 5090
  }
}