{
  "id": "chatcmpl-A3U9mM5kAfwAV6XiZPixvDm7zDOc8",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "o7qm | 'minimal semantics': according to minimalism, all well-formed, declarative sentences (relativized to a context of utterance) express truth-evaluable content which is fixed via the lexico-syntactic constituents of the sentence (plus their manner of composition) alone, where those constituents are, for the most part, not themselves context-sensitive. So, for instance, minimalism claims that there is a truth evaluable content to be recovered for sentences like \"All philosophy students study logic\" and \"Meera is ready\" just on the basis of the words and structure of these sentences (i.e. the first sentence literally claims that every philosophy student, without qualification, studies logic, while the second literally states simply that Meera is ready, without indicating what she is ready for). Even though these contents are not the ones that ordinary speakers who utter these sentences standardly convey, nevertheless minimalists argue that we need to recognize this kind of literal, minimal, type-level content as truth-evaluable semantic content (e.g. to explain our practices around holding speakers responsible for what they say, as well as for drawing distinctions like that between lying and misleading; see Borg explanatory roles). Recovery of minimal content does not (at least in some versions) require access to speaker intentions; in a slogan from Borg two thousand four: grasping semantic content is a matter of \"word-reading not mind-reading\". So, could we assign sentence-level, minimal semantic content to the outputs of LLMs?\nd52l | One potential objection to this suggestion is that LLMs engage in autoregressive processing, which imparts more context-sensitivity to the bearers of meaning than semantic minimalism allows. As noted in section one, in an LLM the vector assigned to a word in a current input string is moderated by the expressions that come before it (so that the vector assigned to \"dogs\" will differ if it is preceded by \"all\" or \"angry\", etc). Yet this might seem to suggest the systems are treating word meanings as much more context-sensitive than the current appeal to type-level semantic properties allows. However, I think this objection is probably too quick. A primary aim of type-level, formal semantics accounts like minimalism is to preserve the compositionality of meaning for complex linguistic items, whereby the meaning of a sentence is fixed by the meaning of the words it contains together with their mode of combination. Yet it is possible that the contextual vector moderation LLMs undertake merely reflects the compositional process. That is to say, rather than reflecting a change to the meaning of a word at the token level, vector moderation could reflect the combinatorial process of integrating word meanings across an\nqlbb | fifteen\nnhsg | entire string. As Fodor and Lepore nineteen ninety-eight: one hundred eleven to one hundred twelve note (in their discussion of treatments of polysemy and the purported need, which they deny, for a generative lexicon of the kind defended by Pustejovsky):\nhy0l | It's news if the lexicons of natural language are generative; lots of people (ourselves included) think they are more or less lists. But it's no news that verb phrases are generative, or that their semantics must somehow integrate the meaning of a verb with the meaning of its arguments.\n4jmz | The difficulty of deciding whether the kind of context-sensitivity introduced by autoregressive processing fits with or challenges a type-level approach to semantic content reflects a point already noted in section two, namely that there are currently no conclusive views on what the internal states of LLMs represent. Given this, however, I think it would be too soon to conclude that the mere fact of autoregressive processing shows that LLM outputs are not realizing type-level semantic content.\nb7cq | A second challenge to the idea that LLMs can be treated as dealing in minimal content emerges from the fact that the outputs of LLMs are not always reflective of this type of content. That is to say, although in response to some prompts ChatGPT does deliver outputs which reflect a sensitivity to the strict, literal content of the prompts it receives, in others the outputs seem to suggest a sensitivity to richer, more contextually determined content. To see this, consider the following exchanges:\n0b39 | In both these exchanges, ChatGPT responses pick up on the minimal content of the input sentence. In response to other prompts, however, LLM outputs seem to focus more on what we might think of as the contextually enriched content of the prompt. For instance, tell ChatGPT that \"Linda took out her key and opened the door\" and the system's response incorporates the bridging inference (that Linda used her keys to open the door). Yet this bridging inference is relevant at the token, not the type, level (that is to say, the sentence form \"P and Q\" does not always invite this kind of bridging; see Carston two thousand two):\nqecd | Yes, the sentence \"Jill took out her keys and opened the door\" implies that the door was locked before Jill opened it. Taking out keys and using them to open a door is typically done to unlock and then open the door. If the door was already unlocked, there would be no need for Jill to use her keys to open it.\n1o7p | In addition, sometimes ChatGPT delivers answers in line with content commonly thought of as richly pragmatically modulated content, i.e. picking up on Gricean implicatures, as in the following:\ncp13 | seventeen\nbgre | As ChatGPT's response notes, Bill's utterance in the imagined context typically suggests that he can't have dinner with Gemma. ChatGPT here reflects content that Bill would commonly be taken to imply by his utterance, but such implications have standardly been thought of as highly contextualized, attaching to token utterances in very specific contexts, not as part of the general, type-level meaning of sentences.\n57lp | So, does the fact that the responses issued by ChatGPT in the above examples capture both minimal and pragmatically enriched content show that we should not treat LLM outputs as possessing minimal semantic content? I think there are two points to note about this. First, the claims are about differences in how ChatGPT is processing linguistic prompts (i.e as reflecting rich contextual information in the latter cases but focusing on a more minimal, type-level content in the former). However, this is not directly relevant for the current claim, which is that LLM outputs can be understood as expressing minimal content. That is to say, the claim is that the sentences LLMs produce can be understood as expressing their literal, type-level meaning, regardless of whether what they are reporting is the strict, literal content of the prompt or some more contextualized content. Secondly, the fact that LLMs seamlessly report on aspects of content that have traditionally been taken to be driven by context rather than by strict, literal meaning, is unsurprising. LLMs are massively holistic systems, where every answer they produce reflects the entire database they were exposed to during pre-training. Since the kind of contextually enriched contents reflected in the second two outputs from ChatGPT above reflect the way in which these kinds of sentences are commonly used by ordinary speakers, we should expect that what are commonly thought of as pragmatic inferences find their way into the system. It is because when keys and doors are mentioned, the most common relationship involves the keys being used to open the door that ChatGPT embodies the bridging inference in its response above. What this shows is that sensitivity to distributional properties across a large enough body of text will result in outputs which are sensitive to (at least some) pragmatic\nukth | eighteen\nkpvt | inferences, just as sensitivity to distributional properties results in a sensitivity to minimal content. The fact that LLM outputs often reflect pragmatically enhanced versions of the input strings they receive speaks against any claim that the distributional properties LLMs map are fixed by the literal meaning of words and sentences alone, but this need not be part of the current claim (that LLM outputs can be understood in terms of the minimal content they express).",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725394270,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 1662,
    "prompt_tokens": 3355,
    "total_tokens": 5017
  }
}