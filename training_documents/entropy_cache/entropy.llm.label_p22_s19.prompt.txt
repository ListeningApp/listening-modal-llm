You will receive raw text from an OCR scan of a document or web page. Each line of input represents one textbox from the document.

Your task is to label each textbox with exactly one of the following labels:
  title: the primary document title, no more than one per document
  heading: the name of the following chapter or section (should never start in the middle of the previous paragraph)
  subheading: a secondary heading, often following another heading (should never start in the middle of the previous paragraph)
  body: main paragraph content: may start or end in the middle of a word, use context to follow paragraphs that are split across textboxes (if a textbox contains both heading and body text, label it "body")
  math: textbox primarily containing math
  imageDescription: standalone callout that only describes an image, chart, table, or similar (typically begins with "Table...", "Figure...", "Fig. ...", "Above...", "Left...", etc.)
  authors: names of contributors to this document
  institutions: list of universities, business, and other institutions affiliated with this document
  publisher: info about the publisher, or provided by the publisher, including copyright info
  pagenum: the page number
  headerOrFooter: boilerplate text page number, or redundant headings at the top/bottom of the page
  toc: the table of contents
  references: bibliographic references and resources
  acknowledgements: thanks and credits to contributors
  appendix: index, appendix, or any other end matter not part of the main thread (including related headings)
  table: tabular data, label, title, or subheading for a grid or list of data
  datum: numeric data or data label from a chart or list
  advertisement: promotional ads and marketing
  layout: non-content related to the layout of the page, e.g. sidebar navigation
  callout: pull quote or long block text that stands apart from the main text
  footnote: footnotes and endnotes near the bottom of the page
  keywords: list of keywords or tags
  url: web address or email
  other: any other text that does not belong to the main thread

Each line of input is prefixed with a textbox ID in the format `id | Text content`. The output must be in the format `id | label`.

Note that body text may be split across multiple textboxes, and textboxes may start or end in the middle of a sentence or word. Because the text was extracted from a web page or document, paragraphs of body text may be interrupted by multiple textboxes of redundant headers, footnotes, page numbers, tables, images, etc. For example, a real heading will never interrupt the middle of a sentence. Use context clues like this to carefully label each textbox.

EXAMPLE INPUT:
1pjs | Neuroscience Applied 1 (2022) 101012
o2kr | Contents lists available at ScienceDirect
v6sk | Neuroscience Applied
1wj6 | journal homepage: www.journals.elsevier.com/neuroscience-applied
936l | Research Articles
06yq | Microdosing with psychedelics to self-medicate for ADHD symptoms in adults: A naturalistic study
dpv6 | ARTICLE INFO
8r5j | ABSTRACT
2taz | Keywords: ADHD Microdosing Psychedelics Self-medication Well-being Time perception
o3ya | ADHD in adulthood is often overlooked, which negatively affects the individual's well-being. First-line phar- macological interventions are effective in many ADHD patients, relieving symptoms rapidly. However, there seems to be a proportion of individuals who discontinue, or fail to respond to these treatments. For these in- dividuals, alternative treatment options should be explored.
r5il | 1. Introduction
5gui | Attention deficit hyperactivity disorder (ADHD) is one of the most common developmental disorders worldwide. Prevalence research indicates that 2.6% of the adult population has persistent ADHD. ADHD in adults is often overlooked because of the high comorbidity rate and lack of knowledge about how ADHD is expressed in adulthood (Kooij et al.,
9d4o | Fig. 1. Flowchart of included participants for each time point.
82qc | 2019). In addition, ADHD is associated with deficits in various domains of cogni- tive functioning. Twenty-five percent of ADHD cases suffered from ADHD symptoms purely because of de- ficiencies in temporal processing (Sonuga-Barke et al., 2010).
ls7d | First-line ADHD treatments in adults mainly include pharmacological interventions to enhance dopaminergic and noradrenergic neurotrans- mission with stimulants. Overall, they have been proven to work effectively in adults with ADHD, inducing fast symptom relief and thereby enhancing the person's quality of life. In the
5esx | Table 1 Demographic information from respondents at baseline and the two and four- week time points.
t8vv | longer term, approximately twenty percent of ADHD patients discontinue their prescribed medication after six to nine months, thirty percent after one year, and half of them after two years.
xrd1 | 2. Material and methods
5746 | 2.1. Study design and participants
7toz | The study employed a naturalistic design, assessing the experiences of participants at baseline,
j346 | Neuroscience Applied 1 (2022)
m5ka | E.C.H.M. Haijen et al.
kx2c | REVIEW
v8uq | before they start MD on their own initiative, and at two and four weeks after MD initiation. The target population included adults diagnosed with ADHD and individuals who experienced ADHD symptoms to the extent that these interfered with their daily lives and who had not been diag- nosed with ADHD before. To be included in the analyses, participants needed to score above a cut-off point on at least one of the subscales of the Conner's Adult ADHD Rating Scale (CAARS-S:SV).
2zmw | Fig. 2. Mean scores of the CAARS-S:SV DSM-IV total symptoms T-scores at baseline (0W) and two (2W) and four weeks (4W) after MD (A) of the whole sample, and (B) per conventional ADHD medication use. Error bars represent mean error.
sdk3 | <LATEX>\lim _ { x ightarrow \infty } rac { 6 x ^ { 2 } + 1 0 0 } { 7 x ^ { 2 } - 1 0 } =</LATEX>
x972 | 2.2. Study procedure
k221 | Mean performance measures of the CAARS-

EXAMPLE OUTPUT:
1pjs | headerOrFooter
o2kr | publisher
v6sk | publisher
1wj6 | publisher
936l | headerOrFooter
06yq | title
dpv6 | other
8r5j | heading
2taz | keywords
o3ya | body
r5il | heading
5gui | body
9d4o | imageDescription
82qc | body
ls7d | body
5esx | imageDescription
t8vv | body
xrd1 | heading
5746 | subheading
7toz | body
j346 | headerOrFooter
m5ka | authors
kx2c | headerOrFooter
v8uq | body
2zmw | imageDescription
sdk3 | math
x972 | heading
k221 | body




ftfg | <LATEX>\int _ { - \infty } ^ { \infty } x ^ { 2 } \sin x d x ?</LATEX> There's no good answer. We say a function <LATEX>f : X \rightarrow \mathbb{R}</LATEX> is integrable if it is measurable and its integral over <LATEX>X ,</LATEX> defined in a certain way I won't explain here, gives a well-defined real number.
7xi5 | 15
w8r5 | SHANNON ENTROPY: A FIRST TASTE
ybik | When you learn an event of probability p has happened, the amount of information you get is - log p.
gii5 | Question. Suppose you know a coin lands heads up 2 3 of the time and tails up : of the time. What is the average or 'expected' amount of information you get when you learn which side landed up?
q5eu | Answer. 2 of the time you get - log 2 of information, and 1 of the time you get - log a. So, the expected amount of information you get is
a7ao | - 3 log 2 - 3 log 3
29a3 | You can do the same thing whenever you have any number of prob- abilities that add to 1. The expected information is called the Shan- non entropy.
sb1e | You flip a coin. You know the probability that it lands heads up. How much information do you get, on average, when you discover which side lands up? It's not hard to work this out. It's a simple example of 'Shannon entropy'. Roughly speaking, entropy is information that you don't know, that you could get if you did enough experiments. Here the experiment is simply flipping the coin and looking at it.
w2yn | Puzzle 18. Suppose you know a coin lands heads up 2 of the time and tails up 2 of the time. What is the expected amount of information you get from a coin flip? If you use base 2 for the logarithm, you get the expected information measured in bits. What do you get?
ap6l | Puzzle 19. Suppose you know a coin lands heads up % of the time and tails up 3 of the time. What is the expected amount of information you get from a coin flip?
iest | Puzzle 20. Suppose you know a coin lands heads up - of the time and tails up & of the time. What is the expected amount of information you get from a coin flip, in bits?
cox8 | If you solve these you'll see a pattern: the Shannon entropy is biggest when the coin is fair. As it becomes more and more likely for one side to land up than the other, the entropy drops. You're more sure about what will happen ... so you learn less, on average, from seeing what happens!
ppqi | We've been doing examples where your experiment has just two possible outcomes: heads up or down. But you can do Shannon entropy for any number of outcomes. It measures how ignorant you are of what will happen. That is: how much you learn on average when it does!
oxou | 16
495n | SHANNON ENTROPY: A SECOND TASTE
awri | According to the weather report there's a <LATEX>\frac { 1 } { 4 }</LATEX> chance that it will rain 1 centimeter, <LATEX>\frac { 1 } { 2 }</LATEX> chance it will rain 2 centimeters, and <LATEX>\frac { 1 } { 4 }</LATEX> chance it will rain 3 centimeters.
bl28 | Question. What is the 'expected' amount of rainfall?
d5bz | Answer. <LATEX>\frac { 1 } { 4 } \cdot 1 + \frac { 1 } { 2 } \cdot 2 + \frac { 1 } { 4 } \cdot 3 = 2</LATEX> centimeters.
ap5o | Question. What is the 'expected' amount of information you learn when you find out how much it rains?
7zvf | Answer. <LATEX>\frac { 3 } { 2 }</LATEX> <LATEX>- \frac { 1 } { 4 } \log \frac { 1 } { 4 } - \frac { 1 } { 2 } \log \frac { 1 } { 2 } - \frac { 1 } { 4 } \log \frac { 1 } { 4 } = \frac { 3 } { 2 } \log 2 ,</LATEX> or in other words, 3 bits. This is the Shannon entropy of the weather report. 2
mov9 | If the weather report tells you it'll rain different amounts with different probabilities, you can figure out the 'expected' amount of rain. You can also figure out the expected amount of information you'll learn when it rains. This is called the 'Shannon entropy'.
ks7m | Shannon entropy is closely connected to information, but we can also think of it as a measure of ignorance. This may seem paradoxical. But it's not. Shannon entropy is the expected amount of information that you don't know when all you know is a probability distribution, which you will know when you see a specific outcome chosen according to this probability distribution.
w1w1 | For example, consider a weather report that says it will rain 1 centimeter with probability 0, 2 centimeters with probability 1, and 3 centimeters with probability 0. The Shannon entropy of this weather report is
h8gv | <LATEX>- 0 \log 0 - 1 \log 1 - 0 \log 0 = 0</LATEX> since by convention we set <LATEX>p \log p = 0</LATEX> when <LATEX>p = 0 ,</LATEX> this being the limit of <LATEX>p \ln p</LATEX> as <LATEX>p</LATEX> approaches 0 from above.
kwkp | What does it mean that this weather report has zero Shannon entropy? It means that when we see a specific outcome chosen according to this probability distribution, we learn nothing! The weather report says it will rain 2 centimeters with probability 1. When this happens, we learn nothing that the weather report didn't already tell us.
yj0m | The Shannon entropy doesn't depend on the amounts of rain, or even whether the forecast is about centimeters of rain or dollars of income. It only depends on the probabilities of the various outcomes. So Shannon entropy is a universal, abstract concept.
ewfp | Shannon entropy is closely connected to Gibbs entropy, which was already known in physics. But by lifting entropy to a more general level and connecting it to digital information, Shannon helped jump-start the information age. In fact a paper of his was the first to use the word 'bit'!
oczy | 17
2ttg | THE DEFINITION OF SHANNON ENTROPY
his3 | Suppose you believe there are n possible outcomes with probabilities p1, . . . , Pn ≥ 0 that sum to 1.
m50z | The average amount of information you learn when one of these outcomes happens, chosen according to this probability distribution, is the Shannon entropy:
me84 | H = - pi log pi i=1 n
o56m | Shannon entropy is larger for probability distributions that are more spread out, and smaller for probability distributions that are more sharply peaked.
slz6 | I've been leading up to it with examples, but here it is in general: Shannon entropy! Gibbs had already used a similar formula in physics-but with base e for the logarithm, an integral instead of a sum, and multiplying the answer by Boltzmann's constant. Shannon applied it to digital information.
90k5 | Here's where the formula for Shannon entropy comes from. We have some set of outcomes, say X. We have a probability distribution on this set, meaning a function p: X -> [0, 1] such that
9m8s | pi = 1.
893y | If we have any function A: X -> R, we define its expected value to be (A) = piAi. iEX
favz | It's a kind of average of A where each value A(i) is 'weighted', i.e. multiplied, by the probability of the ith outcome. We saw an example in the last tweet: the expected amount of rainfall.
7szx | We've seen that if you believe the ith outcome has probability pi, the amount of information you learn if the ith outcome actually occurs is - log pi. Thus, the expected amount of information you learn is
hx9e | (-log p) = - pi log pi. iEX
ywn2 | And this is the Shannon entropy! We denote it by H, or more precisely H (p), so
a63r | H(p) = - pi log pi. iEX In the box above I was taking X to be the set {1, ... , n}. This is often a good thing to do when there are finitely many outcomes.
dqvj | Let's get to know the Shannon entropy a little better.
r17u | 18
3c47 | Puzzle 21. Let <LATEX>X = 1 , 2</LATEX> so that we know a probability distribution <LATEX>p</LATEX> on <LATEX>X</LATEX> if we know <LATEX>p _ { 1 } ,</LATEX> since <LATEX>p _ { 2 } = 1 - p _ { 1 } .</LATEX> Graph the Shannon entropy <LATEX>H \left( p \right)</LATEX> as a function of <LATEX>p _ { 1 } .</LATEX> Show that it has a maximum at <LATEX>p _ { 1 } = \frac { 1 } { 2 }</LATEX> and minima at <LATEX>p _ { 1 } = 0</LATEX> and
g345 | <LATEX>p _ { 1 } = 1 .</LATEX> This makes sense: if you believe <LATEX>p _ { 1 } = 1</LATEX> then you learn nothing when an outcome happens chosen according to the probability distribution <LATEX>p :</LATEX> you are sure outcome 1 will occur, and it does (with probability 1). Similarly, if you believe <LATEX>p _ { 1 } = 0</LATEX> you learn nothing when an outcome happens according to this probability distribution, since you are sure outcome 2 will occur. On the other hand, if <LATEX>p _ { 1 } = \frac { 1 } { 2 } \mathrm { y o u }</LATEX> are maximally undecided about what will happens, and you learn 1 bit of information when it does.
svhz | Puzzle 22. Let <LATEX>X = 1 , 2 , 3 .</LATEX> Draw the set of probability distributions on <LATEX>X</LATEX> as an equilateral triangle whose corners are the probability distributions <LATEX>\left( 1 , 0 , 0 \right) ,</LATEX> <LATEX>\left( 0 , 1 , 0 \right) ,</LATEX> and <LATEX>\left( 0 , 0 , 1 \right) .</LATEX> Sketch contour lines of <LATEX>H \left( p \right)</LATEX> as a function on this triangle. Show it has a maximum at <LATEX>p = \left( \frac { 1 } { 3 } , \frac { 1 } { 3 } , \frac { 1 } { 3 } \right)</LATEX> and minima at the corners of the triangle.
oaen | Again this should make intuitive sense. Here is a harder puzzle along the same lines:
5u67 | Puzzle 23. Let <LATEX>X = \left\{ 1 , \ldots , n \right\} .</LATEX> Show that <LATEX>H \left( p \right)</LATEX> has a maximum at <LATEX>p = \left( \frac { 1 } { n } , \ldots , \frac { 1 } { n } \right)</LATEX> and minima at the probability distributions where <LATEX>p _ { i } = 1</LATEX> for some particular
dvms | <LATEX>i \in X .</LATEX> Here is one of the big lessons from all this:
qrd3 | Shannon entropy is larger for probability distributions that are more spread out, and smaller for probability distributions that are more sharply peaked.
rxrg | Indeed, you can think of Shannon entropy as a measure of how spread out a proba- bility distribution is! The more spread out it is, the more you learn when an outcome occurs, drawn from that distribution.
tf43 | 19
fcco | THE PRINCIPLE OF MAXIMUM ENTROPY
8uji | Suppose there are n possible outcomes. At first you have no reason to think any is more probable than any other.
df3e | Then you learn some facts about the correct probability distribution-but not enough to determine it uniquely! Which probability distribution P1, . .. , Pn should you choose?
53eu | The principle of maximum entropy says:
a5u1 | Of all the probability distributions consistent with the facts you've learned, choose the one with the largest Shannon entropy
ry7u | H = - > i=1 n Pi log på
5mda | What's Shannon entropy good for? For starters, it gives a principle for choosing the 'best' probability distribution consistent with what you know. Choose the one that maximizes the Shannon entropy!
i4li | This is called the 'principle of maximum entropy'. This principle first arose in statistical mechanics, which is the application of probability theory to physics-but we can use it elsewhere too.
r8sm | For example: suppose you have a die with faces numbered 1,2,3,4,5,6. At first you think it's fair. But then you somehow learn that the average of the numbers that comes up when you roll it is 5. Given this, what's the probability that if you roll it, a 6 comes up?
cuin | Sounds like an unfair question! But you can figure out the probability distribution on {1, 2, 3, 4, 5, 6} that maximizes Shannon entropy subject to the constraint that the mean is 5. According to the principle of maximum entropy, you should use this to answer my question!
n1i7 | But is this correct?
0r5m | The problem is figuring out what 'correct' means! But in statistical mechanics we use the principle of maximum entropy all the time, and it seems to work well. The brilliance of E. T. Jaynes was to realize it's a general principle of reasoning, not just for physics.
ne2o | The principle of maximum entropy is widely used outside physics, though still con- troversial. But I think we should use it to figure out some basic properties of a gas-like its energy or entropy per molecule, as a function of pressure and temperature.
sqxn | To do this, we should generalize Shannon entropy to 'Gibbs entropy', replacing the sum by an integral. Or else we should 'discretize' the gas, assuming each molecule has a finite set of states. It sort of depends on whether you prefer calculus or programming. Either approach is okay if we study our gas using classical statistical mechanics.
6pmo | Quantum statistical mechanics gives a more accurate answer. It uses a more general
195r | 20
kqmi | definition of entropy-but the principle of maximum entropy still applies!
g8rd | I won't dive into any calculations just yet. Before doing a gas, we should do some simpler examples-like the die whose average roll is 5. But I can't resisting mentioning one philosophical point. In the box above I was hinting that maximum entropy works when your 'prior' is uniform:
nns9 | Suppose there are n possible outcomes. At first you have no reason to think any is more probable than any other.
9uwy | This is an important assumption: when it's not true, the principle of maximum entropy as we've stated it does not apply. But what if our set of events is something like a line? There's no obvious best probability measure on the line! And even good old Lebesgue measure dx depends on our choice of coordinates. To handle this, we need a generalization of the principle of maximum entropy, called the principle of maximum relative entropy.