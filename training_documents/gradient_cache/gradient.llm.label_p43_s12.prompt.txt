You will receive raw text from an OCR scan of a document or web page. Each line of input represents one textbox from the document.

Your task is to label each textbox with exactly one of the following labels:
  title: the primary document title, no more than one per document
  heading: the name of the following chapter or section (should never start in the middle of the previous paragraph)
  subheading: a secondary heading, often following another heading (should never start in the middle of the previous paragraph)
  body: main paragraph content: may start or end in the middle of a word, use context to follow paragraphs that are split across textboxes (if a textbox contains both heading and body text, label it "body")
  math: textbox primarily containing math
  imageDescription: standalone callout that only describes an image, chart, table, or similar (typically begins with "Table...", "Figure...", "Fig. ...", "Above...", "Left...", etc.)
  authors: names of contributors to this document
  institutions: list of universities, business, and other institutions affiliated with this document
  publisher: info about the publisher, or provided by the publisher, including copyright info
  pagenum: the page number
  headerOrFooter: boilerplate text page number, or redundant headings at the top/bottom of the page
  toc: the table of contents
  references: bibliographic references and resources
  acknowledgements: thanks and credits to contributors
  appendix: index, appendix, or any other end matter not part of the main thread (including related headings)
  table: tabular data, label, title, or subheading for a grid or list of data
  datum: numeric data or data label from a chart or list
  advertisement: promotional ads and marketing
  layout: non-content related to the layout of the page, e.g. sidebar navigation
  callout: pull quote or long block text that stands apart from the main text
  footnote: footnotes and endnotes near the bottom of the page
  keywords: list of keywords or tags
  url: web address or email
  other: any other text that does not belong to the main thread

Each line of input is prefixed with a textbox ID in the format `id | Text content`. The output must be in the format `id | label`.

Note that body text may be split across multiple textboxes, and textboxes may start or end in the middle of a sentence or word. Because the text was extracted from a web page or document, paragraphs of body text may be interrupted by multiple textboxes of redundant headers, footnotes, page numbers, tables, images, etc. For example, a real heading will never interrupt the middle of a sentence. Use context clues like this to carefully label each textbox.

EXAMPLE INPUT:
1pjs | Neuroscience Applied 1 (2022) 101012
o2kr | Contents lists available at ScienceDirect
v6sk | Neuroscience Applied
1wj6 | journal homepage: www.journals.elsevier.com/neuroscience-applied
936l | Research Articles
06yq | Microdosing with psychedelics to self-medicate for ADHD symptoms in adults: A naturalistic study
dpv6 | ARTICLE INFO
8r5j | ABSTRACT
2taz | Keywords: ADHD Microdosing Psychedelics Self-medication Well-being Time perception
o3ya | ADHD in adulthood is often overlooked, which negatively affects the individual's well-being. First-line phar- macological interventions are effective in many ADHD patients, relieving symptoms rapidly. However, there seems to be a proportion of individuals who discontinue, or fail to respond to these treatments. For these in- dividuals, alternative treatment options should be explored.
r5il | 1. Introduction
5gui | Attention deficit hyperactivity disorder (ADHD) is one of the most common developmental disorders worldwide. Prevalence research indicates that 2.6% of the adult population has persistent ADHD. ADHD in adults is often overlooked because of the high comorbidity rate and lack of knowledge about how ADHD is expressed in adulthood (Kooij et al.,
9d4o | Fig. 1. Flowchart of included participants for each time point.
82qc | 2019). In addition, ADHD is associated with deficits in various domains of cogni- tive functioning. Twenty-five percent of ADHD cases suffered from ADHD symptoms purely because of de- ficiencies in temporal processing (Sonuga-Barke et al., 2010).
ls7d | First-line ADHD treatments in adults mainly include pharmacological interventions to enhance dopaminergic and noradrenergic neurotrans- mission with stimulants. Overall, they have been proven to work effectively in adults with ADHD, inducing fast symptom relief and thereby enhancing the person's quality of life. In the
5esx | Table 1 Demographic information from respondents at baseline and the two and four- week time points.
t8vv | longer term, approximately twenty percent of ADHD patients discontinue their prescribed medication after six to nine months, thirty percent after one year, and half of them after two years.
xrd1 | 2. Material and methods
5746 | 2.1. Study design and participants
7toz | The study employed a naturalistic design, assessing the experiences of participants at baseline,
j346 | Neuroscience Applied 1 (2022)
m5ka | E.C.H.M. Haijen et al.
kx2c | REVIEW
v8uq | before they start MD on their own initiative, and at two and four weeks after MD initiation. The target population included adults diagnosed with ADHD and individuals who experienced ADHD symptoms to the extent that these interfered with their daily lives and who had not been diag- nosed with ADHD before. To be included in the analyses, participants needed to score above a cut-off point on at least one of the subscales of the Conner's Adult ADHD Rating Scale (CAARS-S:SV).
2zmw | Fig. 2. Mean scores of the CAARS-S:SV DSM-IV total symptoms T-scores at baseline (0W) and two (2W) and four weeks (4W) after MD (A) of the whole sample, and (B) per conventional ADHD medication use. Error bars represent mean error.
sdk3 | <LATEX>\lim _ { x ightarrow \infty } rac { 6 x ^ { 2 } + 1 0 0 } { 7 x ^ { 2 } - 1 0 } =</LATEX>
x972 | 2.2. Study procedure
k221 | Mean performance measures of the CAARS-

EXAMPLE OUTPUT:
1pjs | headerOrFooter
o2kr | publisher
v6sk | publisher
1wj6 | publisher
936l | headerOrFooter
06yq | title
dpv6 | other
8r5j | heading
2taz | keywords
o3ya | body
r5il | heading
5gui | body
9d4o | imageDescription
82qc | body
ls7d | body
5esx | imageDescription
t8vv | body
xrd1 | heading
5746 | subheading
7toz | body
j346 | headerOrFooter
m5ka | authors
kx2c | headerOrFooter
v8uq | body
2zmw | imageDescription
sdk3 | math
x972 | heading
k221 | body




lf1t | (24)
tze7 | Those local second derivatives with respect to connection weights can be computed from local second derivatives with respect to the total input of the downstream unit
ryvv | 82 EP 02 EP
zdrh | (25)
osmo | where x; is the state of unit j and 02 EP /da? is the second derivative of the instantaneous loss function with respect to the total input to unit i (denoted a;). Interestingly, there is an efficient algorithm to compute those second derivatives which is very similar to the back-propagation procedure used to compute the first derivatives [20], [21]
0b8v | k 02 EP daž + f" (i) on; . aEP
fucs | da? = f'(a)2 Z uk 32 EP
yply | (26
75n5 | Unfortunately, using those derivatives leads to well-known problems associated with every Newton-like algorithm: these terms can be negative and can cause the gradient algorithm to move uphill instead of downhill. Therefore, our second approximation is a well-known trick called the Gauss-Newton approximation, which guarantees that the second derivative estimates are nonnegative. The Gauss-Newton approximation essentially ignores the nonlinearity of the estimated function (the NN, in our case), but not that of the loss function. The back propagation equation for Gauss-Newton approximations of the second derivatives is
ec91 | 82 EP = f'(ai)} } Uni k 02 EP daž
wul6 | (27)
4rhy | This is very similar to the formula for back propagating the first derivatives, except that the sigmoid's derivative and the weight values are squared. The right-hand side is a sum of products of nonnegative terms, therefore the left-hand side term is nonnegative.
092d | The third approximation we make is that we do not run the average in (24) over the entire training set, but run it on a small subset of the training set instead. In addition the re-estimation does not need to be done often since the second-order properties of the error surface change rather slowly. In the experiments described in this paper, we re- estimate the hkk on 500 patterns before each training pass through the training set. Since the size of the training set is 60 000, the additional cost of re-estimating the hkk is negligible. The estimates are not particularly sensitive to the particular subset of the training set used in the averaging. This seems to suggest that the second-order properties of the error surface are mainly determined by the structure of the network, rather than by the detailed statistics of the samples. This algorithm is particularly useful for shared- weight networks because the weight sharing creates ill conditioning of the error surface. Because of the sharing, one single parameter in the first few layers can have an enormous influence on the output. Consequently, the second derivative of the error with respect to this parameter may be very large, while it can be quite small for other parameters elsewhere in the network. The above algorithm compensates for that phenomenon.
94ul | Unlike most other second-order acceleration methods for back-propagation, the above method works in stochastic mode. It uses a diagonal approximation of the Hessian. Like the classical Levenberg-Marquardt algorithm, it uses a "safety" factor u to prevent the step sizes from getting too large if the second derivative estimates are small. Hence the method is called the stochastic diagonal Lev- enberg-Marquardt method.
0vdu | ACKNOWLEDGMENT
a230 | Some of the systems described in this paper are the work of many researchers now at AT&T and Lucent Technolo- gies. In particular, C. Burges, C. Nohl, T. Cauble, and J. Bromley contributed much to the check reading system. Experimental results described in Section III include con- tributions by C. Burges, A. Brunot, C. Cortes, H. Drucker, L. Jackel, U. Müller, B. Schölkopf, and P. Simard. The authors wish to thank F. Pereira, V. Vapnik, J. Denker, and I. Guyon for helpful discussions, C. Stenard and R. Higgins for providing the applications that motivated some of this work, and L. R. Rabiner and L. D. Jackel for relentless support and encouragement.
dvoe | REFERENCES
677t | in Advances in Neural Information Processing Systems 6, J. D. Cowan, G. Tesauro, and J. Alspector, Eds. San Mateo, CA: Morgan Kaufmann, 1994, pp. 327-334.
5hk6 | [7] , Statistical Learning Theory. New York: Wiley, 1998.
viei | [9] S. I. Amari, "A theory of adaptive pattern classifiers," IEEE Trans. Electron. Comput., vol. EC-16, pp. 299-307, 1967.
rk0j | [30] D. H. Hubel and T. N. Wiesel, "Receptive fields, binocular in-
c6x9 | teraction, and functional architecture in the cat's visual cortex," J. Physiology (London), vol. 160, pp. 106-154, 1962.
g5at | [31] K. Fukushima, "Cognition: A self-organizing multilayered neu- ral network," Biological Cybern., vol. 20, pp. 121-136, 1975.
re3j | [32] K. Fukushima and S. Miyake, "Neocognitron: A new algorithm for pattern recognition tolerant of deformations and shifts in position," Pattern Recognit., vol. 15, no. 6, pp. 455-469, Nov. 1982.
6z45 | [52] I. Guyon, I. Poujaud, L. Personnaz, G. Dreyfus, J. Denker, and Y. LeCun, "Comparing different neural net architectures for
20af | classifying handwritten digits," in Proc. IEEE IJCNN, Wash- ington, DC, vol. II, 1989, pp. 127-132,.
huae | [54] J. Schürmann, "A multifont word recognition system for postal address reading," IEEE Trans. Comput., vol. C-27, pp. 721-732, Aug. 1978.
rblh | [55] Y. Lee, "Handwritten digit recognition using k-nearest neigh- bor, radial-basis functions, and backpropagation neural net- works," Neural Computation, vol. 3, no. 3, pp. 440-449, 1991.
gl5i | [74] M. A. Franzini, K. F. Lee, and A. H. Waibel, "Connectionist viterbi training: A new hybrid method for continuous speech recognition," in Proc. Int. Conf. Acoustics, Speech, Signal Pro- cessing, Albuquerque, NM, 1990, pp. 425-428.
x0yg | [75] L. T. Niles and H. F. Silverman, "Combining hidden Markov models and neural network classifiers," in Proc. Int. Conf. Acoustics, Speech, Signal Processing, Albuquerque, NM, 1990, pp. 417-420.
mjt8 | [76] X. Driancourt and L. Bottou, "MLP, LVQ and DP: Comparison & cooperation," in Proc. Int. Joint Conf. Neural Networks, Seattle, WA, vol. 2, 1991, pp. 815-819.
htcn | [77] Y. Bengio, R. De Mori, G. Flammia, and R. Kompe, "Global optimization of a neural network-hidden Markov model hybrid," IEEE Trans. Neural Networks, vol. 3, pp. 252-259, March 1992.
86d4 | [95] S. Nowlan and J. Platt, "A convolutional neural network hand tracker," in Advances in Neural Information Processing Systems 7, G. Tesauro, D. Touretzky, and T. Leen, Eds. San Mateo, CA: Morgan Kaufmann, 1995, pp. 901-908.
z7qp | [96] H. A. Rowley, S. Baluja, and T. Kanade, "Neural network-based
6t1g | face detection," in Proc. IEEE CVPR'96, pp. 203-208.
cpls | [98] H. Bourlard and C. J. Wellekens, "Links between Markov models and multilayer perceptrons," in Advances in Neural Information Processing Systems, D. Touretzky, Ed. Denver: Morgan-Kaufmann, vol. 1, 1989, pp. 186-187.
m50n | [99] Y. Bengio, R. De Mori, G. Flammia, and R. Kompe, "Neural network-Gaussian mixture hybrid for speech recognition or density estimation," in Advances in Neural Information Process- ing Systems 4, J. E. Moody, S. J. Hanson, and R. P. Lippmann, Eds. Denver, CO: Morgan Kaufmann, 1992, pp. 175-182.
eztf | [119] A. H. Kramer and A. Sangiovanni-Vincentelli, "Efficient par-
bwsu | During his time at the Université Pierre et Marie Curie, he proposed an early version of the back-propagation learning algorithm for neural networks. He joined the Department of Computer Science at the University of Toronto, Toronto, Ont., Canada, as a Research Associate in 1987. In 1988, he joined the Adaptive Systems Research Department at AT&T Bell Laboratories, Holmdel, NJ, where he worked on neural networks, machine learning, and handwriting recognition. In 1996 he became Head of the Image Processing Services Research Department at AT&T Labs-Research, Red Bank, NJ. He has published over 70 technical papers and book chapters on neural networks, machine learning, pattern recognition, handwriting recognition, document understanding, image processing, very large scale integration (VLSI) design, and information theory. In addition to the above topics, his current interests include video-based user interfaces, image compression, and content-based indexing of multimedia material.
oqod | Yoshua Bengio received the B.Eng. degree in electrical engineering and the M.Sc. and Ph.D. degrees in computer science from McGill Uni- versity, Montreal, P.Q., Canada, in 1986, 1988, and 1991, respectively.