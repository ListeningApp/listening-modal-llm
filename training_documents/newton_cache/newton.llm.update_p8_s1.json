{
  "id": "chatcmpl-A3UEOaQB56FdQ1qR5w8D5CgWBFlUD",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "usfq | Inexact Newton-type Methods for Optimisation with Nonnegativity Constraints\n4jnl | Four. Numerical Experiments\nows8 | We now compare the performance of our method for solving formula one with several alternatives using various convex and non-convex examples. Specifically, we consider Algorithm two (denoted by MR), projected Newton-CG (denoted by CG) as in Xie and Wright, and projected gradient with line search (denoted by PG). For convex problems, we also include FISTA with line search, while for non-convex settings, we compare against the proximal gradient with momentum and fine-tuned constant step size (denoted by PGM). We exclude proximal Newton methods due to the difficulty of solving its subproblems at each iteration. We also do not consider the Newton-CG log barrier method due to poor practical performance observed in Xie and Wright.\nqkak | For all applicable methods we terminate according to seven with epsilon underscore g equals ten to the power of negative eight. Instead of the highly implementation dependent \"wall-clock\" time, here we plot the objective value against the number of oracle calls, i.e., the number of equivalent function evaluations. For completeness, however, we also include plots of objective value against wall-clock time in Appendix F point Five. The PyTorch implementation for our experiments is available here. All experiments were performed on a GPU cluster. See Appendix F point Three for further experimental details.\n39ur | Four point one. Sparse Regularisation With L one Norm\nnlm3 | We first consider sparse regression using one-regularisation\nmuf5 | Minimise over x in R d f of x plus lambda times L one norm of x, twelve\nlqjf | where f is a smooth function. Although the objective function in twelve is nonsmooth, it can be reformulated into a smooth optimisation problem with nonnegativity constraints; see Appendix F point Two for details. We consider two examples in this context.\n7z7b | Multinomial Regression. In Figures one and two, we consider convex multinomial regression with C classes where f is given by sixty-two. The FISTA method is applied directly to twelve. While FISTA clearly outperforms the others, our method is competitive. Further simulations showing fast local convergence of our method on these examples are given in Appendix F point Four.\ntv8z | Neural Network. Figure three shows the results using a two layer neural network where f is non-convex and defined by sixty-three. Again, PGM is applied directly to twelve and its step size is fine-tuned for best performance. We once again observe superior performance of our method compared with the alternatives.\nwj6v | d equals twenty-seven thousand six hundred fifty-seven with lambda equals ten to the power of negative. Four point two. Nonnegative Matrix Factorisation\nax4d | Given a nonnegative data matrix Y in R plus n by m nonnegative matrix factorisation aims to produce two low rank, say r, nonnegative matrices W in R plus n by eta and H in R plus r by m such that Y approximately equals WH. This can be formulated as\ngtbn | Minimise over W greater than or equal to zero and H greater than or equal to zero D of Y, WH plus R lambda of W, H, thirteen\nui4d | where D of blank, blank is a 'distance' and R lambda of blank, blank is a regularisation term. In Figure four, we consider a text dataset and cosine similarity based distance function, while in Figure five, we use an image dataset and a Euclidean distance function with a nonconvex regulariser; see Appendix F point Three for details. Clearly, our method outperforms all others across both problems.\n2h4x | Five. Conclusions and Future Directions\n0s3f | We developed Newton-MR variants of the two-metric projection framework. By inexactly solving the subproblems using MINRES as well as employing non-positive curvature directions, our proposed variants are suitable for large scale and nonconvex settings. We demonstrated that, under\n1ajd | Inexact Newton-type Methods for Optimisation with Nonnegativity Constraints\nqj5b | certain assumptions, the convergence rates of our methods match the state-of-the-art and showcased competitive practical performance across a variety of problems.\n15wh | Possible avenues for future research include extensions to box constraints, variants with second-order complexity guarantees, and the development of stochastic algorithms.\nad3x | Impact Statement\n6zdx | This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.\nlwg2 | Inexact Newton-type Methods for Optimisation with Nonnegativity Constraints\nzsvp | Inexact Newton-type Methods for Optimisation with Nonnegativity Constraints\nssmj | Inexact Newton-type Methods for Optimisation with Nonnegativity Constraints\nnda1 | Inexact Newton-type Methods for Optimisation with Nonnegativity Constraints\n5mk0 | Inexact Newton-type Methods for Optimisation with Nonnegativity Constraints\n5bgd | A. MINRES and Newton-MR\nm3bl | In this section, for completeness, we discuss MINRES and provide some of its fundamental properties. We note that our presentation is essentially that of Liu and Roosta, Appendix A as the notation and implementation is well adapted to our setting. Recall that MINRES combines the Lanczos process, a QR decomposition, and an updating formula to iteratively solve a symmetric linear least-squares problem of the form\n7ul5 | minimize |Hs plus g|| squared two.\n1b57 | We now discuss each of these aspects in detail.\nrwom | Lanczos Process. Recall that, starting from v one equals g over ||g||, after t iterations of the Lanczos process, the Lanczos vectors {V one, V two, ..., V plus one}, form a basis for the Krylov subspace K plus one H, g. Collecting these vectors into an orthogonal matrix\nw1qt | V of t plus one equals [V one, ... V of t plus one] in R d x (t plus one)\nemp6 | we can write\nvm2i | H V of t equals V of t plus one times T t, where T t in R (t plus one), t is an upper Hessenberg matrix of the form\nu7au | T t equals ã one B two\n1ti5 | B two ã two\nvpp5 | , T, A B plus one e capital T.\ny9yd | T t\nrwaf | B three\nxbze | ten to the power of eighteen ten\np2wh | This relation yields the underlying update process of the MINRES iterations for t greater than or equal to two as,",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725394556,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 1410,
    "prompt_tokens": 3496,
    "total_tokens": 4906
  }
}