{
  "id": "chatcmpl-A0YVBI9LToTrRUeyggscKYOZMKL1V",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "```\nlrry | We first describe a building block dubbed CBHG, illustrated in Figure Two. CBHG consists of a bank of one-dimensional convolutional filters, followed by highway networks and a bidirectional gated recurrent unit recurrent neural net. CBHG is a powerful module for extracting representations from sequences. The input sequence is first convolved with K sets of one-dimensional convolutional filters, where the k-th set contains C sub k filters of width k (i.e. left k equals one, two, up to K). These filters explicitly model local and contextual information (akin to modeling unigrams, bigrams, up to K-grams). The convolution outputs are stacked together and further max pooled along time to increase local invariances. Note that we use a stride of one to preserve the original time resolution. We further pass the processed sequence to a few fixed-width one-dimensional convolutions, whose outputs are added with the original input sequence via residual connections. Batch normalization is used for all convolutional layers. The convolution outputs are fed into a multi-layer highway network to extract high-level features. Finally, we stack a bidirectional GRU RNN on top to extract sequential features from both forward and backward context. CBHG is inspired from work in machine translation, where the main differences include using non-causal convolutions, batch normalization, residual connections, and stride equals one max pooling. We found that these modifications improved generalization.\nhvhm | Three point two. ENCODER\n863b | The goal of the encoder is to extract robust sequential representations of text. The input to the encoder is a character sequence, where each character is represented as a one-hot vector and em-\nfj24 | bedded into a continuous vector. We then apply a set of non-linear transformations, collectively called a \"pre-net\", to each embedding. We use a bottleneck layer with dropout as the pre-net in this work, which helps convergence and improves generalization. A CBHG module transforms the pre-net outputs into the final encoder representation used by the attention module. We found that this CBHG-based encoder not only reduces overfitting, but also makes fewer mispronunciations than a standard multi-layer RNN encoder.\nuz3c | Three point three. DECODER\nd5bi | We use a content-based tanh attention decoder where a stateful recurrent layer produces the attention query at each decoder time step. We concatenate the context vector and the attention RNN cell output to form the input to the decoder RNNs. We use a stack of GRUs with vertical residual connections for the decoder. We found the residual connections speed up convergence. The decoder target is an important design choice. While we could directly predict raw spectrogram, it's a highly redundant representation for the purpose of learning alignment between speech signal and text (which is really the motivation of using sequence to sequence for this task). Because of this redundancy, we use a different target for sequence to sequence decoding and waveform synthesis. The sequence to sequence target can be highly compressed as long as it provides sufficient intelligibility and prosody information for an inversion process, which could be fixed or trained. We use eighty-band mel-scale spectrogram as the target, though fewer bands or more concise targets such as cepstrum could be used. We use a post-processing network to convert from the sequence to sequence target to waveform.\nba4l | We use a simple fully-connected output layer to predict the decoder targets. An important trick we discovered was predicting multiple, non-overlapping output frames at each decoder step. Predicting r frames at once divides the total number of decoder steps by r, which reduces model size, training time, and inference time. More importantly, we found this trick to substantially increase convergence speed, as measured by a much faster (and more stable) alignment learned from attention. This is likely because neighboring speech frames are correlated and each character usually corresponds to multiple frames. Emitting one frame at a time forces the model to attend to the same input token for multiple timesteps; emitting multiple frames allows the attention to move forward early in training. A similar trick is also used in Zen et al. but mainly to speed up inference.\nffx8 | The first decoder step is conditioned on an all-zero frame, which represents a \"GO\" frame. In inference, at decoder step t, the last frame of the r predictions is fed as input to the decoder at step t plus one. Note that feeding the last prediction is an ad-hoc choice here - we could use all r predictions. During training, we always feed every r-th ground truth frame to the decoder. The input frame is passed to a pre-net as is done in the encoder. Since we do not use techniques such as scheduled sampling, we found it to hurt audio quality, the dropout in the pre-net is critical for the model to generalize, as it provides a noise source to resolve the multiple modalities in the output distribution.\ns6su | Three point four. POST-PROCESSING NET AND WAVEFORM SYNTHESIS\ny219 | As mentioned above, the post-processing net's task is to convert the sequence to sequence target to a target that can be synthesized into waveforms. Since we use Griffin-Lim as the synthesizer, the post-processing net learns to predict spectral magnitude sampled on a linear-frequency scale. Another motivation of the post-processing net is that it can see the full decoded sequence. In contrast to sequence to sequence, which always runs from left to right, it has both forward and backward information to correct the prediction error for each individual frame. In this work, we use a CBHG module for the post-processing net, though a simpler architecture likely works as well. The concept of a post-processing network is highly general. It could be used to predict alternative targets such as vocoder parameters, or as a WaveNet-like neural vocoder that synthesizes waveform samples directly.\n06ht | We use the Griffin-Lim algorithm to synthesize waveform from the predicted spectrogram. We found that raising the predicted magnitudes by a power of one point two before feeding to Griffin-Lim reduces artifacts, likely due to its harmonic enhancement effect. We observed that Griffin-Lim converges after fifty iterations (in fact, about thirty iterations seems to be enough), which is reasonably fast. We implemented Griffin-Lim in TensorFlow hence it's also part of the model. While Griffin-Lim is differentiable (it does not have trainable weights), we do not impose any loss on it in this work. We emphasize that our choice of Griffin-Lim is for simplicity; while it already yields strong results, developing a fast and high-quality trainable spectrogram to waveform inverter is ongoing work.\nv3r2 | Four. MODEL DETAILS\n```",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1724695989,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_3aa7262c27",
  "usage": {
    "completion_tokens": 1367,
    "prompt_tokens": 3302,
    "total_tokens": 4669
  }
}