## Learning Factors Analysis - A General Method for Cognitive Model Evaluation and Improvement

Abstract. A cognitive model is a set of production rules or skills encoded in intelligent tutors to model how students solve problems. It is usually generated by brainstorming and iterative refinement between subject experts, cognitive scientists and programmers. In this paper we propose a semi-automated method for improving a cognitive model called Learning Factors Analysis that combines a statistical model, human expertise and a combinatorial search. We use this method to evaluate an existing cognitive model and to generate and evaluate alternative models. We present improved cognitive models and make suggestions for improving the intelligent tutor based on those models.

One Introduction

A cognitive model is a set of production rules or skills encoded in intelligent tutors to model how students solve problems. Productions embody the knowledge that students are trying to acquire, and allows the tutor to estimate each student's learning of each skill as the student works through the exercises.

A good cognitive model captures the fine knowledge components in a curriculum, provides tailored feedback and hints, select problem with difficulty level and learning pace matched to individual students, and eventually, improves student learning. However, initial models are usually generated by brainstorming and iterative refinement between subject experts, cognitive scientists and programmers. These first pass models are best guesses and our experience is that such models can be improved.

In this paper, we propose a method called Learning Factors Analysis and use it to answer three questions relevant to the field of intelligent tutoring systems.

One. How can we describe learning behavior in terms of an existing cognitive model? We need to identify the initial difficulty level of each production and how fast can a student learn each rule. We can then provide parameters that indicate student performance on this set of rules and how that performance improves with practice and instruction on those rules.

Two. How can we evaluate and improve a cognitive model in an inexpensive way? We need to identify the causes of the deviation from the deterministic cognitive model, define the measures of a model's complexity and fit, and mine the student-tutor log data.

Three. How can we use the information from Learning Factors Analysis to improve the tutor and the curriculum? We need to identify over-taught or under-taught rules, and even hidden knowledge components within them. As a result, we can adjust their contribution to curriculum length without compromising student performance.


## Two Literature Review

One measure of the performance of a cognitive model is how the data fit the model. Newell and Rosenbloom found a power relationship between the error rate of performance and the amount of practice. Depicted by equation one, the relationship shows that the error rate decreases according to a power function as the amount of practice increase. The curve for the equation is called a "learning curve".

One

Y equals A times X to the power of B

where

Y equals the error rate X equals the number of opportunities to practice a skill A equals the error rate on the first trial, reflecting the intrinsic difficulty of a skill B equals the learning rate, reflecting how easy a skill is to learn

The learning curve model has been used to visually identify non-obvious or hidden knowledge components. Corbett and Anderson observed that the power relationship might not be readily apparent in some complex skills, which have blips in their learning curves, as shown in figure two. They also found the power relationship holds if the complex skill can be decomposed into subskills, each of which exhibits a smoother learning curve.

As seen in the graphs above, the single production Declare-Parameter produces a learning curve with several blips. However by breaking it into two more specific productions, Declare-First-Parameter and Declare-Second-Parameter, the model becomes more fine-tuned and recognizes that the skills are different. The knowledge decomposition, considering parameter position, that was non-obvious from the original model became revealed on closer inspection of learning curve data.

Other approaches to model refinement include having a simulated student to find incorrect rules and to learn new rules via human tutor intervention, using theory refinement to introduce errors to models incorrect student behaviors and using Q-matrix to discover knowledge structure from student response data. Compared with the simulated student approach, our method does not require building a simulated student. The theory refinement approach starts with an initial knowledge base and keeps correcting errors in the knowledge base from error examples until the knowledge base is consistent with the examples. It may lead to overfit the examples. The Q-matrix approach was used to automatically extract features in the problem set. The model found by this approach may be similar to the model adding or merging difficulty factors in our method.


## Three The Cognitive Model and Its Data Under Investigation

We illustrate the Learning Factors Analysis methodology using data obtained from the Area Unit of the Geometry Cognitive Tutor. The initial cognitive model implemented in the Tutor had fifteen skills that correspond to productions or, in some cases, groups of productions. The productions are

Circle-area - Given the radius, find the area of a circle Circle-circumference - Given the diameter, find the circumference of a circle. Circle-diameter -- Given the radius or circumference, find the diameter of a circle. Circle-radius -- Find the radius given the area, circumference, or diameter. Compose-by-addition - In A plus B equals C, given any two of A, B, or C, find the third. Compose-by-multiplication - In A times B equals C, given any two of A, B, or C, find the third. Parallelogram-area - Given the base and height, find the area of a parallelogram. Parallelogram-side - Given the area and height or base, find the base or height. Pentagon-area - Given a side and the apothem, find the area of a pentagon. Pentagon-side - Given area and apothem, find the side or apothem.

Trapezoid-area - Given the height and both bases, find the area of a trapezoid. Trapezoid base - Given area and height, find the base of a trapezoid. Trapezoid-height - Given the area and the base, find the height of a trapezoid. Triangle-area - Given the base and height, find the area of a triangle. Triangle-side - Given the base and side, find the height of a triangle.

Our data consist of four thousand one hundred two data points involving twenty-four students, and one hundred fifteen problem steps. Each data point is a correct or incorrect student action corresponding to a single production execution. Table one displays typical student action records in this data set. It has five columns - student, success, step, skill, and opportunities. Student is the names of the students. Success is whether the student did that step correctly or not in the first attempt. One means success and zero, failure. Step is the particular step in a tutor problem the students are involved in. "Problem one step one" stands for problem one step one. Skill is the production rule used in that step. Opportunities mean the number of previous times to use a particular skill. It increments every time the skill is used by the same student, and can be computed from the first and fourth columns.


## Four Learning Factor Analysis

Learning Factors Analysis has three components: a statistical model that quantifies the skills, the difficulty factors that may affect student performance in the tutor curriculum, and a combinatorial search that does model selection.


## Four point one The Statistical Model

The power law model applies to individual skills and does not typically include student effects. Because the existing cognitive model has multiple rules, and the data contains multiple students, we made four assumptions about student learning to extend the power law model.

One. Different students may initially know more or less. Thus, we use an intercept parameter for each student.

Two. Students learn at the same rate. Thus, slope parameters do not depend on student. This is a simplifying assumption to reduce the number of parameters in equation two. We chose this simplification, following Draney, Wilson and Pirolli, because we are focused on refining the cognitive model rather than evaluating student knowledge growth.

Three. Some productions are more likely to be known than others. Thus, we use an intercept parameter for each production.

Four. Some productions are easier to learn than others. Thus, we need a slope parameter for each production.

Based on the assumptions, we developed a multiple logistic regression model.

In two equals sum of alpha chi plus sum of beta y plus sum of gamma y t.

Where

P equals the probability to get an item right

X equals the covariates for students

Y equals the covariates for skills

T equals the covariates for the number of opportunities practiced on the skills

Y T equals the covariates for interaction between skill and the number of practice opportunities for that skill

Alpha equals the coefficient for each student, i.e. the student intercept

Beta equals the coefficient for each rule, i.e. the production intercept

Gamma equals the coefficient for the interaction between a production and its opportunities, i.e. the production slope.


## Four point two. Difficulty Factors

A difficulty factor refers specifically to a property of the problem that causes student difficulties. By assessing the performance difference on pairs of problems that vary by one factor at a time, we can identify the hidden knowledge components that can be used to improve a cognitive model. Difficulty factors have been used to empirically evaluate a small number of alternative models.

In our study, subject experts identified four multi-valued factors for the Area Unit of the Geometry Tutor. Table two lists their names and values.

"Embed" indicates whether a shape is embedded in another shape. Consider two tutor problems requiring the same production rule CIRCLE-AREA at some step in the problem. In one of the problems, the circle is embedded in a square; while in the other one, the circle is presented alone. Students may find it harder to find the area of circle when it is embedded in another figure because extra effort is necessary to find the circle and its radius. "Backward" means whether the production rule to be used is in its backward form of a taught formula, or its forward form. The forward form of Compose-by-addition is S equals S one plus S two, and its backward form is S one equals S minus S two. "Repeat" indicates whether the production rule has been used previously in the same problem. "FigurePart" indicates the part of the figure in the geometry shape to be computed.


## Four point three. Combinatorial Search

The goal of the combinatorial search is to do model selection within the logistic regression model space. Difficulty factors are incorporated into an existing cognitive model through a model operator called Binary Split, which splits a skill with a factor value, and a skill without the factor value. For example, splitting production Circle-area by factor Embed with value alone leads to two productions: Circle-area with the factor value alone, and Circle-area with the factor value embed. Table three shows the data before and after a split with Embed.

Table three. The data before and after split. Factors are incorporated in column Skill. The opportunities change accordingly.

A star search is the combinatorial search algorithm in Learning Factors Analysis. It starts from an initial node, iteratively creates new adjoining nodes, explores them to reach a goal node. To limit the search space, it employs a heuristic to rank each node and visits the nodes in order of this heuristic estimate.

In our study, the initial node is the existing cognitive model. Its adjoining nodes are the new models created by splitting the model on the difficulty factors. We do not specify a model to be the goal state because the structure of the best model is unknown. We do specify the stopping criterion by setting the upper bound of the number of node expansions, for this paper to fifty node expansions per search.

The heuristic guiding the search is one of the two scoring functions for regression models, Akaike Information Criterion, and Bayesian Information Criterion. Each search is run twice, guided by a different heuristic each time. A good model captures sufficient variation in the data but is not overly complicated by balancing between model fit and complexity minimizing prediction risk. AIC and BIC are two estimators for prediction risk, and hence used as heuristics in the search.

AIC equals negative two times log likelihood plus two times number of parameters. BIC equals negative two times log likelihood plus number of parameters times number of observations.

Where log-likelihood measures the fit, and the number of parameters, which is the number of covariates in equation two, measures the complexity. Based on these two formulas, the lower the AIC or BIC, the better the balance between model fit and complexity. BIC puts a more severe penalty for complexity, leading to a smaller model than other methods.

A more interpretable metric for fit is Mean Absolute Deviance, the average of the absolute values of the differences between observed values and predicted values. We do not use it as a heuristic because it leads to over fitting. We include it as a measure of the improvement in the model fit.

Figure four illustrates A star search with AIC as the heuristic. The original model is evaluated and AIC is computed. The model is then split into a few new models by incorporating the factors. AICs are computed from each of the new model. A star selects the best one for the next model generation. A star does not always go down. It may go up to select a model to expand if all the new models have worse heuristic scores than a previous model has. After several expansions, it finds a best node with the lowest AIC value.


## Five. Experiments and Results

Five point one. Experiment one

This experiment addresses the question -- How can we describe learning behavior in terms of an existing cognitive model? Specifically, we want to find out the learning rate and initial difficulty level of each rule, and the initial performance of students, given the data. The question is answered by fitting the logistic regression model in equation two and getting the coefficients. The coefficient estimates for the skills and students, and the overall model statistics are summarized in table four.

The higher the intercept of the each skill, the lower the initial difficulty the skill has. The higher the slope of the each skill, the faster students learned the skill. Pentagon-area is the hardest skill with the intercept of negative two point one six. Parallelogram-area is the easiest skill with the intercept of two point one four. Three skills have small slopes close to


## Learning Factors Analysis - A General Method for Cognitive Model Evaluation

zero -- Compose-by-addition (negative point zero four) and Parallelogram-area (negative point zero one), Triangle-area (point zero three). Parallelogram-area was already mastered with an initial success probability point nine five. It appears that more practice on those skills does not lead to much learning gain. Interestingly, although PENTAGON-AREA is the hardest skill among all, it has the highest learning rate point four five, leading to bigger improvement with more practice.

The coefficients for students measure each student's overall performance. The higher the number, the better the student performed. The AIC, BIC and MAD statistics provide a baseline for evaluating alternative models discussed below.


## Five point two Experiment two

This experiment addresses the question -- How can we improve a cognitive model? The question is answered by running LFA on the data including the factors, and searching through the model space. The improved models by LFA with BIC are summarized in table five. The improved models by LFA with AIC is summarized in the interpretation.

LFA suggests better models, which make finer distinctions on some skills in the original model and identify which difficulty factors the subject experts thought would turn out to be psychologically important. All the better models found by AIC and BIC have better (i.e. lower) statistical scores than those of the original. For the best BIC model, its BIC is reduced by thirty-seven, and AIC by sixty-two. The fit of the new model, as measured by MAD, is reduced by point zero one two. The best AIC model reduces AIC by an even larger amount of eighty-three, and increases BIC by eighteen. Its MAD is reduced by point zero two.

The improved skills common to most of the better models are Compose-by-multiplication, Compose-by-addition, Circle-area, and Triangle-area. We will discuss a few examples here.

All the new models suggest splitting Compose-by-multiplication into two skills - Cmarea and CMsegment, making a distinction of the geometric quantity being multiplied. By examining the positions of these problems in the curriculum, CMarea at the forty-third step and CMsegement at the ninetieth. As seen in table six, although the final probability of CMarea is high point nine six, the initial probability of CMsegment is low point three two. This sudden drop in the success probability at later steps corresponds to a significant blip in the learning curve as illustrated in figure two. The distinction between different geometric quantities suggests treating the original skill differently. LFA successfully identified the blip without the need of visually inspecting learning curves.

The subject experts thought embedding a shape into another shape would increase the difficulty of a skill and identified a factor "Embed", hoping LFA could make a distinction on it. LFA split these two skills by Embed in all the top AIC models. The three probabilities of CAalone and CAembed are shown in table seven. Does Embed make find the circle area harder? Note that problems with CAembed are introduced later in the curriculum after students have had significant practice with CAalone, about the time CAalone has reached the average probability of point eight one. At this point, CAembed has an initial probability of point seven one, indicating an increase in difficulty.


## Five point three Experiment three

In experiment two, LFA improved the original model by splitting skills. Experiment three addresses model improvement even further -- Will some skills be better merged than if they are separate skills? Can LFA recover some elements of truth if we search from a merged model, given difficulty factors?

We merged some skills in the original model to remove some of the distinctions, which are represented as the difficulty factors. Circle-area and Circle-radius are merged into one skill Circle; Circle-circumference and Circle-diameter into Circle-CD; Parallelogram-area and Parallelogram-side into Parallelogram; Pentagon-area, and Pentagon-side into Pentagon; Trapezoid-area, Trapezoid-base, Trapezoid-height into Trapezoid. The new merged model has eight skills -- Circle, Crcle-CD, Compose-by-addition, Compose-by-multiplication, Parallelogram, Pentagon, Trapezoid, Triangle.

Then we substituted the original skill names with the new skill name in the data, ran LFA including the factors, and had the A star algorithm search through the model space. The improved models by LFA with BIC are summarized in table eight. The improved models by LFA with AIC are summarized in the interpretation.


## Learning Factors Analysis - A General Method for Cognitive Model Evaluation

LFA fully recovered three skills (Circle, Parallelogram, Triangle), suggesting the distinctions made in the original model are necessary. LFA partially recovered two skills (Triangle, Trapezoid), suggesting the some original distinctions are necessary and some are not. LFA did not recover one skill (Circle-CD), suggesting that the original distinctions might not be necessary. LFA recovered one skill (Pentagon) in a different way, suggesting the original distinction may not be as significant as the distinction caused by another factor. We discuss a few examples here.

In BIC model one, Circle is split into Circle*area, and Circle*radius. The other two BIC models and all the AIC models split it into Circle*backward, and Circle*forward, which are equivalent to Circle-AR *area, and Circle-AR*radius because of the one-to-one relationship between forward and area and between backward and radius. Thus, LFA fully recovers the Circle skills.

None of the models recovered Circle-CD. This suggests that it may not be necessary to have two separate skills for Circle-circumference and Circle-Diameter. It appears that once students learn the formula circumference equals n times diameter, they can fairly easily apply it in the forward or backward direction.

In one of the top AIC models, Pentagon is split into Pentagon*initial and Pentagon*repeat, instead of Pentagon*area and Pentagon*side. This suggests that the distinction between the first use of a Pentagon skill in a problem and later uses of that skill in the same problem may be more significant than the distinction between the area and the side. Usually repeated use of a skill in the same problem is easier than the original use. For instance, once a student makes the initial relatively difficult determination that the Pentagon formula is relevant to a problem and recalls it, he need only use it again and perform easier arithmetic in repeated opportunities in that same problem.


## Five point four Combining the Results from Experiment One, Two, Three

By combining the results from the three experiments, we can address question three -- How can we use LFA to improve the tutor and the curriculum by identifying over-taught or under-taught rules, and adjusting their contribution to curriculum length without compromising student performance?

Parallelogram-side has a high intercept (two point zero six) and a low slope (negative point zero one). Its initial success probability is point nine four and the average number of practices per student is fourteen point nine. Much practice spent on an easy skill is not a good use of student time. Reducing the amount of practice for this skill should save student time without compromising their performance. Trapezoid-height has a low intercept (negative one point five five), and a positive slope (point two seven). Its initial success probability is point two nine and the average number of practices per student is four point two. The final success probability is point six nine, far away from the level of mastery. More practice on this skill is needed for students to reach mastery.

The advantage of LFA goes even further. An original rule may have two split rules, each of which need decidedly different amounts of practice, because they have different initial difficulty and learning rates. However, students who have appeared to master the original rule in the curriculum before even reading the second split rule might not get enough practice on the second split rule. Compose-by-multiplication is such a case, as seen in table nine.

With final probability point nine two students seem to have mastered Compose-by-multimplication. However, the decomposition of the skill shows a different picture. CMarea does well with final probability point nine six. But CMsegment has final probability only point six zero and an average amount of practice less than two. The knowledge-tracing algorithm in the tutor may let the student go after he reaches the mastery on Compose-by-addition in the original model. But with the model found by LFA, the knowledge-tracing algorithm will be able to catch the weakness of students in acquiring CMsegment.


## Six Conclusions and Future Work

Learning Factors Analysis is a way to combine statistics, human expertise and combinatorial search to evaluate and improve a cognitive model. The system we have developed is implemented in Java and is able to evaluate a model in seconds and conduct a search evaluating hundreds of models in four to five hours. The statistics for each model are meaningful, and the new improved models have better statistical scores and are interpretable. We are planning to use the method for datasets from other tutors to discover its potential for model and tutor improvement.