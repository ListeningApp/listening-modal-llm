You will receive raw text from an OCR scan of a document or web page. Each line of input represents one textbox from the document. Your task is to faithfully reproduce the text for our text-to-speech engine so that it is easily read aloud.

Note that the text might not start at the beginning and paragraphs may be split acrosss multiple textboxes. Textboxes may start or end in the middle of a word which is acceptable: do not combine text between textboxes and leave broken words at the start and end of a textbox unchanged.

You will receive one textbox per-line in the format `id | Textbox content`. The output must be in the format `id | Clean content`.

RULES:
 • Remove inline citations and references e.g. "(Author, 2021)" or "(https://wikipedia.org/article.html)" or "[ECMOS 35b, 47, 49]".
 • Spell out numbers, dates, and chemical formulas for the TTS engine e.g. "-2.5" -> "negative two point five" or "Jan. 2020" -> "January twenty twenty".
 • Spell out LaTeX formulas for the TTS engine e.g. "<LATEX>p = 1</LATEX>" -> "P equals one".
 • Rejoin hyphenated word fragments within the same textbox e.g. "synth -esize" -> "synthesize".
 • Fix simple typos and OCR errors e.g. "O nce upon a 1ime." -> "Once upon a time.".
 • Delete nonsequiter textboxes that interrupt the middle of a sentence if they obviously do not belong.

Otherwise, the output should exactly match the input text:
 • Do not combine text across lines or attempt to fix words broken across lines.
 • Ensure each ID from the input is included in the output.
 • Make as few changes as possible while respecting the above rules.

EXAMPLE INPUT:
```
ccpt | stigate the effectiveness of psychedelics for ADHD symp- toms in adults.
0fpw | 2. Materia1 and methods
dqn8 | 2.1. Study design and participants
cop0 | The study employed a naturalistic design, assessing the experiences of participants at bas-,
lhkq | eline before they start, and at 2 and 4 weeks after initiation. The target population included adults with ADHD symptoms who had not been diag- nosed with ADHD before. To be included, participants needed to score on Conner's Adult ADHD Rating Scale (CAARS-S:SV). This cut-off was indicative of clinically elevated symptoms (Conners et al., 1999) (see section 2.3).
82ju | 2.2. Study procedure
y2qo | The online advertisement was placed on a website providing information about psychedelics (www.microdo sing.nl). Interested participants were redirected to information explaining the study rationale and procedure. The baseline survey took 20 min to complete. If the survey had not been completed after 24 h, a reminder was sent. Each of the surveys at the 2- and 4-week time points took about 15 min to complete. Par- ticipants were able to pause the surveys. Data collection occurred between Nov. 2020 and Jul. 2021. The study was approved by the Ethics Review Committee of Psy- chology and Neuroscience at Maastricht University (ERCPN- 215_05_11_2019_A1).
1piq | 2.3. Measures
7fqw | 2.3.1. Demographic information and history of substance use
rhaz | At baseline, demographic information was collected. History of substance use assessed experience with psyche- delics (i.e., ayahuasca, DMT, 5-MeO-DMT, LSD, novel lysergamides (e.g., 1P-LSD, ALD-52), psilocybin, salvia divinorum, ibogaine, and mescaline) in both full (psychedelic) doses and microdoses.
wak1 | 2.3.2. Psychiatric and physiological diagnoses
wtfz | Participants were asked whether they had a current diagnosis of a disorder. These answer options were chosen because most of the listed diagnoses are often reported to co-occur with ADHD (Kooij et al., 2019), or because these diagnoses were reported to be common in people who microdose (Fadiman and Korb, 2019). We 
3j2l | REVIEW ARTICLE Frontiers in Psychiatry | www.frontiersin.org
03k3 | 2
1k9e | constructed a variable Comorbidity alongside ADHD, differenti- ating respondents with and without a comorbid diagnosis alongside ADHD (<LATEX>p < . 0 0 1 =</LATEX> only ADHD or no ADHD diagnosis; <LATEX>p > . 0 1 =</LATEX> ADHD and at least one other diagnosis). We constructed a variable Medication use alongside microdosing, differentiating respondents who were and were not using conven- tiona1 ADHD medication alongside microdosing (<LATEX>0 =</LATEX> only microdosing; <LATEX>1 =</LATEX> conven-
```

EXAMPLE OUTPUT:
```
ccpt | stigate the effectiveness of psychedelics for ADHD symptoms in adults.
0fpw | Two. Material and methods
dqn8 | Two point one. Study design and participants
cop0 | The study employed a naturalistic design, assessing the experiences of participants at bas-,
lhkq | eline before they start, and at two and four weeks after initiation. The target population included adults with ADHD symptoms who had not been diagnosed with ADHD before. To be included, participants needed to score on Conner's Adult ADHD Rating Scale. This cut-off was indicative of clinically elevated symptoms (see section two point three).
82ju | Two point two. Study procedure
y2qo | The online advertisement was placed on a website providing information about psychedelics. Interested participants were redirected to information explaining the study rationale and procedure. The baseline survey took twenty minutes to complete. If the survey had not been completed after twenty-four hours, a reminder was sent. Each of the surveys at the two and four-week time points took about fifteen minutes to complete. Participants were able to pause the surveys. Data collection occurred between November twenty twenty and July twenty twenty-one. The study was approved by the Ethics Review Committee of Psychology and Neuroscience at Maastricht University.
1piq | Two point three. Measures
7fqw | Two point three point one. Demographic information and history of substance use
rhaz | At baseline, demographic information was collected. History of substance use assessed experience with psychedelics (i.e., ayahuasca, DMT, five-MeO-DMT, LSD, novel lysergamides (e.g., one-P-LSD, ALD-fifty-two), psilocybin, salvia divinorum, ibogaine, and mescaline) in both full (psychedelic) doses and microdoses.
wak1 | Two point three point two. Psychiatric and physiological diagnoses
wtfz | Participants were asked whether they had a current diagnosis of a disorder. These answer options were chosen because most of the listed diagnoses are often reported to co-occur with ADHD, or because these diagnoses were reported to be common in people who microdose. We
3j2l |
03k3 |
1k9e | constructed a variable Comorbidity alongside ADHD, differentiating respondents with and without a comorbid diagnosis alongside ADHD (P is less than point zero zero one equals only ADHD or no ADHD diagnosis; P is greater than point zero one equals ADHD and at least one other diagnosis). We constructed a variable Medication use alongside microdosing, differentiating respondents who were and were not using conventional ADHD medication alongside microdosing (zero equals only microdosing; one equals conven-
```

INPUT:
```
wqfv | StyleTTS: A Style-Based Generative Model for Natural and Diverse Text-to-Speech Synthesis
j30a | Abstract-Text-to-Speech (TTS) has recently seen great progress in synthesizing high-quality speech owing to the rapid development of parallel TTS systems. Yet producing speech with naturalistic prosodic variations, speaking styles, and emotional tones remains challenging. In addition, many existing parallel TTS models often struggle with identifying optimal monotonic alignments since speech and duration generation typically occur independently. Here, we propose StyleTTS, a style-based generative model for parallel TTS that can synthesize diverse speech with natural prosody from a reference speech utterance. Using our novel Trans- ferable Monotonic Aligner (TMA) and duration-invariant data augmentation, StyleTTS significantly outperforms other baseline models on both single and multi-speaker datasets in subjective tests of speech naturalness and synthesized speaker similarity. Through self-supervised learning, StyleTTS can generate speech with the same emotional and prosodic tone as the reference speech without needing explicit labels for these categories. In addition, when trained with a large number of speakers, our model can perform zero-shot speaker adaption. The source code and audio samples can be found on our demo page at https://styletts.github.io/.
7bdk | I. INTRODUCTION
fim0 | Text-to-speech (TTS), also known as speech synthesis, has made significant strides with the advent of deep learning, producing increasingly human-like synthetic speech [1], [2], [3]. However, synthesizing expressive speech that captures the full range of prosodic, temporal, and spectral features, also known as paralinguistic information, remains a challenge [4]. A single piece of text can be spoken in various ways, influenced by context, emotional tone, and a speaker's unique linguistic habits. Hence, TTS is fundamentally a one-to-many mapping problem requiring innovative approaches.
ojro | While several strategies have been proposed to address this, including methods based on variational inference [1], [5], [6], [7], flow-based modeling [1], [8], [9], controlling pitch, duration and energy [10], [11], and using external prosody encoder [12], [13], [14], the production of synthesized speech still falls short of real human speech. In particular, accurately modeling and incorporating different speakers' speaking styles and emotional tones poses a significant challenge.
l6t9 | Many attempts have been made to integrate style information into TTS models [12], [13], [15], [16]. These approaches are predominantly based on autoregressive models such as Tacotron. Non-autoregressive parallel TTS models, such as Fastspeech [17] and Glow-TTS [9], however, have several advantages over autoregressive models. These models generate speech in parallel, enabling fast speech synthesis, and they are also more robust to longer and out-of-distribution (OOD) utterances. Moreover, because phoneme duration, pitch, and energy are predicted in- dependently from speech, models such as FastSpeech2 [11] and
1ujy | FastPitch [18] allow fully controllable speech synthesis. At the same time, these models have limitations. They predominantly concentrate on speech synthesis from a single target speaker and often achieve multi-speaker extensions by concatenating speaker embeddings with the encoder output. Models that explore speech styles also incorporate styles by concatenating style vectors and phoneme embeddings as input to the decoder. [12], [13], [15], [16]. This approach may not capture the temporal variation of acoustic features in the target speech effectively. In contrast, the domain of style transfer introduces styles through conditional normalization methods like adaptive instance normalization (AdaIN) [19]. This technique has proven effective in neural style transfer [20], [21], [22], generative modeling [23], [24], [25], and neural image editing [26], [27]. Application of these methods in speech synthesis has not been extensively explored yet, restricted primarily to voice conversion and speaker adaptation [28], [29], [30], [31], [32].
qzor | The structure of parallel TTS models allows for the entire speech to be synthesized, presenting an opportunity to leverage the powerful AdaIN module for integrating generalized styles in diverse speech synthesis. Recent state-of-the-art models mostly employ the non-autoregressive parallel framework for TTS, but because they do not directly align the input text and speech like autoregressive models do, an external aligner such as the Montreal Forced Aligner [33] that is pre-trained on a large dataset is often required. Since the external aligner is not trained on the TTS data and objectives, the alignments are not optimally suited for the TTS task. Although training internal aligners alleviates some generalization problems [1], [9], [34], [35], overfitting can occur as the aligners are trained on a smaller TTS dataset with only a mel-reconstruction loss.
a0by | Here, we introduce StyleTTS, a model that addresses the aforementioned challenges of incorporating diverse speaking styles and learning a reliable monotonic aligner. StyleTTS incorporates style-based generative modeling into a parallel TTS framework to enable natural and expressive speech synthesis. It leverages AdaIN to integrate style vectors derived from reference audio, capturing the full spectrum of a speaker's stylistic features. This allows our model to synthesize speech that emulates the prosodic patterns and emotional tones in the reference audio. With various reference audios, we can synthesize the same text in different speaking styles, effectively realizing the one-to-many mapping that many TTS systems find challenging. In addition, our model employs a novel Transferable Monotonic Aligner (TMA) to find the optimal text alignment, aided by a novel duration-invariant data augmenta- tion scheme to produce naturalistic prosody robust to potentially suboptimal duration predictions. Our model's design is robust against the generalization problems of external aligners and
u1ky | overfitting problems that can be caused by internal aligners.
htm9 | This paper presents the following novel contributions: (i) the introduction of the Transferable Monotonic Aligner (TMA), a new transfer learning scheme that refines pre-trained text aligners for TTS tasks, (ii) a duration-invariant data augmen- tation method for improving prosody prediction, and (iii) a parallel TTS model that incorporates generalized speech styles for natural and expressive speech synthesis. Together, these contributions pave the way for advanced TTS technologies enhancing human-computer interactions.
8hul | II. METHODS
87vz | A. Proposed Framework
ym66 | Given t & T the input phonemes and x 6 X' an arbitrary reference mel-spectrogram, our goal is to train a system that generates the mel-spectrogram & E X' that corresponds to the speech of t and reflects the generalized speech styles of x. Generalized speech styles are defined as any characteristics in the reference audio x except the phonetic content [16], including but not limited to prosodic pattern, lexical stress, formants transition, speaking rate, and speaker identity. Our framework consists of eight modules that can be divided into three major categories: (i) speech generation modules that include the text encoder, style encoder, and decoder, (ii) TTS prediction modules that include the duration and prosody predictor, and (iii) utility modules only used during training that include the discriminator, text aligner, and pitch extractor. An overview of our framework is provided in Figure 1. We detail each of the modules below.
l0a3 | Text Encoder. The text encoder T transforms the phonemes t into a hidden representation htext = T(t). It consists of a 3-layer CNN followed by a bidirectional LSTM [36].
```

OUTPUT:
```
