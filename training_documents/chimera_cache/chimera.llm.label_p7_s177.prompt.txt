You will receive raw text from an OCR scan of a document or web page. Each line of input represents one textbox from the document.

Your task is to label each textbox with exactly one of the following labels:
  title: the primary document title, no more than one per document
  heading: the name of the following chapter or section (should never start in the middle of the previous paragraph)
  subheading: a secondary heading, often following another heading (should never start in the middle of the previous paragraph)
  body: main paragraph content: may start or end in the middle of a word, use context to follow paragraphs that are split across textboxes (if a textbox contains both heading and body text, label it "body")
  math: textbox primarily containing math
  imageDescription: standalone callout that only describes an image, chart, table, or similar (typically begins with "Table...", "Figure...", "Fig. ...", "Above...", "Left...", etc.)
  authors: names of contributors to this document
  institutions: list of universities, business, and other institutions affiliated with this document
  publisher: info about the publisher, or provided by the publisher, including copyright info
  pagenum: the page number
  headerOrFooter: boilerplate text page number, or redundant headings at the top/bottom of the page
  toc: the table of contents
  references: bibliographic references and resources
  acknowledgements: thanks and credits to contributors
  appendix: index, appendix, or any other end matter not part of the main thread (including related headings)
  table: tabular data, label, title, or subheading for a grid or list of data
  datum: numeric data or data label from a chart or list
  advertisement: promotional ads and marketing
  layout: non-content related to the layout of the page, e.g. sidebar navigation
  callout: pull quote or long block text that stands apart from the main text
  footnote: footnotes and endnotes near the bottom of the page
  keywords: list of keywords or tags
  url: web address or email
  other: any other text that does not belong to the main thread

Each line of input is prefixed with a textbox ID in the format `id | Text content`. The output must be in the format `id | label`.

Note that body text may be split across multiple textboxes, and textboxes may start or end in the middle of a sentence or word. Because the text was extracted from a web page or document, paragraphs of body text may be interrupted by multiple textboxes of redundant headers, footnotes, page numbers, tables, images, etc. For example, a real heading will never interrupt the middle of a sentence. Use context clues like this to carefully label each textbox.

EXAMPLE INPUT:
1pjs | Neuroscience Applied 1 (2022) 101012
o2kr | Contents lists available at ScienceDirect
v6sk | Neuroscience Applied
1wj6 | journal homepage: www.journals.elsevier.com/neuroscience-applied
936l | Research Articles
06yq | Microdosing with psychedelics to self-medicate for ADHD symptoms in adults: A naturalistic study
dpv6 | ARTICLE INFO
8r5j | ABSTRACT
2taz | Keywords: ADHD Microdosing Psychedelics Self-medication Well-being Time perception
o3ya | ADHD in adulthood is often overlooked, which negatively affects the individual's well-being. First-line phar- macological interventions are effective in many ADHD patients, relieving symptoms rapidly. However, there seems to be a proportion of individuals who discontinue, or fail to respond to these treatments. For these in- dividuals, alternative treatment options should be explored.
r5il | 1. Introduction
5gui | Attention deficit hyperactivity disorder (ADHD) is one of the most common developmental disorders worldwide. Prevalence research indicates that 2.6% of the adult population has persistent ADHD. ADHD in adults is often overlooked because of the high comorbidity rate and lack of knowledge about how ADHD is expressed in adulthood (Kooij et al.,
9d4o | Fig. 1. Flowchart of included participants for each time point.
82qc | 2019). In addition, ADHD is associated with deficits in various domains of cogni- tive functioning. Twenty-five percent of ADHD cases suffered from ADHD symptoms purely because of de- ficiencies in temporal processing (Sonuga-Barke et al., 2010).
ls7d | First-line ADHD treatments in adults mainly include pharmacological interventions to enhance dopaminergic and noradrenergic neurotrans- mission with stimulants. Overall, they have been proven to work effectively in adults with ADHD, inducing fast symptom relief and thereby enhancing the person's quality of life. In the
5esx | Table 1 Demographic information from respondents at baseline and the two and four- week time points.
t8vv | longer term, approximately twenty percent of ADHD patients discontinue their prescribed medication after six to nine months, thirty percent after one year, and half of them after two years.
xrd1 | 2. Material and methods
5746 | 2.1. Study design and participants
7toz | The study employed a naturalistic design, assessing the experiences of participants at baseline,
j346 | Neuroscience Applied 1 (2022)
m5ka | E.C.H.M. Haijen et al.
kx2c | REVIEW
v8uq | before they start MD on their own initiative, and at two and four weeks after MD initiation. The target population included adults diagnosed with ADHD and individuals who experienced ADHD symptoms to the extent that these interfered with their daily lives and who had not been diag- nosed with ADHD before. To be included in the analyses, participants needed to score above a cut-off point on at least one of the subscales of the Conner's Adult ADHD Rating Scale (CAARS-S:SV).
2zmw | Fig. 2. Mean scores of the CAARS-S:SV DSM-IV total symptoms T-scores at baseline (0W) and two (2W) and four weeks (4W) after MD (A) of the whole sample, and (B) per conventional ADHD medication use. Error bars represent mean error.
sdk3 | <LATEX>\lim _ { x ightarrow \infty } rac { 6 x ^ { 2 } + 1 0 0 } { 7 x ^ { 2 } - 1 0 } =</LATEX>
x972 | 2.2. Study procedure
k221 | Mean performance measures of the CAARS-

EXAMPLE OUTPUT:
1pjs | headerOrFooter
o2kr | publisher
v6sk | publisher
1wj6 | publisher
936l | headerOrFooter
06yq | title
dpv6 | other
8r5j | heading
2taz | keywords
o3ya | body
r5il | heading
5gui | body
9d4o | imageDescription
82qc | body
ls7d | body
5esx | imageDescription
t8vv | body
xrd1 | heading
5746 | subheading
7toz | body
j346 | headerOrFooter
m5ka | authors
kx2c | headerOrFooter
v8uq | body
2zmw | imageDescription
sdk3 | math
x972 | heading
k221 | body




ffx8 | <LATEX>\left[ \left( D - D / 2 f + 1 \right) M _ { a } \right. ,</LATEX> <LATEX>\left. D * M _ { a } \right]</LATEX> For any <LATEX>f \in F ,</LATEX> Chimera can scale to more micro-batches (i.e., <LATEX>\left. N > D \right)</LATEX> using the methods discussed in Section 3.5. For a given <LATEX>f ,</LATEX> Chimera incurs <LATEX>2 \left( D / f / 2 - 1 \right)</LATEX> bubbles, but has to maintain <LATEX>2 f</LATEX> model replicas and synchronize the gradients of 2f stages on each worker.
s6su | The larger the value of <LATEX>f ,</LATEX> the less bubbles (and the more balanced activations memory consumption), but also the higher gradient syn- chronization overhead and weights memory consumption. When <LATEX>f = Q ,</LATEX> Chimera degrades to pure data parallelism. Empirical results in Section 4.4 show that <LATEX>f > 1</LATEX> rarely brings more performance bene- fit on the models used for evaluation. Thus, <LATEX>f = 1</LATEX> (i.e., a combination of two pipelines) is the default setting for Chimera in this paper, unless otherwise stated. We expect that <LATEX>f > 1 ,</LATEX> whose features are summarized in Table 3, would further improve the performance for future deep models with deeper pipeline.
y219 | 4 EXPERIMENTAL EVALUATION
06ht | We conduct our experiments mainly on the CSCS Piz Daint super- computer. Each Cray XC50 compute node contains an Intel Xeon E5-2690 CPU, and one NVIDIA P100 GPU with 16 GB global mem- ory. The compute nodes of Piz Daint are connected by Cray Aries interconnect network in a Dragonfly topology.
v3r2 | We also evaluate the performance on a small cluster equipped with 32 NVIDIA V100 GPUs. The cluster has four GPU servers connected by Infiniband, and each server has eight V100 GPUS with NVLink. Each V100 GPU has 32 GB global memory.
nv42 | Figure 9: Memory consumption distribution among 32 GPU nodes of Piz Daint. The red and blue circles indicate the max- imum and minimum memory consumption, respectively.
xags | We evaluate the performance of the schemes listed in Table 2, which covers the state-of-the-art. For a fair comparison, all schemes are implemented in PyTorch [41] with GLOO [15] distributed back- end for both the point-to-point <LATEX>\left( p 2 p \right)</LATEX> communication between pipeline stages and gradient synchronization (allreduce) across the stage replicas, and GPU devices are utilized for acceleration. Al- though NCCL [40] backend of PyTorch performs better for allreduce
jc47 | Table 4: Neural networks used for evaluation.
ko3c | <LATEX>> = 5 1 2</LATEX> across GPU nodes (with GPUDirect RDMA), it does not support p2p communication. Using NCCL for gradient synchronization and GLOO for <LATEX>p 2 p</LATEX> at the same time fails, which is also observed in PipeDream [38]. We use the language models summarized in Ta- ble 4 for evaluation, and the max sequence length of Bert-48 and GPT-2 are set to 128 and 632 respectively, unless otherwise stated. The mini-batch size and sequence length we use are consistent with those in the machine learning community [13, 43, 56, 59]. Since Chimera is a synchronous approach without compromising conver- gence accuracy, we focus on the training throughput comparison.
xek3 | 4.1 Memory Consumption
bbpr | Figure 9 presents the memory consumption (including both activa- tions and weights) distribution among 32 GPU nodes of Piz Daint for Bert-48 and GPT-2 in different configurations. Since GPipe in- jects all the micro-batches at once, the high activation memory cost of it leads to OOM (Out of Memory) in all the configurations. PipeDream has the second highest memory consumption because up to D versions of weights have to be stashed. PipeDream-2BW reduces the stashed weights versions to 2. However, for language models, the first stage usually has more weights than other stages since it includes an extra embedding layer. Also, the pipeline sched- ule of PipeDream-2BW and DAPPLE determines that the activa- tion memory consumption on the first worker is the highest. This double imbalance causes that the first worker commonly has the peak memory consumption for both PipeDream-2BW and DAPPLE. Since PipeDream-2BW stashes two versions of weights, it incurs OOM as pipeline stages get coarser. In contrast, the schedule of bidirectional pipelines in Chimera determines that it has a more balanced memory consumption as shown in Figure 9, which is con- sistent with the analysis in Talbe 2. With the lowest activations memory cost occurring on the first (also the last) worker (see Fig- ure 2), the excessive weights of the first stage can be amortized in Chimera. Thus, although Chimera maintains two model replicas, it still has a little lower peak memory consumption than DAPPLE (the state-of-the-art synchronous approach which maintains one copy of the model) for four out of six configurations in Figure 9. Although GEMS achieves the lowest (and also balanced) memory consumption among all approaches, this is at the cost of loss of parallelism. Overall, Chimera is on par with the state-of-the-art for the peak memory consumption, with a more balanced usage among the workers. These results are consistent with our analysis in Table 2.
5zyf | 4.2 Parallel Scalability
9own | We first find the best configuration for each approach, and compare their best performance in the test of weak scaling.
bqfb | Figure 10: Performance tuning for the baselines for Bert-48 on 32 GPU nodes of Piz Daint. B=512, except for PipeDream uses the maximum B fitting in the memory. R denotes activation recomputation to avoid OOM. Star marks the best performance.
wwdt | 4.2.1 Performance Optimization Space for the Baselines. Given the mini-batch size B and the number of workers P, the best config- uration of B (micro-batch size), D ( pipeline stages), and W (the number of replicated pipelines) is not obvious a priori because of the trade-offs (i.e., computational efficiency and bubbles, allre- duce and p2p communication overhead). We search the space of the parameters (W, D, and B (for power-of-two)) to find the best performance for each baseline. The results for Bert-48 on 32 GPU nodes are presented in Figure 10.
mn1r | For synchronous baselines (such as GPipe and DAPPLE), the value of B affects both computational efficiency and the bubble ra- tio. The planner of DAPPLE [16] gives an answer for how to select the configuration of W and D based on the profiling information, but it is not clear for how to select the best B. From Figure 10 we can see the highest throughput of both DAPPLE and GPipe (with activation recomputation) is achieved by (W=8, D=4, B=4), under which they hit the sweet spot for the trade-off between p2p commu- nication overhead and allreduce communication overhead by (W=8, D=4), and the sweet spot for the trade-off between bubble ratio and computational efficiency by B=4 (and N=16). GEMS prefers a large B for high computational efficiency since a smaller B does not help a lot to reduce the bubble ratio, and therefore its best performance is achieved by (W=8, D=4, B=32).
5vqw | Asynchronous baselines (PipeDream-2BW and PipeDream) al- ways prefer the maximum B fitting in the device memory, since there is no bubble problem for them. Note that PipeDream conducts gradient synchronization across W pipelines after each backward pass on a micro-batch, thus its B is limited by the maximum B. Since the frequent gradient synchronization of PipeDream leads to high allreduce overhead, its best performance is achieved with a deeper pipeline than others, namely by (W=4, D=8, B=48). PipeDream- 2BW scales to large B by accumulating the gradients for more than D micro-batches (i.e., N>=D), and its best performance is achieved by (W=8, D=4, B=16) with activation recomputation.
meyw | For GPT-2, we present the performance tuning for each baseline by searching the parameter space in Figure 11.
2e62 | Figure 11: Performance tuning for the baselines for GPT-2 on 512 GPU nodes of Piz Daint. B=512, except for PipeDream uses the maximum B fitting in the device memory. R denotes activation recomputation to avoid OOM. Star marks the best performance.
75y3 | 4.2.2 Performance Modelling of Chimera. We first evaluate the performance of Chimera with different gradient synchronization strategies discussed in Section 3.2. We use eager-sync to denote
s2g2 | Figure 12: Throughput comparison between different gra- dient synchronization strategies of Chimera for Bert-48 on Piz Daint (D=4, B=8, and B scales from 256 to 1,024 as P scales from 16 to 64).
q1q2 | Figure 13: The practical and modelled throughput of Chimera on Piz Daint for Bert-48 on 32 GPU nodes with ₿=256, and GPT-2 on 512 GPU nodes with B=512.
cb7r | eager synchronization also conducted for the middle stages, and eager-sync-opt to denote eager synchronization not conducted for the middle stages. Results in Figure 12 show that eager-sync-opt achieves higher (e.g., 1.09x on 64 nodes) throughput than eager-sync. These empirical results support our claim in Section 3.2.
68xu | Figure 13 presents the practical training throughput of Chimera and the throughput predicted by the performance model (see Sec- tion 3.4). Note that since Chimera greatly alleviates the bubble problem, it greedily chooses the largest B that fits in the device memory. The performance model is mainly used to select the best configuration of W and D. Therefore, Chimera has a much smaller tuning space compared with the synchronous baselines. The error of the performance model (see Equation 1) is within 10% for both Bert-48 and GPT-2. For Bert-48, the performance model accurately selects the best configuration, i.e., W=8 and D=4. For GPT-2, the per- formance model selects W=16 and D=32, but the best performance is achieved by W=64 and D=8. However, the best performance is only 1.7% higher than the one selected by the model. The inaccurate prediction is mainly because our model may overestimate the cost
hd35 | of activation recomputation used with W=64 and D=8. Although these two configurations achieve very close performance for GPT-2, it is worth mentioning that D=8 works better when scaling to large mini-batches because of less computation and p2p communication overhead, while D=32 works better when scaling to more machines because of less gradient synchronization overhead.
2zpk | 4.2.3 Comparison with the Best Performance. Figures 14 and 15 present the results of weak scaling on Bert-48 and GPT-2, respec- tively. For all the baselines we present the best performance after searching the parameter space at different scales. Especially, to achieve the best performance, GPipe switches from D=8 to D=16 for GPT-2 on more than 512 GPU nodes. For Chimera, we present the practical throughput using the best configuration predicted by the performance model. The configuration used by each approach for the best performance is annotated in the legends of the figures.
epu7 | Figure 14: Weak scaling for Bert-48 on Piz Daint. As P scales from 16 to 64, B scales from 256 to 1,024, except for PipeDream whose B scales from 24 to 96.
7od1 | Figure 15: Weak scaling for GPT-2 on Piz Daint. As P scales from 512 to 2,048, B scales from 512 to 2,048, except for PipeDream whose B scales from 128 to 512.
0bqy | For both Bert-48 and GPT-2, Chimera outperforms all the base- lines at all scales. For Bert-48 on 64 nodes, Chimera outperforms PipeDream and PipeDream-2BW (asynchronous approaches) by 1.94x and 1.17x, respectively, and outperforms GPipe, GEMS, and DAPPLE (synchronous approaches) by 1.32x, 2.41x, and 1.19x, re- spectively. PipeDream frequently synchronizes the gradients after each backward pass, which compromises the training throughput.
b780 | PipeDream-2BW uses B=16 with recomputation to achieve the best performance. Although PipeDream-2BW does not have bubble problem, it may not have enough computation to fully overlap the gradient synchronization overhead. GEMS has the highest bubble ratio and therefore has lower throughput than the others. To achieve the best performance, GPipe and DAPPLE use B=4 to reduce the bubble ratio but at the cost of lower computational efficiency. In contrast, Chimera has low bubble ratio while using B=8 for higher computational efficiency, and therefore outperforms GPipe and DAPPLE.
ccmb | For GPT-2 on 2,048 nodes, Chimera outperforms PipeDream and PipeDream-2BW (asynchronous approaches) by 2.01x and 1.16x, respectively, and outperforms GPipe, GEMS, and DAPPLE (synchro- nous approaches) by 1.42x, 2.34x, and 1.38x, respectively. There are two major advantages of Chimera: (1) Chimera has a low bub- ble ratio; (2) benefiting from a balanced memory consumption (as discussed in Section 4.1), Chimera with D=32 fits in the de- vice memory without activation recomputation, while all other approaches except GEMS require recomputation. Chimera out- performs PipeDream-2BW mainly because no recomputation is required, and outperforms GPipe and DAPPLE because of both less bubbles and no recomputation. Using 512 nodes as the base- line, Chimera achieves 91.4% parallel efficiency on 2,048 nodes in weak scaling for GPT-2, which demonstrates the efficiency of the communication scheme used in Chimera.
ma93 | Note that we use the same model partition method as the default setting in PipeDream-2BW, namely evenly partitioning the basic layers among the workers. Other model partition methods trying to balance the weights among the workers may help to reduce the peak memory consumption of PipeDream-2BW, but this is outside the scope of this paper. Generally, Chimera is on-par with PipeDream-2BW (the latest asynchronous approach) in terms of training throughput, but more convergence-friendly since there is no stale weights in Chimera.