You will receive raw text from an OCR scan of a document or web page. Each line of input represents one textbox from the document. Your task is to faithfully reproduce the text for our text-to-speech engine so that it is easily read aloud.

Note that the text might not start at the beginning and paragraphs may be split acrosss multiple textboxes. Textboxes may start or end in the middle of a word which is acceptable: do not combine text between textboxes and leave broken words at the start and end of a textbox unchanged.

You will receive one textbox per-line in the format `id | Textbox content`. The output must be in the format `id | Clean content`.

RULES:
 • Remove inline citations and references e.g. "(Author, 2021)" or "(https://wikipedia.org/article.html)" or "[ECMOS 35b, 47, 49]".
 • Spell out numbers, dates, and chemical formulas for the TTS engine e.g. "-2.5" -> "negative two point five" or "Jan. 2020" -> "January twenty twenty".
 • Spell out LaTeX formulas for the TTS engine e.g. "<LATEX>p = 1</LATEX>" -> "P equals one".
 • Rejoin hyphenated word fragments within the same textbox e.g. "synth -esize" -> "synthesize".
 • Fix simple typos and OCR errors e.g. "O nce upon a 1ime." -> "Once upon a time.".
 • Delete nonsequiter textboxes that interrupt the middle of a sentence if they obviously do not belong.

Otherwise, the output should exactly match the input text:
 • Do not combine text across lines or attempt to fix words broken across lines.
 • Ensure each ID from the input is included in the output.
 • Make as few changes as possible while respecting the above rules.

EXAMPLE INPUT:
```
ccpt | stigate the effectiveness of psychedelics for ADHD symp- toms in adults.
0fpw | 2. Materia1 and methods
dqn8 | 2.1. Study design and participants
cop0 | The study employed a naturalistic design, assessing the experiences of participants at bas-,
lhkq | eline before they start, and at 2 and 4 weeks after initiation. The target population included adults with ADHD symptoms who had not been diag- nosed with ADHD before. To be included, participants needed to score on Conner's Adult ADHD Rating Scale (CAARS-S:SV). This cut-off was indicative of clinically elevated symptoms (Conners et al., 1999) (see section 2.3).
82ju | 2.2. Study procedure
y2qo | The online advertisement was placed on a website providing information about psychedelics (www.microdo sing.nl). Interested participants were redirected to information explaining the study rationale and procedure. The baseline survey took 20 min to complete. If the survey had not been completed after 24 h, a reminder was sent. Each of the surveys at the 2- and 4-week time points took about 15 min to complete. Par- ticipants were able to pause the surveys. Data collection occurred between Nov. 2020 and Jul. 2021. The study was approved by the Ethics Review Committee of Psy- chology and Neuroscience at Maastricht University (ERCPN- 215_05_11_2019_A1).
1piq | 2.3. Measures
7fqw | 2.3.1. Demographic information and history of substance use
rhaz | At baseline, demographic information was collected. History of substance use assessed experience with psyche- delics (i.e., ayahuasca, DMT, 5-MeO-DMT, LSD, novel lysergamides (e.g., 1P-LSD, ALD-52), psilocybin, salvia divinorum, ibogaine, and mescaline) in both full (psychedelic) doses and microdoses.
wak1 | 2.3.2. Psychiatric and physiological diagnoses
wtfz | Participants were asked whether they had a current diagnosis of a disorder. These answer options were chosen because most of the listed diagnoses are often reported to co-occur with ADHD (Kooij et al., 2019), or because these diagnoses were reported to be common in people who microdose (Fadiman and Korb, 2019). We 
3j2l | REVIEW ARTICLE Frontiers in Psychiatry | www.frontiersin.org
03k3 | 2
1k9e | constructed a variable Comorbidity alongside ADHD, differenti- ating respondents with and without a comorbid diagnosis alongside ADHD (<LATEX>p < . 0 0 1 =</LATEX> only ADHD or no ADHD diagnosis; <LATEX>p > . 0 1 =</LATEX> ADHD and at least one other diagnosis). We constructed a variable Medication use alongside microdosing, differentiating respondents who were and were not using conven- tiona1 ADHD medication alongside microdosing (<LATEX>0 =</LATEX> only microdosing; <LATEX>1 =</LATEX> conven-
```

EXAMPLE OUTPUT:
```
ccpt | stigate the effectiveness of psychedelics for ADHD symptoms in adults.
0fpw | Two. Material and methods
dqn8 | Two point one. Study design and participants
cop0 | The study employed a naturalistic design, assessing the experiences of participants at bas-,
lhkq | eline before they start, and at two and four weeks after initiation. The target population included adults with ADHD symptoms who had not been diagnosed with ADHD before. To be included, participants needed to score on Conner's Adult ADHD Rating Scale. This cut-off was indicative of clinically elevated symptoms (see section two point three).
82ju | Two point two. Study procedure
y2qo | The online advertisement was placed on a website providing information about psychedelics. Interested participants were redirected to information explaining the study rationale and procedure. The baseline survey took twenty minutes to complete. If the survey had not been completed after twenty-four hours, a reminder was sent. Each of the surveys at the two and four-week time points took about fifteen minutes to complete. Participants were able to pause the surveys. Data collection occurred between November twenty twenty and July twenty twenty-one. The study was approved by the Ethics Review Committee of Psychology and Neuroscience at Maastricht University.
1piq | Two point three. Measures
7fqw | Two point three point one. Demographic information and history of substance use
rhaz | At baseline, demographic information was collected. History of substance use assessed experience with psychedelics (i.e., ayahuasca, DMT, five-MeO-DMT, LSD, novel lysergamides (e.g., one-P-LSD, ALD-fifty-two), psilocybin, salvia divinorum, ibogaine, and mescaline) in both full (psychedelic) doses and microdoses.
wak1 | Two point three point two. Psychiatric and physiological diagnoses
wtfz | Participants were asked whether they had a current diagnosis of a disorder. These answer options were chosen because most of the listed diagnoses are often reported to co-occur with ADHD, or because these diagnoses were reported to be common in people who microdose. We
3j2l |
03k3 |
1k9e | constructed a variable Comorbidity alongside ADHD, differentiating respondents with and without a comorbid diagnosis alongside ADHD (P is less than point zero zero one equals only ADHD or no ADHD diagnosis; P is greater than point zero one equals ADHD and at least one other diagnosis). We constructed a variable Medication use alongside microdosing, differentiating respondents who were and were not using conventional ADHD medication alongside microdosing (zero equals only microdosing; one equals conven-
```

INPUT:
```
q1q2 | LLMs, Turing Tests and Chinese Rooms: the prospects for meaning in Large Language Models
cb7r | Discussions of artificial intelligence and computational systems in philosophy of mind (and elsewhere) have been shaped to a large extent by two brilliant and highly influential thought- experiments: Alan Turing's Imitation Test for thinking systems and John Searle's Chinese Room Argument (these two thought experiments are universally well-known, but just for sake of completeness I'll sketch them very briefly in §1). As is extremely well-known, Turing argued for a behavioural test for intelligence. Searle, on the other hand, argued that passing a purely behavioural test, focused at the level of inputs and outputs, could never guarantee that meaning, understanding, or thinking was occurring in a system, since a purely formal, syntactic capacity could underpin the behaviour. In many ways, recent debates about large language models (LLMs) are struggling to move beyond the divide marked by these original, opposing thought- experiments. Thanks to the open access status of some large language models (systems like ChatGPT and Bard), we all now have, available on any suitable laptop or phone, an extraordinarily powerful example of a system which (at least within certain parameters) is capable of passing a Turing Test. Yet those who feel swayed by Searle's Chinese Room are likely to remain unconvinced that the incredible behavioural prowess displayed by these systems is sufficient to show that LLMs deserve the attribution of any richer predicates.
68xu | One way to move this debate forward, stressed in several places already (xxx), is to 'look under the hood' of AI systems, asking exactly what (if anything) the elements they manipulate represent and what features are relevant to the computations they perform. While this is certainly an important strategy, and I'll touch on it below, in this paper I want to pursue a different route, exploring the features those sceptical about LLMs might take to ground meaning. In §1 I set out some common ground between Believers and Sceptics, concerning how LLMs work and the kind of features a Sceptic is likely to demand from an LLM before she would be willing to treat it as a semantic engine. In §§2-3, I explore the first potential ground of meaning, which requires a robust relation between lingsuistic signs and external objects or states of affairs. In section 3 I argue that concerns about worldly connections can be met, so that the outputs of LLMs should be viewed as genuinely meaningful. Then in (§4) I turn to the idea that LLMs require original intentionality before we admit them to the space of meaning. I suggest that this demand is not a prerequisite for meaning per se, but rather for agency or conscious understanding. I'll suggest
hd35 | that the demands for original intentionality are not met by LLMs, but that nor should we want them to be.
2zpk | Turing Tests, Chinese Rooms and LLMs
epu7 | Ideas generated in philosophy of mind usually remain within the confines of the ivory tower where they were born, but this is certainly not true of Turing Tests and possibly not true of Chinese Rooms. Still, just to ensure everything is on the table, in this section I'll recap very briefly on the two thought-experiments which have done so much to shape understanding in this field.
7od1 | Alan Turing proposed what he called 'the imitation game' in his 1950 paper 'Computing machinery and intelligence' as an operationalised method to replace the question "Can machines think?". Turing's starting point was the recognition that terms like "thinking" and "intelligence" are difficult to define and are prone to yield divergent interpretations in different contexts. Instead, then, he proposed replacing questions framed in these vague terms with an investigation into a much more tractable question: "Can a machine pass the imitation test?". In the imitation test, a human subject poses questions to an unknown target, via text, and receives answers in the same way. If, given a reasonable length of conversation, the human subject is unable to tell whether they are interacting with another human or with a machine, then, if they are in fact interacting with a machine, that machine is judged to pass the imitation test. While Turing doesn't in fact say very much about the repercussions of passing such a test, the suggestion seems to be that, for all intents and purposes, a successful machine would qualify as a thinking thing. The only other option, Turing seems to think, would be adopting some kind of solipsistic condition on thinking, requiring first-personal conscious awareness, so that we judge thinking to be going on only when we 'know it from the inside'. However, as Turing 1950: ?? points out, this solipsistic perspective is not the one we adopt in everyday life, instead he notes that "it is usual to have the polite convention that everyone thinks". A similar courtesy, it seems, should be extended to behaviourally adept machines.
0bqy | In Searle's famous thought experiment from 1980, however, behavioural success of the kind envisaged by Turing is held to be insufficient for attribution of rich predicates like thinking or meaning to a system. For Searle objects that success at the level of outputs could come from a system which did nothing more than manipulate symbols in terms of their formal, syntactic properties, with no consideration of semantic content, or meaning, occurring within the system.
b780 | To see this, he asks us to consider a sealed room with a human subject in it. The subject receives what appear to her to be meaningless marks via an input slot. She then uses a giant look-up table to pair the incoming marks with a further set of marks and she outputs this further set of apparently meaningless marks via an output slot. Unbeknownst to the subject, however, the incoming symbols are questions in Chinese and the series of marks which she outputs constitute answers to these questions (again, in Chinese).1 Searle asks us to consider whether the individual in the room knows or understands Chinese and the intuitive answer, despite her impressive performance with Chinese questions, is that she does not. Yet, Searle contends, as for the individual in the Chinese Room, so for the computer. Both systems are purely syntactic engines with no semantic properties in play.
ccmb | So, what are we to say of LLMs? All parties agree that Large Language Models are trained for the task of minimizing prediction errors when deciding on the most probable next word in a sequence. Crudely, faced with the sentence "Emily paid in her cheques at the _", the task of an LLM is to fill in the blank with the most probable next word, e.g. completing this sentence with the expression "bank".2 In order to carry out this task, LLMs utilise what is known as transformer architecture (Vaswani et al 2017), which allows expressions of a natural language to be mapped into vectors within a multidimensional vector space. During pre-training, the system is provided with an almost unimaginably huge dataset (hundreds of billions to trillions of token words) which it uses to learn the statistical relationships between linguistic items in the database. The system does this by repeatedly masking words in sentences within its database and 'making guesses' (initially entirely randomly) about what the masked word is. Once the correct word is revealed the system then recalibrates the vectors it assigns to words in light of this information. The vector assigned to each word is thus determined by the statistical correlations between expressions across the entire training data. Following this unsupervised learning, there is then a period of human feedback which helps the system fine-tune its guesses for new input strings and then recalibrate resulting vector weights. Finally, vectors are also moderated by the correlations in the input strings themselves, so that, e.g., the vector assigned to a token of "dog" will differ dependent on whether the input string to the LLM is "The big dog ... " or "The barking dog ... ", etc. (see Jurafsky and Martin 2023 for discussion).
```

OUTPUT:
```
