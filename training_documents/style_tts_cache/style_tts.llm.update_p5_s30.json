{
  "id": "chatcmpl-A0Zlc3Du9kk4uYpn86Ri1EJEQWCFG",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "```\nap6l | We performed two subjective evaluations: mean opinion score of naturalness to measure the naturalness of synthesized speech, and mean opinion score of similarity to evaluate the similarity between synthesized speech and reference for the multi-speaker model. We recruited native English speakers located in the United States to participate in the evaluations on Amazon Mechanical Turk. In every experiment, we randomly selected one hundred text samples from the test set. For each text, we synthesized speech using our model and the baseline models and included the ground truth for comparison. The baseline models include Tacotron two, FastSpeech two, and VITS. For zero-shot speaker adaptation experiments, we compared our model with StyleSpeech and YourTTS. All baseline models are pre-trained and publicly available. The generated mel-spectrograms were converted into waveforms using the Hifi-GAN vocoder for all models. Each set of speech was rated by ten raters on a scale from one to five with zero point five point increments. For a fair comparison, we downsampled our synthesized audio into twenty-two kilohertz to match those from baseline models. We used random references when synthesizing speech for the single-speaker and zero-shot speaker adaptation experiments. For multi-speaker models, because our training did not require speaker labels, for a fair comparison with other models that use explicit speaker embeddings during training, we averaged the style vectors computed using all samples in the training set from the same speaker as the reference style.\niest | When evaluating each set, we randomly permuted the order of the models and instructed the subjects to listen and rate them without telling them the model labels. It is similar to multiple stimuli with hidden reference and anchor, allowing the subjects to compare subtle differences among models. We used the ground truth as hidden attention checkers: raters were dropped from analysis if the mean opinion score of the ground truth was not ranked top two among all the models.\ncox8 | We also conducted objective evaluations using ASR metrics. We evaluated the robustness of the models to different lengths of text input. We created four test sets with text length L less than ten, ten less than or equal to L less than fifty, fifty less than or equal to L less than one hundred, and one hundred less than or equal to L, respectively. Each set contains one hundred texts sampled from the WSJ zero dataset. We calculated the word error rate of the synthesized speech from both single-speaker and multi-speaker models using a pre-trained ASR model from ESPnet. To measure the inference speed, we computed the real-time factor,\nppqi | which denotes the time in seconds needed for the model to synthesize a one-second waveform. Real-time factor was measured on a server with one NVIDIA twenty eighty Ti GPU and a batch size of one. In addition, we conducted the same analysis on the correlations of acoustic features associated with emotions between reference and synthesized speech using four multi-speaker models. Since there is no style in FastSpeech two and VITS, we used a pre-trained X-vector model from Kaldi to extract the speaker embedding as the reference vector.\noxou | Four. Results\n495n | A. Model Performance.\nawri | Tables one, two, and three showcase the results of human subjective evaluations on the LJSpeech and LibriTTS datasets. When assessed for naturalness and similarity, StyleTTS clearly outperforms other models, demonstrating its superior performance under both single-speaker, multi-speaker, and zero-shot settings. Our models are more robust compared to other models, especially for long input texts. Since we do not use generative flows that require inverse Jacobian\nbl28 | computation, our model is faster than VITS, even though it was not trained end-to-end like VITS.\nd5bz | We do note that our evaluation results differ from those reported in the baseline models, particularly for VITS. The VITS model has been reported to yield results very close to the ground truth. However, in our evaluation, VITS was found not to reach ground truth levels of performance. The primary factor leading to this discrepancy is the difference in evaluation methods. In VITS experiments, the traditional Mean Opinion Score evaluation was used, where raters evaluated each module individually without any reference. The use of a reference point in our MUSHRA-like evaluation provides an anchor for rating, particularly the ground truth as the reference, which potentially lowers the scores of other models. A similar discrepancy has been reported in a very recent study that examines the effects of evaluation methods on the mean opinion score results, and our evaluation of VITS is comparable to other studies that have tried to reproduce it on both LJSpeech and LibriTTS datasets.\nap5o | B. Visualization of Style Vectors\n7zvf | To verify that our model can learn meaningful style representations, we projected the style vectors extracted from reference audios into a two-dimensional plane for visualization using t-SNE. We selected fifty samples of each emotion from a single speaker in ESD and projected the style vectors of each audio into the two-dimensional space. It can be seen in Figure two part a that our style vector distinctively encodes the emotional tones of reference sentences even though the training does not use emotion labels. We also computed the style vectors using speech samples from the same speaker with our single-speaker model. This model is only\n```",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1724700852,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_a2ff031fb5",
  "usage": {
    "completion_tokens": 1089,
    "prompt_tokens": 2853,
    "total_tokens": 3942
  }
}