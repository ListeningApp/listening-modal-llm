{
  "id": "chatcmpl-A0ZgTkYPQyiCqp7sAEpshYr0cxEet",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "```\nktv2 | It's a kind of average of A where each value A(i) is 'weighted', i.e. multiplied, by the probability of the ith outcome. We saw an example in the last tweet: the expected amount of rainfall.\nwoy6 | We've seen that if you believe the ith outcome has probability pi, the amount of information you learn if the ith outcome actually occurs is negative log pi. Thus, the expected amount of information you learn is\n6xev | negative log P equals negative pi log pi.\nu2ud | And this is the Shannon entropy! We denote it by H, or more precisely H (p), so\n3qum | H (p) equals negative pi log pi. In the box above I was taking X to be the set {one, ... , n}. This is often a good thing to do when there are finitely many outcomes.\nobst | Let's get to know the Shannon entropy a little better.\n3mg9 | Puzzle twenty-one. Let X equals one, two so that we know a probability distribution P on X if we know P sub one, since P sub two equals one minus P sub one. Graph the Shannon entropy H of P as a function of P sub one. Show that it has a maximum at P sub one equals one-half and minima at P sub one equals zero and\n4j8f | P sub one equals one. This makes sense: if you believe P sub one equals one then you learn nothing when an outcome happens chosen according to the probability distribution P: you are sure outcome one will occur, and it does (with probability one). Similarly, if you believe P sub one equals zero you learn nothing when an outcome happens according to this probability distribution, since you are sure outcome two will occur. On the other hand, if P sub one equals one-half, you are maximally undecided about what will happens, and you learn one bit of information when it does.\ne3ba | Puzzle twenty-two. Let X equals one, two, three. Draw the set of probability distributions on X as an equilateral triangle whose corners are the probability distributions (one, zero, zero), (zero, one, zero), and (zero, zero, one). Sketch contour lines of H of P as a function on this triangle. Show it has a maximum at P equals (one-third, one-third, one-third) and minima at the corners of the triangle.\n8c0j | Again this should make intuitive sense. Here is a harder puzzle along the same lines:\nurfd | Puzzle twenty-three. Let X equals { one, to, n }. Show that H of P has a maximum at P equals (one over n, to, one over n) and minima at the probability distributions where P sub i equals one for some particular\nwhtg | i in X. Here is one of the big lessons from all this:\n2u5k | Shannon entropy is larger for probability distributions that are more spread out, and smaller for probability distributions that are more sharply peaked.\nf8wg | Indeed, you can think of Shannon entropy as a measure of how spread out a probability distribution is! The more spread out it is, the more you learn when an outcome occurs, drawn from that distribution.\nhgz1 | THE PRINCIPLE OF MAXIMUM ENTROPY\nkhf5 | Suppose there are n possible outcomes. At first you have no reason to think any is more probable than any other.\nya4d | Then you learn some facts about the correct probability distribution-but not enough to determine it uniquely! Which probability distribution P one, to, P n should you choose?\ndzub | The principle of maximum entropy says:\noidq | H equals negative sum from i equals one to n Pi log pi.\nfp0u | What's Shannon entropy good for? For starters, it gives a principle for choosing the 'best' probability distribution consistent with what you know. Choose the one that maximizes the Shannon entropy!\n34lv | This is called the 'principle of maximum entropy'. This principle first arose in statistical mechanics, which is the application of probability theory to physics-but we can use it elsewhere too.\ngmoh | For example: suppose you have a die with faces numbered one, two, three, four, five, six. At first you think it's fair. But then you somehow learn that the average of the numbers that comes up when you roll it is five. Given this, what's the probability that if you roll it, a six comes up?\n2xvo | Sounds like an unfair question! But you can figure out the probability distribution on {one, two, three, four, five, six} that maximizes Shannon entropy subject to the constraint that the mean is five. According to the principle of maximum entropy, you should use this to answer my question!\nfhh2 | But is this correct?\nc2q0 | The problem is figuring out what 'correct' means! But in statistical mechanics we use the principle of maximum entropy all the time, and it seems to work well. The brilliance of E. T. Jaynes was to realize it's a general principle of reasoning, not just for physics.\n4wwg | The principle of maximum entropy is widely used outside physics, though still controversial. But I think we should use it to figure out some basic properties of a gas-like its energy or entropy per molecule, as a function of pressure and temperature.\nix7t | To do this, we should generalize Shannon entropy to 'Gibbs entropy', replacing the sum by an integral. Or else we should 'discretize' the gas, assuming each molecule has a finite set of states. It sort of depends on whether you prefer calculus or programming. Either approach is okay if we study our gas using classical statistical mechanics.\na46e | Quantum statistical mechanics gives a more accurate answer. It uses a more general\nozpq | definition of entropy-but the principle of maximum entropy still applies!\n22sy | I won't dive into any calculations just yet. Before doing a gas, we should do some simpler examples-like the die whose average roll is five. But I can't resisting mentioning one philosophical point. In the box above I was hinting that maximum entropy works when your 'prior' is uniform:\nkuwj | Suppose there are n possible outcomes. At first you have no reason to think any is more probable than any other.\n```",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1724700533,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_3aa7262c27",
  "usage": {
    "completion_tokens": 1308,
    "prompt_tokens": 3361,
    "total_tokens": 4669
  }
}