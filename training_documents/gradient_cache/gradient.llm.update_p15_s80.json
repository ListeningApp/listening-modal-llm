{
  "id": "chatcmpl-A3U7QIhYe2b6o7dSIpF0bABIvaVdf",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "dndq | on the regular test set, with a computational cost of only six hundred fifty thousand multiply-adds per recognition, i.e., only about sixty percent more expensive than LeNet-five.\nxzce | D. Discussion\nw4vd | A summary of the performance of the classifiers is shown in Figs. nine through twelve. Fig. nine shows the raw error rate of the classifiers on the ten thousand example test set. Boosted LeNet-four performed best, achieving a score of zero point seven percent, closely followed by LeNet-five at zero point eight percent.\ncuh0 | the methods. Patterns are rejected when the value of the corresponding output is smaller than a predefined threshold. In many applications, rejection performance is more significant than raw error rate. The score used to decide upon the rejection of a pattern was the difference between the scores of the top two classes. Again, Boosted LeNet-four has the best performance. The enhanced versions of LeNet-four did better than the original LeNet-four, even though the raw accuracies were identical.\nk36t | Training time was also measured. K-NN's and tangent distance classifier have essentially zero training time. While the single-layer net, the pairwise net, and PCA+quadratic net could be trained in less than an hour, the multilayer net training times were expectedly much longer, but only required ten to twenty passes through the training set. This amounts\nz1wx | to two to three days of CPU to train LeNet-five on a Silicon Graphics Origin two thousand server using a single two hundred megahertz Rten thousand processor. It is important to note that while the training time is somewhat relevant to the designer, it is of little interest to the final user of the system. Given the choice between an existing technique and a new technique that brings marginal accuracy improvements at the price of considerable training time, any final user would choose the latter.\no6gf | The overall performance depends on many factors including accuracy, running time, and memory requirements. As computer technology improves, larger capacity recognizers become feasible. Larger recognizers in turn require larger training sets. LeNet-one was appropriate to the available technology in nineteen eighty-nine, just as LeNet-five is appropriate now. In nineteen eighty-nine a recognizer as complex as LeNet-five would have required several weeks' training and more data than were available and was therefore not even considered. For quite a long time, LeNet-one was considered the state of the art. The local learning classifier, the optimal margin classifier, and the tangent distance classifier were developed to improve upon LeNet-one-and they succeeded at that. However, they\n0hs6 | in turn motivated a search for improved NN architectures. This search was guided in part by estimates of the capacity of various learning machines, derived from measurements of the training and test error as a function of the number of training examples. We discovered that more capacity was needed. Through a series of experiments in architecture, combined with an analysis of the characteristics of recognition errors, LeNet-four and LeNet-five were crafted.\nm05x | We find that boosting gives a substantial improvement in accuracy, with a relatively modest penalty in memory and computing expense. Also, distortion models can be used to increase the effective size of a data set without actually requiring to collect more data.\nds29 | The SVM has excellent accuracy, which is most remarkable because, unlike the other high performance classifiers, it does not include a priori knowledge about the problem. In fact, this classifier would do just as well if the image pixels were permuted with a fixed mapping and lost their pictorial structure. However, reaching levels of performance comparable to the convolutional NN's can only be done at considerable expense in memory and computational requirements. The RS-SVM requirements are within a factor of two of the convolutional networks, and the error rate is very close. Improvements of those results are expected as the technique is relatively new.\nqcmr | When plenty of data are available, many methods can attain respectable accuracy. The neural-net methods run much faster and require much less space than memory-based techniques. The NN's advantage will become more striking as training databases continue to increase in size.\n7c6p | E. Invariance and Noise Resistance\nd1ee | Convolutional networks are particularly well suited for recognizing or rejecting shapes with widely varying size, position, and orientation, such as the ones typically produced by heuristic segmenters in real-world string recognition systems.\nitx4 | In an experiment like the one described above, the importance of noise resistance and distortion invariance is not obvious. The situation in most real applications is quite different. Characters generally must be segmented out of their context prior to recognition. Segmentation algorithms are rarely perfect and often leave extraneous marks in character images (noise, underlines, neighboring characters), or sometimes cut characters too much and produce incomplete characters. Those images cannot be reliably size-normalized and centered. Normalizing incomplete characters can be very dangerous. For example, an enlarged stray mark can look like a genuine \"one.\" Therefore, many systems have resorted to normalizing the images at the level of fields or words. In our case, the upper and lower profiles of entire fields (i.e., amounts in a check) are detected and used to normalize the image to a fixed height. While this guarantees that stray marks will not be blown up into character-looking images, this also creates wide variations of the size and vertical position of characters after segmentation. Therefore it is preferable to use a recognizer that is robust to such variations. Fig. thirteen shows several examples of\nvwrs | distorted characters that are correctly recognized by LeNet-five. It is estimated that accurate recognition occurs for scale variations up to about a factor of two, vertical shift variations of plus or minus about half the height of the character, and rotations up to plus or minus thirty degrees. While fully invariant recognition of complex shapes is still an elusive goal, it seems that convolutional networks offer a partial answer to the problem of invariance or robustness with respect to geometrical distortions.\nxjkw | IV. MULTIMODULE SYSTEMS AND GRAPH TRANSFORMER NETWORKS\nwght | The classical back-propagation algorithm, as described and used in the previous sections, is a simple form of gradient-based learning. However, it is clear that the gradient back-propagation algorithm given by four describes a more general situation than simple multilayer feedforward networks composed of alternated linear transformations and sigmoidal functions. In principle, derivatives can be back-propagated through any arrangement of functional modules, as long as we can compute the product of the Jacobians of those modules by any vector. Why would we want to train systems composed of multiple heterogeneous modules? The answer is that large and complex trainable systems need to be built out of simple, specialized modules. The simplest example is LeNet-five, which mixes convolutional layers, subsampling layers, fully connected layers, and RBF layers. Another less trivial example, described in Sections Four-A and Four-B, is a system for recognizing words, that can be trained to simultaneously segment and recognize words without ever being given the correct segmentation.\n3lgb | W2 in the figure), external inputs and outputs (Z , D, E), and intermediate state variables (X1, X2, X3, X4, X5).\nttwt | A. An Object-Oriented Approach\nx5l2 | Object-oriented programming offers a particularly convenient way of implementing multimodule systems. Each module is an instance of a class. Module classes have a \"forward propagation\" method (or member function) called fprop whose arguments are the inputs and outputs of the module. For example, computing the output of module three in Fig. fourteen can be done by calling the method fprop on module three with the arguments X3, X4, X5.",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725394124,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 1605,
    "prompt_tokens": 3348,
    "total_tokens": 4953
  }
}