{
  "id": "chatcmpl-A3U9nytg9WeviYYgtyy8kKQmTYlRE",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "ccmb | inferences, just as sensitivity to distributional properties results in a sensitivity to minimal content. The fact that LLM outputs often reflect pragmatically enhanced versions of the input strings they receive speaks against any claim that the distributional properties LLMs map are fixed by the literal meaning of words and sentences alone, but this need not be part of the current claim (that LLM outputs can be understood in terms of the minimal content they express).\nma93 | So, should we claim that LLM outputs are meaningful because they possess type-level content arrived at through a kind of semantic deference (so that the expressions produced by LLMs count as meaningful simply because they are tokens of types of symbols that belong to a meaningful communicative system, put together in grammatically respectable ways)? I think the answer to this question is likely to depend on one's views on what is required for belonging to a linguistic practice, for some versions of A-style approaches re-import an appeal to current intentions at this juncture and any such move would place even this kind of derived content out of reach of an LLM. Take, for example, Kripke's theory of names which holds that the referent of a name is fixed by an initial act of 'baptism', where an agent who is in contact with an object introduces a linguistic label for it. A subsequent user of the name then refers to the object named by the original baptiser (even if the current user is not in contact with that object themselves) just in case the current speaker's use of the name belongs to a chain of past uses which can be traced back to the original baptism. Importantly, however, the current user needs to intend her utterance to belong to that chain of uses. Thus current speaker intentions do matter for fixing meaning for Kripke, even though they are not determinative of reference. If intentions are needed for semantic deference (in the way that the Kripkean model suggests) then type-level semantic content is not available for LLM outputs, since the system cannot intend its outputs to be part of a social practice. On the other hand, if a current token expression can be taken as belonging to a chain of uses regardless of whether the speaker intends this or not, I would suggest that LLM outputs can be held to express genuine, type-level content.\nc6hh | Finally, however, two questions remain: should LLMs be said to grasp or understand the semantic content expressed by their outputs? And can an LLM be said to be asserting type-level\ny7ea | semantic content (do LLMs 'mean what they say')? In answer to the first question, I think whether we take large language models to grasp semantic content will depend on what we say about the internal states of the systems and the stance we take on what is required to represent a given property. The inner states of LLMs are generated by the distributional properties of words alone, but, as Lake and Murphy note, these distributional properties track what seem to be genuinely semantic properties like inferential role and conceptual connections (in the same way that, in the thought experiment suggested in section two certain mutations realise red-headedness, or as a system which tracks a certain arrangements of atoms might be held to be representing the property of fragility). Similarly, an advocate of the derived intentionality model of LLM outputs might hold that the intentional practices of speakers are embedded in the distributional facts of the massive training database to which LLMs are exposed in such a way that internal manipulations which turn on these kinds of statistical properties also, de facto, turn on genuine meaning properties. In this way, it might be possible to argue that vector weightings can be understood as representing meanings not merely distributions. However, as the discussion of naturalised theories of content at the close of section two point one alluded to, the question of what it is for an internal state to represent one distal property over another is vexed and I think that currently the jury must remain out on whether LLMs engage in the kind of genuinely content-driven internal transitions which would be needed to warrant an attribution of grasp of semantic properties to the system.\nnav4 | Finally, what about the illocutionary force of LLM outputs: do LLMs assert type-level semantic content? I think that the answer to this question is 'no'. In line with many philosophical theories, I think that truth has a key role to play in communication and assertion. But as already noted in section two, truth does not seem to get a grip in LLMs. This can be seen in the tendency of such systems to output plausible sounding falsehoods alongside truths and to go on to support any such claims by 'fabricating' evidence. This tendency reflects the fact that, as we might put it, the norm of LLM outputs is plausibility not truth. I think this means that, even though we should count LLM outputs as meaningful we should not count them as assertions. We should thus ensure that any of their outputs which make it into the public sphere are marked as such, indicating that a higher degree of vigilance would be needed before the semantic content is endorsed.\ngem8 | To summarise, then: the Sceptical worry that meaning requires an appropriate connection to the world can be met in three ways: by adopting an internalist approach to meaning, by embedding LLMs in robotic systems, or by adopting a type-level, deference model of semantic content. This latter approach, I suggest, in fact captures the way in which meaning\ne8nr | attaches to the vast majority of human linguistic performances. By producing well-formed strings of token signs which belong to an extant system of human communication, both the sentences we utter and the sentences LLMs produce get to have semantic content (assuming entry to the linguistic practice is not intention-dependent). The main difference between us and the machines, then, as far as linguistic meaning is concerned, is that we unquestionably represent semantic properties qua semantic properties and we also (often) mean what we say.\nk78w | Four. Original intentionality\ng9du | Searle writes that \"intrinsic Intentionality is observer-independent - I have a state of hunger regardless of what any observer thinks. Derived Intentionality is observer- dependent - it is only in relation to observers, users, and so on, that, for example, a sentence of French has the meaning it has\". He then holds that it is human brains alone which are capable of giving rise to this kind of original intentionality. This idea of an observer-independent, inner mental life where a subject takes one thing to stand for another is of course far from uncontroversial. Dennett, for instance, famously argues that all intentionality is observer- dependent (coming about through the adoption of the Intentional Stance), while Boden nineteen eighty-eight objects that the idea of original intentionality is ineliminably mysterious, relying on a dualist approach to the mind that should be rejected. This paper is obviously not the place to try and resolve such absolutely fundamental questions about the human mind, but let's suppose that Searle is right here (at least about intrinsic intentionality, without the chauvinistic limitation to human brains), what would that mean for LLMs? First, to stand any chance of having original intentionality emerge in an artificial system it seems clear that it will need the kind of rich embedding in a non-linguistic environment envisaged in the Robot Reply (section two point two). It is too early to know exactly what LLMs will look like once successfully embedded in multi-modal systems, but at the very least we can conclude that as things stand LLMs are not candidates for intrinsic intentionality (or for consciousness). Secondly, intrinsic intentionality requires more than just contact with things beyond the text, it requires a subject of experience, so that worldly",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725394271,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 1611,
    "prompt_tokens": 3327,
    "total_tokens": 4938
  }
}