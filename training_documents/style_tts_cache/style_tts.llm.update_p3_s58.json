{
  "id": "chatcmpl-A0ZlbzoArKyYrTt4zfaDv0g6PHT4o",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "```\n1jyv |\nolce | where N is the number of phonemes in t, ti is the i-th phoneme token of t, ti is the i-th predicted phoneme token, and CE(.) is the cross-entropy loss function.\nerx1 | Since this alignment is not necessarily monotonic, we use a simple L one loss Lmono that forces the soft attention alignment to be close to its non-differentiable monotonic version:\nb3bt | Lmono equals Ex,t absolute value of @align minus @hard absolute value one , where @align equals A(x, t) is the attention alignment and @hard is the monotonic hard alignment obtained through dynamic programming algorithms.\n0w8m | Duration and prosody prediction\nvcu3 | Adversarial objectives. We employ two adversarial loss functions, the original cross-entropy loss function Lady for adversarial training and the additional feature-matching loss Lfm, to improve the sound quality of the reconstructed mel-spectrogram:\nhjzj | Lady equals Ex,t log D(x) plus log (one minus D(x)) ,\nj79f |\n6tbv | T divided by N summation absolute value D prime of x minus D prime of x absolute value one ,\nxb13 | Lfm equals Ex,t\ncxqi | where & is the reconstructed mel-spectrogram by G, T is the total number of layers in D and D prime denotes the output feature map of l-th layer with N number of features. The feature matching loss can be seen as a reconstruction loss of hidden layer features of real and generated speech as judged by the discriminator.\nqfmx | First stage full objectives. Our full objective functions in the first stage can be summarized as follows with hyperparameters As25, Amono, dadv and Afm:\nrut3 | min max Lmel plus As25 Ls2s plus Amono Lmono\nm3cp | G,A,E,F,T D\nmq4s |\n95el | plus Xadv Lady plus Afm Lfm\na0a5 | Second Stage Objectives:\ngocl | Duration prediction. We employ the L one loss to train our duration predictor\nob1z | Ldur equals Ed absolute value d minus dpred absolute value one\nm4cb |\nm61y | where d is the ground truth duration obtained by summing Calign along the mel frame axis. dpred equals L(R(htext, s)) is the predicted duration under the style vector s.\nu2ob | Prosody prediction. We train our prosody predictor via a unique data augmentation scheme. Since the duration predictor is trained separately from other modules (using only duration), the duration predictions it produces may not always be optimal or compatible with the prosody predictor. To make the prosody predictor more robust to these potentially suboptimal duration predictions, we augment the data to introduce prosody invariance over the duration.\n2gnm | More specifically, instead of using the ground truth alignment, pitch, and energy of the original mel-spectrogram, we first apply a one-D bilinear interpolation to stretch or compress the mel-spectrogram in time to obtain the augmented sample. As a result, the speech speed changes, yet the pitch and energy curves remain consistent. Accordingly, the prosody predictor learns to maintain pitch and energy prediction invariance, regardless of the duration of speech. This approach helps mitigate issues with unnatural prosody when the predicted duration is incorrect.\nemgw | We use Lf zero and Ln, which are F zero and energy reconstruction loss, respectively:\nxaqt | Co equals Epa absolute value Pa minus Pp (S (htext, S) @ align ) absolute value ,\n5rcb | Ln equals Ex absolute value na minus Pn (S (htext, S) @align ) absolute value ,\n524j | where p, n and @align are the pitch, energy and alignment of the augmented dataset. Pp denotes the pitch output from the prosody predictor, and Pn denotes the energy output.\nxd3m | Decoder reconstruction. Lastly, we aim to ensure that the predicted pitch and energy can be effectively used by the decoder. Given that the mel-spectrogram is stretched or compressed during data augmentation, using them as the ground truth may lead to unwanted artifacts in the predicted prosody. Instead, we train the prosody predictor to produce pitch and energy predictions that can effectively reconstruct the decoder's outputs.\n8qb6 | Lde equals E,t absolute value minus G (htext @align, s, p, i) absolute value , where equals G (htext @align, s, p, absolute value x absolute value) is the reconstruction of , p equals Pp (S (htext, S) @align) the predicted pitch and equals Pn (S (htext, S) @align) the predicted energy.\nwkbn | Second stage full objectives. Our full objective functions in the second stage can be summarized as follows with hyperparameters Adur, Afo, and An:\nftfg | min Lde plus Adur duration plus AfO F zero plus AnCn\n7xi5 | Three. EXPERIMENTS\nw8r5 | A. Datasets\nybik | We conducted experiments on three datasets. We trained a single-speaker model on the LJSpeech dataset. The LJSpeech dataset consists of thirteen thousand one hundred short audio clips with a total duration of approximately twenty-four hours. We used the same split as VITS where the training set contains twelve thousand five hundred samples, the validation set one hundred samples and the test set five hundred samples. We also trained a multi-speaker model on the LibriTTS dataset. The LibriTTS train-clean-four hundred sixty subset consists of approximately\ngii5 | two hundred forty-five hours of audio from one thousand one hundred fifty-one speakers. We removed utterances with a duration longer than thirty seconds and shorter than one second. We randomly split the train-clean-four hundred sixty subset into a training (ninety-eight percent), a validation (one percent), and a test (one percent) set and use the test set for evaluation. We also used the VCTK dataset to show that our model is capable of zero-shot speaker adaptation. We used the same training and test speaker split as in VCTK, where eighty-eight speakers were used for training and the rest twenty were used for testing.\nq5eu | In addition, we trained a multi-speaker model on the emotional speech dataset (ESD) to demonstrate the capacity to synthesize speech with diverse prosodic patterns. ESD consists of ten Chinese and ten English speakers reading the same four hundred short sentences in five different emotions. We trained our model on ten English speakers with all five emotions. We\na7ao | upsampled training audios to twenty-four kilohertz to match the LibriTTS dataset. We converted text sequences into phoneme sequences using an open-source tool. We extracted mel-spectrograms with a FFT size of two thousand forty-eight, hop size of three hundred, and window length of twelve hundred in eighty mel bins using TorchAudio.\n29a3 | B. Training\nsb1e | For both stages, we trained all models for two hundred epochs using the AdamW optimizer with beta one equals zero, beta two equals zero point ninety-nine, weight decay equals ten to the power of negative four, learning rate gamma equals ten to the power of negative four and batch size of sixty-four samples. We set s2s equals zero point two, dadv equals one, mono equals five, f m equals zero point two, Adur equals one, AfO equals zero point one, and An equals one. This setting of hyperparameters makes sure that all loss values are on the same scale and that the training is not sensitive to these hyperparameters. The scale factor ranges from zero point seventy-five to one point twenty-five for data augmentation. We randomly divided the mel-spectrograms into segments of the shortest length in the batch. The training was conducted on a single NVIDIA A forty GPU.\nw2yn | C. Evaluations\n```",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1724700851,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_df84d6bd70",
  "usage": {
    "completion_tokens": 1664,
    "prompt_tokens": 3427,
    "total_tokens": 5091
  }
}