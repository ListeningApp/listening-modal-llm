{
  "id": "chatcmpl-A3U7Ql6wUnHyxjmvOYDx4FrMgQKnr",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "5pto | Translated in terms of penalties, the penalty of an interpretation should be the negative logarithm of the sum of the negative exponentials of the penalties of the individual paths. The overall penalty will be smaller than all the penalties of the individual paths.\nmo6k | Given an interpretation, there is a well-known method, called the forward algorithm for computing the above quantity efficiently. The penalty computed with this procedure for a particular interpretation is called the forward penalty. Consider again the concept of constrained\nzh6q | graph, the subgraph of the interpretation graph which contains only the paths that are consistent with a particular label sequence. There is one constrained graph for each possible label sequence (some may be empty graphs, which have infinite penalties). Given an interpretation, running the forward algorithm on the corresponding constrained graph gives the forward penalty for that interpretation. The forward algorithm proceeds in a way very similar to the Viterbi algorithm, except that the operation used at each node to combine the incoming cumulated penalties, instead of being the min function, is the so-called logadd operation, which can be seen as a \"soft\" version of the min function\nkp04 | fn equals logadd of all upstream arcs, penalty on arc I plus fsi\n3e8r | Equation thirteen\n8g7g | where fstart equals zero, Un is the set of upstream arcs of node n, Ci is the penalty on arc I, and\n5j4f | logadd of x one, x two, and so on through In equals negative log summation e to the negative power of\n4xe7 | Note that because of numerical inaccuracies, it is better to factorize the largest e to the negative ri (corresponding to the smallest penalty) out of the logarithm.\npg4b | An interesting analogy can be drawn if we consider that a graph on which we apply the forward algorithm is equivalent to an neural network on which we run a forward propagation, except that multiplications are replaced by additions, the additions are replaced by log-adds, and there are no sigmoids.\nb0d0 | One way to understand the forward algorithm is to think about multiplicative scores (e.g., probabilities) instead of additive penalties on the arcs: score equals exponential of negative penalty. In that case the Viterbi algorithm selects the path with the largest cumulative score (with scores multiplied along the path), whereas the forward score is the sum of the cumulative scores associated to each of the possible paths from the start to the end node. The forward penalty is always lower than the cumulated penalty on any of the paths, but if one path \"dominates\" (with a much lower penalty), its penalty is almost equal to the forward penalty. The forward algorithm gets its name from the forward pass of the well-known Baum-Welsh algorithm for training hidden Markov models. Section eight point E gives more details on the relation between this work and hidden Markov models.\nd7eu | The advantage of the forward penalty with respect to the Viterbi penalty is that it takes into account all the different ways to produce an answer, not just the one with the lowest penalty. This is important if there is some ambiguity in the segmentation, since the combined forward penalty of two paths C one and C two associated with the same label sequence may be less than the penalty of a path C three associated with another label sequence, even though the penalty of C three might be less than any one of C one or C two.\nxndr | The forward-training GTN is only a slight modification of the previously introduced Viterbi-training GTN. It suffices to turn the Viterbi transformers in Figure nineteen into forward scorers that take an interpretation graph as input and produce\nm6hi | the forward penalty of that graph on output. Then the penalties of all the paths that contain the correct answer are lowered, instead of just that of the best one.\nvpkf | Back propagating through the forward penalty computation (the forward transformer) is quite different from back propagating through a Viterbi transformer. All the penalties of the input graph have an influence on the forward penalty, but penalties that belong to low-penalty paths have a stronger influence. Computing derivatives with respect to the forward penalties fn computed at each n node of a graph is done by back-propagation through the graph\n4d67 | partial derivative of afdi with respect to cfa equals negative Ci (equation fifteen)\ne7j2 | partial derivative of fn equals e to the negative fn, i in set Dn\nlqzt | where Dn equals the set of downstream arcs from node n. From the above derivatives, the derivatives with respect to the arc penalties are obtained\n1ocm | partial derivative equal \n527v | e to the negative Ci minus fsi plus fdi. Equation sixteen\nlrri | This can be seen as a \"soft\" version of the back propagation through a Viterbi scorer and transformer. All the arcs in Gc have an influence on the loss function. The arcs that belong to low penalty paths have a larger influence. Back propagation through the path selector is the same as before. The derivative with respect to Gint arcs that have an alter ego in Ge are simply copied from the corresponding arc in Gc. The derivatives with respect to the other arcs are zero.\n5cl9 | Several authors have applied the idea of back-propagating gradients through a forward scorer to train speech recognition systems, including Bridle and his o-net model and Haffner and his TDNN model, but these authors recommended discriminative training as described in the next section.\nu7a3 | D. Discriminative Forward Training\nw409 | The information contained in the forward penalty can be used in another discriminative training criterion which we will call the discriminative forward criterion. This criterion corresponds to maximization of the posterior probability of choosing the paths associated with the correct interpretation. This posterior probability is defined as the exponential of minus the constrained forward penalty, normalized by the exponential of minus the unconstrained forward penalty. Note that the forward penalty of the constrained graph is always larger or equal to the forward penalty of the unconstrained interpretation graph. Ideally, we would like the forward penalty of the constrained graph to be equal to the forward penalty of the complete interpretation graph. Equality between those two quantities is achieved when the combined penalties of the paths with the correct label sequence is negligibly small compared to the penalties of all the other paths, or that the posterior probability associated to the paths with the correct interpretation is almost one, which is precisely what we want. The corresponding GTN training architecture is shown in Figure twenty-one.\nm04y | Let the difference be denoted Edforw, and let us call Ccforw the forward penalty of the constrained graph and\n23jk | Cforw the forward penalty of the complete interpretation graph\noa63 | Edforw equals Ccforw minus Cforw. Equation seventeen\ne3lv | Edforw is always positive since the constrained graph is a subset of the paths in the interpretation graph, and the forward penalty of a graph is always larger than the forward penalty of a subgraph of this graph. In the ideal case, the penalties of incorrect paths are infinitely large, therefore the two penalties coincide and Edforw is zero. Readers familiar with the Boltzmann machine connectionist model might recognize the constrained and unconstrained graphs as analogous to the \"clamped\" (constrained by the observed values of the output variable) and \"free\" (unconstrained) phases of the Boltzmann machine algorithm.",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725394124,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 1557,
    "prompt_tokens": 3249,
    "total_tokens": 4806
  }
}