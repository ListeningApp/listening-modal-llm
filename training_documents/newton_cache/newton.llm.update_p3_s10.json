{
  "id": "chatcmpl-A3UEKoX4YeC4rCedeE1cioZ33wZFO",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "4uik | With a view to eliminate the necessity of forming and inverting the Hessian, Kim et al. extend TMP to utilize a quasi-Newton update with asymptotic convergence guarantees in the convex setting. Also in this vein, Xie and Wright\ne0rl | considered \"projected Newton-CG\", which entails a combination of the projected gradient and the inexact Newton steps that preserve the simplicity of projection onto R plus to the power of D. In particular, Newton-CG steps are based on the capped CG procedure of Royer et al.. Unfortunately, the gradient and Newton-CG steps are not taken simultaneously. Instead, the algorithm employs projected gradient steps across all components until optimality is attained in the approximately active set. Only at that point is the Newton-CG step applied in the approximately inactive set. This implies that the algorithm may take projected gradient steps at most iterations, potentially impeding its practical performance.\n7hew | Hessian-free Inexact Methods. In high-dimensional settings, storing the Hessian matrix may be impractical. Moreover, an approximate direction can often be computed at a fraction of the cost of a full Newton step. In this context, Hessian-free inexact Newton-type algorithms leverage Krylov subspace methods, which are particularly well-suited for these scenarios. Krylov subspace solvers can recover a reasonable approximate direction in just a few iterations and only require access to the Hessian-vector product mapping, V maps to H of X V. The computational cost of a Hessian-vector product is comparable to that of a gradient evaluation and does not require the explicit formation of H. Indeed, H of X V can be computed by obtaining the gradient of the map X maps to the inner product of G of X and V using automatic differentiation, leading to one additional back propagation compared to computing\nonvv | G of X. Complexity in Optimisation. Recently, there has been a growing interest in obtaining global worst case iteration complexity guarantees for optimisation methods, namely a bound on the number of iterates required for the algorithm to compute an approximate solution. For instance, in unconstrained and nonconvex settings, gradient descent produces an approximate first-order optimal point satisfying the norm of G of X is less than or equal to Epsilon G in at most big O of Epsilon G to the power of negative two iterations for objectives with Lipschitz continuous gradients. This rate has been shown to be tight. Without additional assumptions, similar rates have also been shown for second-order methods. However, for objectives with both Lipschitz continuous gradient and Hessian, this rate can be improved to big O of Epsilon G to the power of negative three halves, which is also shown to be tight over a wide class of second-order algorithms. Second-order methods which achieve this rate include cubic regularised Newton's method and its adaptive variants, modified trust region based methods and line search methods including Newton-CG and Newton-MR as well as their inexact variants. Many of the above works also provide explicit bounds on the operational complexity, that is, a bound on the number\nadbb | Inexact Newton-type Methods for Optimisation with Nonnegativity Constraints\nh6cg | of fundamental computational units (e.g. gradient evaluations, Hessian vector products) to obtain an approximate solution.\nunxh | In the constrained setting, direct comparison between bounds is difficult due to differences in approximate optimality conditions; see discussion in Xie and Wright for the bound constraint case. However, the algorithms in Cartis et al.; Birgin and Mart√≠nez achieve big O of Epsilon G to the power of three halves for a first-order optimal point with certain types of constraints, which is shown to be tight in Cartis et al.. More specific to the bound constraint case, the Newton-CG log barrier method of O'Neill and Wright achieves a complexity of big O of Deg to the power of I minus one half two plus Epsilon G to the power of three halves, while the projected Newton-CG algorithm of Xie and Wright obtains a rate of big O of Epsilon G under a set of approximate optimality conditions similar to this work.\nycmp | Optimality Conditions. Recall that X satisfies the first-order necessary conditions for one if\nwabe | X is greater than or equal to zero, and the partial derivative of F of X with respect to I is zero, if X sub I is greater than zero, F of X is greater than zero, if X four is equal to zero.\nvpyq | Six.\nai73 | We seek a point which satisfies these conditions to some E tolerance. There are a number of ways to adapt six into an approximate condition. In this work we adopt Xie and Wright, Definition one.\n1lzj | Definition two point one. Epsilon-Optimal Point. A point, X, is called Epsilon-approximate first-order optimal, Epsilon-FO, if\nboyi | G sub I is greater than or equal to negative V Epsilon, for all I in A of X, V Epsilon. Seven A\n9lwd | Norm of the diagonal of X A G four is less than or equal to Epsilon, Seven B\nd2vd | Norm of G I is less than or equal to Epsilon, Seven C\now2p | We take Seven A and Seven B to be trivially satisfied if A of X, V Epsilon is empty and similar for Seven C if I of X, V Epsilon is empty.\nq5v1 | This definition has been shown to be asymptotically exact. Lemma two point two. Suppose that E sub K converges to zero and we have a sequence such that X sub K satisfies the corresponding E sub X-FO optimality condition. If X sub K converges to X star then X star satisfies Six.\nty34 | Three. Newton-MR Two-Metric Projection\ngzzp | We now propose and theoretically study our extensions of the TMP framework, which involves simultaneously employing gradient and inexact Newton steps, which are, respectively, restricted to the active and inactive sets.\nkh7y | Three point one. MINRES and Its Properties\n7xl2 | The inexact Newton step is based on the recently proposed Newton-MR framework.",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725394552,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 1275,
    "prompt_tokens": 3362,
    "total_tokens": 4637
  }
}