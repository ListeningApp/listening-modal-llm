You will receive raw text from an OCR scan of a document or web page. Each line of input represents one textbox from the document.

Your task is to label each textbox with exactly one of the following labels:
  title: the primary document title, no more than one per document
  heading: the name of the following chapter or section (should never start in the middle of the previous paragraph)
  subheading: a secondary heading, often following another heading (should never start in the middle of the previous paragraph)
  body: main paragraph content: may start or end in the middle of a word, use context to follow paragraphs that are split across textboxes (if a textbox contains both heading and body text, label it "body")
  math: textbox primarily containing math
  imageDescription: standalone callout that only describes an image, chart, table, or similar (typically begins with "Table...", "Figure...", "Fig. ...", "Above...", "Left...", etc.)
  authors: names of contributors to this document
  institutions: list of universities, business, and other institutions affiliated with this document
  publisher: info about the publisher, or provided by the publisher, including copyright info
  pagenum: the page number
  headerOrFooter: boilerplate text page number, or redundant headings at the top/bottom of the page
  toc: the table of contents
  references: bibliographic references and resources
  acknowledgements: thanks and credits to contributors
  appendix: index, appendix, or any other end matter not part of the main thread (including related headings)
  table: tabular data, label, title, or subheading for a grid or list of data
  datum: numeric data or data label from a chart or list
  advertisement: promotional ads and marketing
  layout: non-content related to the layout of the page, e.g. sidebar navigation
  callout: pull quote or long block text that stands apart from the main text
  footnote: footnotes and endnotes near the bottom of the page
  keywords: list of keywords or tags
  url: web address or email
  other: any other text that does not belong to the main thread

Each line of input is prefixed with a textbox ID in the format `id | Text content`. The output must be in the format `id | label`.

Note that body text may be split across multiple textboxes, and textboxes may start or end in the middle of a sentence or word. Because the text was extracted from a web page or document, paragraphs of body text may be interrupted by multiple textboxes of redundant headers, footnotes, page numbers, tables, images, etc. For example, a real heading will never interrupt the middle of a sentence. Use context clues like this to carefully label each textbox.

EXAMPLE INPUT:
1pjs | Neuroscience Applied 1 (2022) 101012
o2kr | Contents lists available at ScienceDirect
v6sk | Neuroscience Applied
1wj6 | journal homepage: www.journals.elsevier.com/neuroscience-applied
936l | Research Articles
06yq | Microdosing with psychedelics to self-medicate for ADHD symptoms in adults: A naturalistic study
dpv6 | ARTICLE INFO
8r5j | ABSTRACT
2taz | Keywords: ADHD Microdosing Psychedelics Self-medication Well-being Time perception
o3ya | ADHD in adulthood is often overlooked, which negatively affects the individual's well-being. First-line phar- macological interventions are effective in many ADHD patients, relieving symptoms rapidly. However, there seems to be a proportion of individuals who discontinue, or fail to respond to these treatments. For these in- dividuals, alternative treatment options should be explored.
r5il | 1. Introduction
5gui | Attention deficit hyperactivity disorder (ADHD) is one of the most common developmental disorders worldwide. Prevalence research indicates that 2.6% of the adult population has persistent ADHD. ADHD in adults is often overlooked because of the high comorbidity rate and lack of knowledge about how ADHD is expressed in adulthood (Kooij et al.,
9d4o | Fig. 1. Flowchart of included participants for each time point.
82qc | 2019). In addition, ADHD is associated with deficits in various domains of cogni- tive functioning. Twenty-five percent of ADHD cases suffered from ADHD symptoms purely because of de- ficiencies in temporal processing (Sonuga-Barke et al., 2010).
ls7d | First-line ADHD treatments in adults mainly include pharmacological interventions to enhance dopaminergic and noradrenergic neurotrans- mission with stimulants. Overall, they have been proven to work effectively in adults with ADHD, inducing fast symptom relief and thereby enhancing the person's quality of life. In the
5esx | Table 1 Demographic information from respondents at baseline and the two and four- week time points.
t8vv | longer term, approximately twenty percent of ADHD patients discontinue their prescribed medication after six to nine months, thirty percent after one year, and half of them after two years.
xrd1 | 2. Material and methods
5746 | 2.1. Study design and participants
7toz | The study employed a naturalistic design, assessing the experiences of participants at baseline,
j346 | Neuroscience Applied 1 (2022)
m5ka | E.C.H.M. Haijen et al.
kx2c | REVIEW
v8uq | before they start MD on their own initiative, and at two and four weeks after MD initiation. The target population included adults diagnosed with ADHD and individuals who experienced ADHD symptoms to the extent that these interfered with their daily lives and who had not been diag- nosed with ADHD before. To be included in the analyses, participants needed to score above a cut-off point on at least one of the subscales of the Conner's Adult ADHD Rating Scale (CAARS-S:SV).
2zmw | Fig. 2. Mean scores of the CAARS-S:SV DSM-IV total symptoms T-scores at baseline (0W) and two (2W) and four weeks (4W) after MD (A) of the whole sample, and (B) per conventional ADHD medication use. Error bars represent mean error.
sdk3 | <LATEX>\lim _ { x ightarrow \infty } rac { 6 x ^ { 2 } + 1 0 0 } { 7 x ^ { 2 } - 1 0 } =</LATEX>
x972 | 2.2. Study procedure
k221 | Mean performance measures of the CAARS-

EXAMPLE OUTPUT:
1pjs | headerOrFooter
o2kr | publisher
v6sk | publisher
1wj6 | publisher
936l | headerOrFooter
06yq | title
dpv6 | other
8r5j | heading
2taz | keywords
o3ya | body
r5il | heading
5gui | body
9d4o | imageDescription
82qc | body
ls7d | body
5esx | imageDescription
t8vv | body
xrd1 | heading
5746 | subheading
7toz | body
j346 | headerOrFooter
m5ka | authors
kx2c | headerOrFooter
v8uq | body
2zmw | imageDescription
sdk3 | math
x972 | heading
k221 | body




1pjs | arXiv:2107.06925v3 [cs.DC] 25 Feb 2022
o2kr | Chimera: Efficiently Training Large-Scale Neural Networks with Bidirectional Pipelines
v6sk | Shigang Li shigang.li@inf.ethz.ch Department of Computer Science, ETH Zurich Switzerland
1wj6 | ABSTRACT
936l | Training large deep learning models at scale is very challenging. This paper proposes Chimera, a novel pipeline parallelism scheme which combines bidirectional pipelines for efficiently training large- scale models. Chimera is a synchronous approach and therefore no loss of accuracy, which is more convergence-friendly than asynchro- nous approaches. Compared with the latest synchronous pipeline approach, Chimera reduces the number of bubbles by up to 50%; ben- efiting from the sophisticated scheduling of bidirectional pipelines, Chimera has a more balanced activation memory consumption. Evaluations are conducted on Transformer based language models. For a GPT-2 model with 1.3 billion parameters running on 2,048 GPU nodes of the Piz Daint supercomputer, Chimera improves the training throughput by 1.16x-2.34x over the state-of-the-art synchronous and asynchronous pipeline approaches.
06yq | CCS CONCEPTS
dpv6 | . Theory of computation -> Parallel algorithms; . Comput- ing methodologies -> Neural networks.
8r5j | KEYWORDS
2taz | distributed deep learning, pipeline parallelism, data parallelism, operator parallelism, model parallelism
o3ya | 1 INTRODUCTION
r5il | Deep learning is continuing to deliver groundbreaking results on the path towards human-level intelligence. This path is character- ized by growing model size, in just six years, the compute require- ments for model training grew by 300,000 times [3]. Transform- ers [54] are a typical representative in this trend. As the model size grows, Transformer based models have proven their success in in the field of natural language processing [13, 43, 43, 54]. Recent work [8-10, 14] shows that Transformers also achieve promising re- sults in computer vision tasks, i.e., on par or better than other types of models such as convolutional [31] and recurrent [21] networks. These growing models must be trained on distributed accelera- tor supercomputers. Even today's models are too big to be stored on a single accelerator-for example, GPT-3's 175 billion parame- ters [7] require 350 GiB main memory if stored with 16 bits precision. Switch transformers [17] have in their largest configuration 1.6 tril- lion parameters, a 6.4 TiB storage requirements. Furthermore, the necessary memory for activations, gradients, and optimizer state during training at least triples these memory requirements.
5gui | Torsten Hoefler htor@inf.ethz.ch Department of Computer Science, ETH Zurich Switzerland
82qc | Thus, full model replicas at each accelerator, as used in simple data parallelization schemes, are not sufficient. Large models must instead be distributed among many, often hundreds of accelerators to just fit into main memory. Deep neural networks consist of a layered architecture and can thus be distributed in two ways: (1) the operators of a layer can be split across multiple accelerators in a method called operator parallelism or (2) the model could be distributed layer by layer in a method called pipeline parallelism [5]. Operators of a typical fully-connected layer have computational characteristics similar to matrix multiplication, and splitting such a layer requires a communication volume of O(n2 /VP) [27, 32] for an n Xn matrix. By exploiting the inherent structure of Transformer based language models [51], operator parallelism requires two allre- duce [35, 53] operations on the output activations for each basic Transformer layer. Using a layer-wise model partition, pipeline parallelism on the other hand only requires point-to-point commu- nication to transfer the output activations between pipeline stages, with each stage containing a group of consecutive layers. Therefore, pipeline parallelism commonly has a lower communication cost than operator parallelism. However, pipeline parallelism suffers from bubbles or weight staleness (see Section 2), which are the problems this work aims to solve. Overall, operator parallelism and pipeline parallelism are orthogonal and complementary to each other for distributing large deep learning models.
ls7d | Yet, pipeline parallelism is not trivial: The backpropagation algo- rithm needs to "remember" the output activations computed during the forward pass as inputs to the backward pass (cf. Figure 2). This creates wildly different memory requirements for each accelera- tor in the pipeline, even though each accelerator has an identical compute load. Specifically, for some recently proposed pipeline approaches such as DAPPLE [16], PipeDream [38], and PipeDream- 2BW [39], the first accelerator of a pipeline of depth D has to store D such activations while the last accelerator requires memory for one. This does not only lead to lower memory utilization in the later pipeline stages (and only 50% overall), it also leads to reduced performance because the micro-batch size has to be chosen to fit the first accelerator in the pipeline. This imbalance can be alleviated by restricting the number of micro-batches that are simultaneously allowed in the pipeline. However, this introduces bubbles and limits the overall system utilization.
tzt3 | Both micro-batch size and pipeline utilization are most important for the computational efficiency: larger micro-batches improve per- formance due to better re-use in the matrix-multiply-like operations and less pipeline bubbles (stalls) utilize the existing accelerators better. The computational efficiency relates directly to the cost and time for of training a model. We propose a new pipelining scheme,
t8vv | called Chimera, that runs fully-packed bidirectional pipelines through the same set of accelerators. Chimera enables
qvnu | · to keep the overall training synchronous without relying on stale weights,
kggw | · a higher pipeline utilization (less bubbles) than existing ap- proaches and thus higher performance,
xrd1 | · the same peak activation memory consumption as the state- of-the-art methods, with an extra benefit of more balanced memory consumption, and
5746 | · easy configurability to various pipelined deep neural net- works as well as system architectures guided by an accurate performance model.
7toz | Figure 1: GPT-2 on 2,048 GPU nodes for mini-batch size=2,048. We show the bubble ratio (pipeline overhead), memory cost (OOM requires activation recomputation, de- noted by R), and the best throughput (D is the corresponding number of pipeline stages) for each approach. Details will be discussed in Sections 2 and 4.2.
j346 | For example, GPT-3 required 314 Zettaflop (mixed fp16/fp32) to train [7], which would take a single A100 GPU more than 100 years. The estimated cost to train GPT-3 varies between $4.6m-$12m. We show that Chimera enables end-to-end performance improvements between 1.38x-2.34x per iteration for the synchronous training regime for a comparable GPT-2 model on 2,048 GPU nodes of the Piz Daint supercomputer, as shown in Figure 1. This enables savings of more than $1.2m to $5m when training very large models on practical systems.
v8uq | 2 BACKGROUND AND RELATED WORK
x972 | Mini-batch stochastic gradient descent (SGD) [6] is the mainstream method to train deep neural networks. Let b be the mini-batch size, Wt the neural network weights at step t, (xi, yi) a sample in the mini-batch, and { a loss function. During training, it computes the loss in the forward pass for each sample as { (Wt, Xi, yi), and then a stochastic gradient in the backward pass as
cwrw | Ve (Wt, Xi, yi).
4i0e | The model is trained in iterations such that Wt+1 = Wt - ntgt. In more general terms, first-order stochastic gradient update rules can take different forms (e.g., by adding a momentum term), which is represented as W+1 = wt + U (gt, W(,,+), t ).
h2l3 | To scale up the training process to parallel machines, data par- allelism [20, 33, 49, 60] is the common method, in which the mini- batch is partitioned among P workers and each worker maintains a copy of the entire model. Gradient synchronization across the workers is often implemented using an allreduce. However, recent deep learning models [7, 13, 43, 45] scale rapidly from millions to billions of parameters. Pure data parallelism may not work for these large models since it either suffers from low efficiency caused by synchronizing gradients of the entire model across the workers or the model is simply too large to fit in a device.
dfzb | Operator parallelism is a solution to train large models by parti- tioning the operators of a layer among multiple workers, but it may suffer from high communication volume as discussed in Section 1. Hybrid approaches [29, 30], which combine operator parallelism with data parallelism, suffer from the similar problem to the pure operator parallelism.
u6ty | To reduce the communication volume of operator parallelism, pipeline parallelism is intensively studied [16, 19, 26, 28, 38, 39, 55, 57]. The key idea is to partition the model in a layer-wise way and treat each worker (and the layers on it) as a pipeline stage. The mini-batch is partitioned into multiple micro-batches, that are pipelined across the stages to increase the resources utilization. Recent work [16, 28, 39] also shows improved performance when combining pipeline parallelism with data parallelism. However, to efficiently pipeline deep neural network training is challenging because of (1) a training step contains one one forward pass fol- lowed by one backward pass, (2) the gradient computation in the backward pass rely on the intermediate results of the forward pass, and (3) to achieve good convergence accuracy the mini-batch size is usually not very large.
awvu | Table 1 summarizes the symbols frequently used in the this paper. Next, we analyze the pros and cons of the state-of-the-art pipeline approaches when handling the challenges above from the aspects listed in Table 2, and then show how our approach achieves the best balance among all aspects. To better understand the analysis in Table 2, Figure 2 presents an example for each approach.
on9b | Table 1: The list of symbols frequently used in the paper.
a8ge | Bubbles in the pipeline. For better convergence quality, synchro- nous approaches synchronize the gradients and flush the pipeline
h27a | Table 2: Comparison between different pipeline schemes.
pdjh | 1 Intervals for the values across the workers.
xmgs | Figure 2: Pipeline parallelism schemes, with four pipeline stages (D=4) and four micro-batches (N=4) within a training itera- tion, except that PipeDream updates the model after each backward pass on a micro-batch.
bd9h | at the end of each training iteration, as shown in Figure 2. There- fore, synchronous approaches lead to pipeline bubbles. To utilize the pipeline, GPipe [26] injects N micro-batches into the pipeline concurrently; DAPPLE [16] uses the One-Forward-One-Backward (1F1B [38, 39]) schedule with periodic flushes. Both GPipe and DAP- PLE incur 2(D-1) bubbles (i.e., D-1 bubbles in the forward passes and D-1 bubbles in the backward passes). In contrast, Chimera (this work) incurs D-2 bubbles (i.e., D/2-1 bubbles in the forward passes and D/2-1 bubbles in the backward passes), which is about 50% reduction compared with DAPPLE and GPipe. We define the bubble ratio as the bubble overhead divided by the overall runtime of the pipeline. In practice, the workload of a backward pass is about two times of a forward pass, which leads to D/2-1 bubbles in the mid- dle of the pipeline for Chimera (bubble ratio = (D-2)/(3N/2+D-2)), as shown in Figure 2. We will discuss how to remove the middle bubbles in Section 3.5 (bubble ratio = (D-2)/(2N+D-2)).
m047 | Table 2 presents the bubble ratio for all approaches with the consideration of typical workloads of forward and backward passes. Although the bubble ratio of GPipe and DAPPLE decreases as N (the number of micro-batches executed by each worker within a training iteration) increases, a large enough N (N>=4D as suggested in [26]) usually cannot be obtained without hurting the efficiency for the following three reasons: (1) There is usually an empirical maximum ₿ (mini-batch size) for a model, exceeding which would compromise model convergence [5, 12, 58-60]. (2) An increase of N implies an decrease of B (micro-batch size) for a given . However, modern accelerators require a large enough B to achieve high computational efficiency. (3) Scaling to large-scale machines by combining with data parallelism (which has proven to be an effective way [16, 39]) would decrease N for a given B.
gy8c | GEMS [28] is a memory-efficient pipeline approach which sched- ules micro-batches among two model replicas. Since GEMS is mainly designed for small B and has at most two active micro-batches at
y4mj | Figure 3: Model replicas and bidirectional pipelines scheduling of Chimera.
vpgq | the same time, its bubble ratio is much higher than the other ap- proaches and cannot be alleviated by a larger N. Asynchronous approaches (such as PipeDream [38] and PipeDream-2BW [39]) do not have periodic pipeline flushes, so they do not have bubble problem but with stale weights.
ousl | Memory consumption. Memory overhead mainly comes from two aspects: the weight parameters and the activations (the inter- mediate results of the forward pass required in the backward pass to compute gradients). For GPipe and DAPPLE, each worker maintains the weights of one pipeline stage. For GEMS and Chimera (with the default setting), each worker maintains the weights of two pipeline stages since there are two pipelines in two directions. PipeDream updates the model after each backward pass on a micro-batch (N=1); therefore, to ensure weight version consistency between forward and backward passes, it requires to stash up to D versions of weights on a worker, which has the same memory cost as pure data paral- lelism. By using gradient accumulation (N>=D), PipeDream-2BW reduces the number of weight versions to be stashed to 2.