{
  "id": "chatcmpl-A3UG0OU8ztQcM7IoNHMrXVry13gmI",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "``` \n6746 | In summary: in the continuous case, entropy is not invariant under reparametrizations: our choice of coordinates matters! And this can make entropy negative. This applies not only to R but many other measure spaces we'll be considering, like R. This issue will be very important.\nr5zg | After learning this, it should be less of a shock that the entropy of a probability distribution on R can be infinite, or even undefined:\nhk56 | Puzzle thirty-three. Find three probability distributions P on the real line that have entropy positive infinity, negative zero, and undefined because it's of the form positive infinity minus infinity.\n29ac | ENTROPY, ENERGY AND TEMPERATURE\nno4d | Suppose a system has some measure space X of states with energy E: X to R. In thermal equilibrium the probability distribution on states, P: X to R, maximizes the Gibbs entropy S equals negative k integral of P(x) ln P(x) dx\nnh8n | subject to a constraint on the expected value of energy: (E) equals integral of P(x) E(x) dx\nircn | Typically when this happens P is the Boltzmann distribution\nnpsf | e to the power of negative E(x)/kT\ndynl | P(x) =\nmsij | integral of e to the power of negative E(x)/kT dx\n8n3m | where T is the temperature and k is Boltzmann's constant.\nech9 | Then as we vary (E) we have\np94v | d(E) equals TdS\ngf1i | We can now generalize a lot of our work from a finite set of states to a general measure space. I won't redo all the arguments, just state the results and point out a couple of caveats.\nrouh | For any measure space X we say a function P: X to [zero, zero) is a probability distribution if it's measurable and integral of P(x) dx equals one.\n080i | We can define a version of Shannon entropy for P by\n4fln | H equals negative integral of P(x) log P(x) dx,\ntmlf | but physicists mainly use the Gibbs entropy, defined by\noy09 | S equals negative k integral of P(x) ln P(x) dx.\nwvb1 | As I warned you last time, this can take values in [negative infinity, zero], though we are mainly interested in cases when it's finite. If we think of X as the space of states of some system, we can pick any measurable function E: X to R and call it the 'energy'. Its expected value is then\nqi9a | (E) equals integral of E(x)P(x) dx\nr7ii | at least when this integral converges.\nezxx | We say the probability distribution P describes thermal equilibrium if it maximizes S subject to a constraint (E) equals c. Typically when this happens P is a Boltzmann distribution\nzzb7 | P(x) equals one over x e to the power of negative beta E(x) times e to the power of negative beta E(x) dx\nzts3 | where beta is called the coolness. I say 'typically' because even when X is a finite set, we saw in Puzzle twenty-four that there can be thermal equilibria that are not Boltzmann distributions, but only limits of Boltzmann distributions as beta approaches positive infinity or approaches negative infinity. This can also happen for other measure spaces X. I will not delve into this, because my goal now is to get to some physics.\nq5zo | As before, we can write beta equals one over kT, at least if beta does not equal zero, and then write the Boltzmann distribution as\nz20w | X to the power of negative E(x)/kT dx e to the power of negative E(x)/kT\nig3d | P(x) =\ndgts | Also as before, the Boltzmann distributions obey the crucial relation\np1zf | dH equals beta d(E).\nk0wj | Rewriting this in terms of Gibbs entropy S equals kH and temperature T equals one over k beta, it becomes this famous relation between temperature, entropy and the expected energy: TdS equals d(E).\np83v | Notice that the units match here. The Shannon entropy H is dimensionless, but since k has units of energy over temperature, the Gibbs entropy S equals kH has units of energy over temperature. Thus TdS has units of energy, as does d(E).\n5yts | THE CHANGE IN ENTROPY\nl1qt | As we change the temperature of a system from T one to T two while keeping it in thermal equilibrium, the change in its entropy is\n183q | S(T one) minus S(T zero) equals integral from T zero to T one d(E) over T\nvdw3 | where (E) is its expected energy at temperature T.\n8whh | Last time we saw that as we change the expected energy (E) of a system while keeping it in thermal equilibrium, this fundamental relation holds:\niywu | TdS equals d(E).\nqn1v | We can rewrite this as\no9ng | dS equals d(E) over T\n6od4 | and then integrate this from one temperature to another-remember, as the expected energy changes, so does the temperature. We get\njx1z | integral from T zero to T one d(E) over T equals S(T one) minus S(T zero).\nhrlm | This is the main way people do experiments to 'measure entropy'. Slowly heat something up, keeping track of how much energy it takes to increase its temperature each little bit. Using this data you can approximately calculate the integral at left and that gives the change in entropy!\ng779 | But so far we're just measuring changes in entropy. How can you figure out the actual value of the entropy? One way is to assume the Third Law of Thermodynamics, which says that in thermal equilibrium the entropy approaches zero as the temperature approaches zero from above. This gives\nby5l | integral from T zero to T one d(E) over T equals S(T)\nwye3 | This is how people often 'measure the entropy' of a system in thermal equilibrium. They heat it up starting from absolute zero, very slowly so they hope it is close to thermal equilibrium at every moment-and they take data on how much energy is used, and approximately calculate the integral at left!\noc7w | But this relies on the Third Law of Thermodynamics. So where does that come from?\nk5k4 | THE THIRD LAW OF THERMODYNAMICS\nwngq | If a system has countably many states, with just one of lowest energy, and thermal equilibrium is possible for this system for some temperature T greater than zero, then its entropy in thermal equilibrium approaches zero as T approaches zero from above:\nudft | limit S(T) equals zero as T approaches zero from above.\nfoay | Some people say the Third Law of Thermodynamics this way: \"entropy is zero at absolute zero\". But it's not really that simple indeed, other people say it's impossible to reach absolute zero. Above I've stated a version of the Third Law that's actually a theorem. Let's prove it!\n43mx | Actually, let's prove it now for systems with only finitely many states. It'll be easier to handle systems with countably infinite number of states later, when we've developed more tools. And by the way, we'll see the Third Law isn't always true for systems with a continuum of states. It will fail for all three of the problems on our big to-do list: the classical harmonic oscillator, the classical particle in a box and the classical ideal gas. This is often taken as a failure of classical mechanics, since switching to quantum mechanics makes the Third Law hold for these systems.\nld7j | Let's show that for a system with finitely many states i equals one, ... , n with energies E i, as the temperature T approaches zero from above, the entropy of the system in thermal equilibrium approaches k ln N where N is the number of lowest-energy states. In thermal equilibrium\nmf5v | P i equals e to the power of negative E i over kT.\nf31w | Thus, all states with the lowest energy have the same probability, while as the temperature approaches zero from above, any higher-energy states have P i approaches zero. So, as the temperature approaches zero from above, the probability of the system being in any one of its N lowest-energy states approaches one over N, and we get\nyofz | limit as T approaches zero from above S(T) equals limit as T approaches zero from above negative k sum from i equals one to n P i ln P i equals negative k sum from i equals one to N of one over N ln of one over N.\n4mdw | ln N equals k ln N.\n49f1 | In particular, if the system has just one lowest-energy state, we get the Third Law of Thermodynamics:\ncsae | limit as T approaches zero from above S(T) equals zero.\nc29v | Here T approaches zero from above means that T is approaching zero from above.\n```",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725394656,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 1909,
    "prompt_tokens": 3516,
    "total_tokens": 5425
  }
}