{
  "id": "chatcmpl-A3UEM0JMychzrfF6uTc15ZHxa4Zds",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "z6fn | Proof. We note that the proof of Lemma C point one utilizes no curvature properties of the residual. With this fact in mind, the proof is entirely the same as Lemma C point one in Appendix C with Varsigma taking the place of O.\nc7zf | The following lemma gives the amount of decrease obtained from the inactive set step whenever the inactive set is nonempty and the inactive set termination condition is not satisfied.\niwkf | Lemma B point three (Sufficient Decrease: Inactive Set Case). Suppose F satisfies Assumption three point one. Let X subscript K plus one equals the projection of X subscript K plus alpha subscript K P subscript K be the update computed at iteration K of Algorithm one, where alpha subscript K satisfies the line search criterion. Suppose the inactive set of X subscript K comma delta subscript K is not equal to the empty set and the termination condition is not satisfied. If the residual type equals the solution, then\n4jzo | F of X subscript K plus one minus F of X subscript K is less than negative rho Varsigma minimum of the quantity two times one minus rho times the minimum of the quantities one and Varsigma times the product of the constants divided by the gradient constant times the square of epsilon subscript K, comma the product of the constants and delta subscript K times the square of epsilon subscript K. where the constant is as defined in equation twenty. Otherwise,\ni5zo | With the residual type equals NPC, F of X subscript K plus one minus F of X subscript K is less than negative rho minimum of the quantity two times one minus rho times the square of eta divided by the product of the gradient constant and the sum of the squares of eta and the gradient constant times the square of epsilon subscript K comma the product of eta and delta subscript K divided by the square root of the sum of the squares of eta and the gradient constant times the square of epsilon subscript K. Proof. Since alpha subscript K satisfies the line search condition, we have\nwe91 | F of X subscript K plus one minus F of X subscript K is less than or equal to rho times the inner product of the gradient of K projection of X subscript K plus alpha P subscript K minus X subscript K plus alpha rho times the inner product of the gradient of K P subscript K is less than or equal to alpha rho times the inner product of the gradient of K P subscript K, where we use the fact that the inner product of the gradient of K projection of X subscript K plus alpha P subscript K minus X subscript K is less than or equal to zero. We now consider the residual type equals the solution and the residual type equals NPC cases.\n3h4l | Inexact Newton-type Methods for Optimization with Nonnegativity Constraints\nmy5y | When the residual type equals the solution, we have P subscript R equals S subscript K. Using the line search condition comma the left-hand side inequality in equation twenty with Varsigma equals eta, we have\nk3e7 | F of X subscript K plus one minus F of X subscript K is less than or equal to P A K times the inner product of the gradient of K comma S subscript K all squared\nzuhq | is less than or equal to negative P A K of the inner product of S subscript K and the projection\npt13 | is less than negative P S A K times the absolute value\nrw3z | is less than negative P S minimum of two times one minus P\nvoc3 | gradient minimum of one comma Varsigma absolute value\nhuha | is less than negative P S minimum of two times one minus P L G minimum of one comma Varsigma all squared\notjb | is less than negative P S minimum of two times one minus P minimum of one comma Varsigma squared\n1rbe | is less than negative P S minimum of two times one minus P minimum of one comma Varsigma constant\nz978 | gradient epsilon subscript K Varsigma gradient\nqxdv | where we applied the norm of gradient greater than Varsigma on the final line.\n06co | When the residual type equals NPC, we have, P equals the residual factor. We first note that, since the inexactness condition has not been met, by applying Assumption three point one and using the fact that\njei2 | the norm of the Hessian is squared equals the norm of the gradient squared minus the norm of the residual squared,\ntti3 | we get\nr4pa | the norm of the residual is greater than or equal to the square root of\nivgr | eta squared plus the gradient squared\nvobc | gradient norm minus\n3rdh | Let W equal eta divided by the square root of eta squared plus the gradient squared. Proceeding similarly to the solution case but using equation twenty-one, we have\nlbvs | F of X subscript K plus one minus F of X subscript K is less than or equal to P A K times the inner product of the gradient of K comma the absolute value of I\n2rt2 | is less than or equal to negative P A K times the absolute value of the residual\n3ld6 | is less than negative P minimum two times one minus P gradient squared\n1ac5 | is less than negative P minimum two times one minus P W squared gradient\noau9 | is less than negative P minimum two times one minus P W squared gradient epsilon\n9i0g | again, making use of the norm of the gradient greater than six percent in the final line.",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725394554,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 1165,
    "prompt_tokens": 3396,
    "total_tokens": 4561
  }
}