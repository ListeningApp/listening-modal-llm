{
  "id": "chatcmpl-A3UAGD3O7ZiS2YsULmp61OhufgZNV",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "````\n9sp0 | Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents\nnbpk | Abstract\nxs3r | Can world knowledge learned by large language models be used to act in interactive environments? In this paper, we investigate the possibility of grounding high-level tasks, expressed in natural language (for example, \"make breakfast\"), to a chosen set of actionable steps (for example, \"open fridge\"). While prior work focused on learning from explicit step-by-step examples of how to act, we surprisingly find that if pre-trained language models are large enough and prompted appropriately, they can effectively decompose high-level tasks into mid-level plans without any further training. However, the plans produced naively by language models often cannot map precisely to admissible actions. We propose a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions. Our evaluation in the recent VirtualHome environment shows that the resulting method substantially improves executability over the language model baseline. The conducted human evaluation reveals a trade-off between executability and correctness but shows a promising sign towards extracting actionable knowledge from language models.\nffuj | Introduction One\nh7f6 | Large language models have made impressive advances in language generation and understanding in recent years. See for a recent summary of their capabilities and impacts. Being trained on large corpora of human-produced language, these models are thought to contain a lot of information about the world - albeit in linguistic form.\nt74k | We ask whether we can use such knowledge contained in language models not just for linguistic tasks, but to make goal-driven decisions that can be enacted in interactive, embodied environments. But we are not simply interested in whether we can train models on a dataset of demonstrations collected for some specific environment - we are instead interested in whether language models already contain information necessary to accomplish goals without any additional training.\nqdn7 | More specifically, we ask whether world knowledge about how to perform high-level tasks (such as \"make breakfast\") can be expanded to a series of groundable actions (such as \"open fridge\", \"grab milk\", \"close fridge\", et cetera) that can be executed in the environment. For our investigation, we use the recently proposed VirtualHome environment. It can simulate a large variety of realistic human activities in a household environment and supports the ability to perform them via embodied actions defined with a verb-object syntax. However, due to the open-ended nature of the tasks, it is difficult to autonomously evaluate their success. We rely on human evaluation to decide whether sequences of actions meaningfully accomplish posed tasks.\ndib4 | We find that large GPT-three and Codex models, when prompted with a single fixed example of a task description and its associated sequence of actions, can produce very plausible action plans for the task we're interested in. Such completions reflect the information already stored in the model - no model fine-tuning is involved. Additionally, we only observe this effect in the larger models. Unfortunately, despite their semantic correctness, the produced action plans are often not executable in the environment. Produced actions may not map precisely to admissible actions, or may contain various linguistic ambiguities.\nyvqj | We propose several tools to improve executability of the model's outputs. First, we enumerate all admissible actions and map the model's output phrases to the most semantically-similar admissible action (we use similarity measure between sentence embeddings produced by a RoBERTa model in this work, but other choices are possible). Second, we use the model to autoregressively generate actions in a plan by conditioning past actions that have been made admissible via the technique above. Such on-the-fly correction can keep generation anchored to admissible actions. Third, we provide weak supervision to the model by prompting the model with a known task example similar to the query task. This is somewhat reminiscent of prompt tuning approaches but does not require access to gradients or internals of the model.\nvz2f | Using the above tools to bias model generation, we find that we improve executability of action plans from eighteen percent to seventy-nine percent (see Figure one) without any invasive modifications to model parameters or any extra gradient or internal information beyond what is returned from the model's forward pass. This is advantageous because it does not require any modifications to the model training procedure and can fit within existing model serving pipelines. However, we do find there to be some drop in correctness of the action sequences generated with the above tools (as judged by humans), indicating a promising step, but requiring more research on the topic.\nwyu0 | To summarize, our paper's contributions are as follows:\ns8xw | Â· We show that without any training, large language models can be prompted to generate plausible goal-driven action plans, but such plans are frequently not executable in interactive environments.\nafv1 | We propose several tools to improve executability of the model generation without invasive probing or modifications to the model.\nfwf1 | We conduct a human evaluation of multiple techniques and models and report on the trade-offs between executability and semantic correctness.\n3wck | Two Evaluation Framework\njq2u | Simulating open-ended tasks that resemble naturalistic human activities requires an environment to support a rich set of diverse interactions, rendering most existing embodied environments unsuitable for our investigation. One exception is VirtualHome, which we evaluate on as it models complex human activities, though only in a household setting. To measure correctness of the generated action plans, for which evaluating computationally is inherently difficult for these open-ended tasks, we conduct a human evaluation similar to Puig et al. We note that since no further training is involved throughout our investigations, the observations and findings presented in this paper should also translate to similar embodied environments, likely even beyond the household domain.\n9zf9 | Two point one Evaluated Environment: VirtualHome\nclwo | Preliminaries In VirtualHome, activities are expressed as programs. Each program consists of a sequence of textual action steps, where each step is written as: [action] (arg) (idx). Each action refers to one of the forty-two atomic actions supported in VirtualHome, such as \"walk\" and \"open\". Full list of atomic actions can be found in Appendix A point four. Different actions take in different numbers of arguments, such as \"bedroom\" and \"fridge\", that are necessary for specifying an interaction. Associated with each argument is a unique id specifying the corresponding node in the environment graph, in case of multiple instances of the same object class are present in the graph. For the sake of simplicity, we omit the id in the remaining discussions of this paper and allow automatic assignment by the environment. An example program is shown below for the task \"Relax on sofa\":\naki1 | [WALK] (living_room) (one) [WALK] (television) (one) [FIND] (television) (one) [SWITCHON] (television) (one) [FIND] (sofa) (one) [SIT] (sofa) (one) [TURNTO] (television) (one) [WATCH] (television) (one)\nvf5h | Evaluated Tasks We use the ActivityPrograms knowledge base collected by Puig et al. for evaluation. It contains two thousand eight hundred twenty-one different entries annotated by Amazon Mechanical Turk workers. Each entry contains one) a high-level task name (for example, \"Watch TV\"), two) detailed instructions expressed in natural language to complete the task (for example, \"Sit on my couch directly opposite my TV, switch on my TV with the remote control and watch\"), and three) an executable program containing all necessary steps for a robotic agent (example above). We omit the use of detailed instructions two) as we desire direct extraction of executable programs three) from only high-level task names one). There are two hundred ninety-two distinct high-level tasks in the knowledge base, from which we randomly sample eighty-eight held-out tasks for evaluation. The remaining two hundred four tasks are used as demonstration set from which we are allowed.\n```",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725394300,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 1640,
    "prompt_tokens": 3389,
    "total_tokens": 5029
  }
}