{
  "id": "chatcmpl-A3U7QvM3liuBOEXO1Tr0NUr2sMs4E",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "nkj9 | VIII. GRAPH TRANSFORMER NETWORKS AND TRANSDUCERS\n35a8 | In Section Four, GTN's were introduced as a generalization of multilayer, multimodule networks where the state information is represented as graphs instead of fixed-size vectors. This section reinterprets the GTN's in the framework of generalized transduction and proposes a powerful graph composition algorithm.\n337o | A. Previous Work\ncuth | Numerous authors in speech recognition have used gradient-based learning methods that integrate graph-based statistical models (notably HMM's) with acoustic recognition modules, mainly Gaussian mixture models, but also neural networks. Similar ideas have been applied to handwriting recognition. However, there has been no proposal for a systematic approach to multilayer graph-based trainable systems. The idea of transforming graphs into other graphs has received considerable attention in computer science\n09z2 | through the concept of weighted finite-state transducers. Transducers have been applied to speech recognition and language translation, and proposals have been made for handwriting recognition. This line of work has been mainly focused on efficient search algorithms and on the algebraic aspects of combining transducers and graphs (called acceptors in this context), but very little effort has been devoted to building globally trainable systems out of transducers. What is proposed in the following sections is a systematic approach to automatic training in graph-manipulating systems. A different approach to graph-based trainable systems, called input-output HMM, was proposed.\n13fv | B. Standard Transduction\nv5pj | In the established framework of finite-state transducers, discrete symbols are attached to arcs in the graphs. Acceptor graphs have a single symbol attached to each arc whereas transducer graphs have two symbols (an input symbol and an output symbol). A special null symbol is absorbed by any other symbol (when concatenating symbols to build a symbol sequence). Weighted transducers and acceptors also have a scalar quantity attached to each arc. In this framework, the composition operation takes as input an acceptor graph and a transducer graph and builds an output acceptor graph. Each path in this output graph (with symbol sequence Sout) corresponds to one path (with symbol sequence Sin) in the input acceptor graph and one path and a corresponding pair of input-output sequences (Sout, Sin) in the transducer graph. The weights on the arcs of the output graph are obtained by adding the weights from the matching arcs in the input acceptor and transducer graphs. In the rest of the paper, we will call this graph composition operation using transducers the (standard) transduction operation.\ntrab | A simple example of transduction is shown in Fig. twenty-eight. In this simple example, the input and output symbols on the transducer arcs are always identical. This type of transducer graph is called a grammar graph. To better understand the transduction operation, imagine two tokens sitting each on the start nodes of the input acceptor graph and the transducer graph. The tokens can freely follow any arc labeled with a null input symbol. A token can follow an arc labeled with a nonnull input symbol if the other token also follows an arc labeled with the same input symbol. We have an acceptable trajectory when both tokens reach the end nodes of their graphs (i.e., the tokens have reached the terminal configuration). This trajectory represents a sequence of input symbols that complies with both the acceptor and the transducer. We can then collect the corresponding sequence of output symbols along the trajectory of the transducer token. The above procedure produces a tree, but a simple technique described in Section Eight-C can be used to avoid generating multiple copies of certain subgraphs by detecting when a particular output state has already been seen.\ngrxs | The transduction operation can be performed very efficiently, but presents complex bookkeeping prob-\nu9dn | lems concerning the handling of all combinations of null and nonnull symbols. If the weights are interpreted as probabilities (normalized appropriately) then an acceptor graph represents a probability distribution over the language defined by the set of label sequences associated to all possible paths (from the start to the end node) in the graph.\nt25w | An example of application of the transduction operation is the incorporation of linguistic constraints (a lexicon or a grammar) when recognizing words or other character strings. The recognition transformer produces the recognition graph (an acceptor graph) by applying the neural network recognizer to each candidate segment. This acceptor graph is composed with a transducer graph for the grammar. The grammar transducer contains a path for each legal sequence of symbol, possibly augmented with penalties to indicate the relative likelihoods of the possible sequences. The arcs contain identical input and output symbols. Another example of transduction was mentioned in Section Five: the path selector used in the HOS training GTN is implementable by a composition. The transducer graph is linear graph which contains the correct label sequence. The composition of the interpretation graph with this linear graph yields the constrained graph.\nx62u | C. Generalized Transduction\naoc8 | If the data structures associated to each arc took only a finite number of values, composing the input graph and an appropriate transducer would be a sound solution. For our applications however, the data structures attached to the arcs of the graphs may be vectors, images or other\n3mdu | high-dimensional objects that are not readily enumerated. We present a new composition operation that solves this problem.\nm04v | Instead of only handling graphs with discrete symbols and penalties on the arcs, we are interested in considering graphs whose arcs may carry complex data structures, including continuous-valued data structures such as vectors and images. Composing such graphs requires additional information.\n3aip | One. When examining a pair of arcs (one from each input graph), we need a criterion to decide whether to create corresponding arc(s) and node(s) in the output graph, based on the information attached to the input arcs. We can decide to build an arc, several arcs, or an entire subgraph with several nodes and arcs.\nq3gu | Two. When that criterion is met, we must build the corresponding arc(s) and node(s) in the output graph and compute the information attached to the newly created arc(s) as a function that the information attached to the input arcs.\ntxab | These functions are encapsulated in an object called a composition transformer. An instance of composition transformer implements the following three methods:\ndov0 | One. check(arc_one, arc_two) compares the data structures pointed to by arcs arc_one (from the first graph) and arc_two (from the second graph) and returns a boolean indicating whether corresponding arc(s) should be created in the output graph;\nxd5o | Two. fprop (n_graph, upnode, downnode, arc_one, arc_two) is called when check(arc_one,arc_two) returns true; this method creates new arcs and nodes between nodes upnode and downnode in the output graph n_graph, and computes the information attached to these newly created arcs as a function of the attached information of the input arcs arc_one and arc_two;\n3gny | Three. bprop(n_graph, upnode, downnode, arc_one, arc_two) is called during training in order to propagate gradient information from the output subgraph between upnode and downnode into the data structures on the arc_one and arc_two, as well as with respect to the parameters that were used in the fprop call with the same arguments; this method assumes that the function used by fprop to compute the values attached to its output arcs is differentiable.\nws73 | The check method can be seen as constructing a dynamic architecture of functional dependencies, while the fprop method performs a forward propagation through that architecture to compute the numerical information attached to the arcs. The bprop method performs a backward propagation through the same architecture to compute the partial derivatives of the loss function with respect to the information attached to the arcs. This is illustrated in Fig. twenty-eight.\nvyos | Function generalized_composition (P_GRAPH graph_one,",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725394124,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 1644,
    "prompt_tokens": 3408,
    "total_tokens": 5052
  }
}