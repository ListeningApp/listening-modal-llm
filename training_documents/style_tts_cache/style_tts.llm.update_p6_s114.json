{
  "id": "chatcmpl-A0ZlYOl8cZYSY1zPvEemi2giDMb9U",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "```\nd0ik | B. Visualization of Style Vectors\n0x0s | To verify that our model can learn meaningful style representations, we projected the style vectors extracted from reference audios into a two-dimensional plane for visualization using t-SNE. We selected fifty samples of each emotion from a single speaker in ESD and projected the style vectors of each audio into the two-dimensional space. It can be seen in Figure two a that our style vector distinctively encodes the emotional tones of reference sentences even though the training does not use emotion labels. We also computed the style vectors using speech samples from the same speaker with our single-speaker model. This model is only\n4exe | trained on the LJSpeech dataset and therefore has never seen the selected speaker from ESD during training. Nevertheless, in Figure two b, we see that our model can still clearly capture the emotional tones of the sentences, indicating that even when the reference audio is from a speaker different from the single speaker seen during training, it still can synthesize speech with the correct emotional tones. This shows that our model can implicitly extract emotions from an unlabeled dataset in a self-supervised manner. Lastly, we show projected style vectors from ten unseen VCTK speakers each with fifty samples in Figure two c. Different speakers are perfectly separated from each other in the two-dimensional projection. This indicates that our model can learn speaker identities without explicit speaker labels and hence perform zero-shot speaker adaptation.\n0myz | C. Style-Enabled Diverse Speech Synthesis\nxh3i | To show that the learned style vectors indeed enable diverse speech synthesis, we provide an example of synthesized speech with two different reference audios using our single-speaker model trained on the LJSpeech dataset in Figure three. It can be seen clearly that the synthesized speech captures various aspects of the reference speech, including the pitch, prosody, pauses, and formant transitions. To systematically quantify this effect, we drew six scatter plots showing the correlations\nsyiw | between synthesized and reference speech in acoustic features traditionally used for speech emotion recognition. The six features are pitch mean, pitch standard deviation, energy mean, energy standard deviation, harmonics-to-noise ratio, and speaking rate. All six features demonstrate a significant correlation between the synthesized and reference speech with the correlation coefficients all above zero point six. Our model also outperforms other models on multi-speaker datasets in acoustic feature correlations. The results indicate that multiple aspects of the synthesized speech are matched to the reference, allowing flexible control over synthesized speech simply by selecting appropriate reference audios. Since our models also allow fully controllable pitch, energy, and duration, our approach is among the most flexible models in terms of controllability for speech synthesis.\nkg3t | D. Ablation Study\nlvae | We further conduct an ablation study to verify the effectiveness of each component in our model with experiments of subjective human evaluation. We instructed the subjects to compare our single-speaker model to those with one component ablated. We converted the ratings into comparative mean opinion scores by taking the difference between the mean opinion scores of the baseline and ablated models. The results are shown in table seven, and more details are in Appendix A.\no9sc | The leftmost table shows the results related to the proposed Transferable Monotonic Aligner training. When training consists of one hundred percent hard alignments so that no gradient is back-propagated to the parameters of the text aligner, the rated mean opinion score is decreased by negative zero point two six. This is due to the covariate shift between the pre-training data and text-to-speech\n121u | data. An example of bad alignment of the pre-trained external aligner is shown in Figure five. This shows that fine-tuning the aligner is effective in improving the quality of synthesized speech. However, when using zero percent hard alignment, one hundred percent soft attention alignment, the model gets overfitted to reconstruct speech with soft alignment and is unable to produce audible speech using hard alignment during inference, negative two point nine eight comparative mean opinion score. We also see that both Transferable Monotonic Aligner objectives, equations three and four, are important for high-quality speech synthesis.\nzh03 | The table in the middle shows the effects of removing various training techniques and components. Using an external pitch extractor, such as acoustic-based methods, decreases the mean opinion score by negative zero point one one. This is likely caused by the acoustic-based pitch extraction method sometimes failing to extract the correct F-zero curve, and fine-tuning the pitch extractor along with the decoder makes the model learn better pitch representation. Without a pre-trained text aligner, the rated mean opinion score is decreased by negative zero point three nine. This indicates that our transfer learning is helpful for mitigating overfitting problems when training internal aligners with a relatively small dataset. Removing our novel duration-invariant data augmentation also lowers the performance. Lastly, training without discriminators significantly affects the perceived sound quality.\n1mvw | The rightmost table shows architecture changes by removing the residual features and replacing the AdaIN components in the decoder and predictor with instance normalization, AdaLN, and simple feature concatenation. Their effects on style reflection are also shown in Table eight. Removing the residual features in the decoder decreases both naturalness and correlations between synthesized and reference speech. Layer normalization is also worse than instance normalization for both metrics. Concatenating styles in place of AdaIN dramatically decreases the correlations and lowers rated naturalness, confirming our observation that all previous methods that use concatenation to incorporate style information are not as effective as AdaIN due to the lack of temporal modulations. Lastly, we see that replacing AdaIN with instance normalization does not significantly affect the rated naturalness,\n6d4s | suggesting that the improved naturalness is not due to the introduction of styles but our novel technical improvements including Transferable Monotonic Aligner, data augmentation, use of instance normalization, pitch extractor, and residual features. Nevertheless, styles enable diverse speech synthesis which models without styles cannot do.\nmqah | V. CONCLUSIONS\nxxr8 | We introduced StyleTTS, a novel natural and diverse text-to-speech synthesis approach. Our research takes a distinctive step forward in leveraging the strengths of parallel text-to-speech systems with several novel constitutions that include a unique transferable monotonic aligner training while integrating style information via AdaIN. We demonstrated that this method can effectively reflect stylistic features from reference audio. Moreover, the style vectors from our model encode a rich set of information present in the reference audio, including pitch, energy, speaking rates, formant transitions, and speaker identities. This allows easy control of the synthesized speech's prosodic patterns and emotional tones by choosing an appropriate reference style while benefiting from robust and fast speech synthesis of parallel text-to-speech systems. Collectively, they enable natural speech synthesis with diverse speech styles that go beyond what was achieved in previous text-to-speech systems.\nrl4j | Our contribution lies not only in the theoretical underpinnings but also in its practical applicability. Our approach empowers various new applications, including movie dubbing, book narration, unsupervised speech emotion recognition, personalized speech generation, and any-to-any voice conversion. Our source code and pre-trained models are publicly available to assist research in this area further.\nvz32 | VI. ACKNOWLEDGMENTS\nncb0 | In this section, we describe the detailed settings of each condition in Table seven and provide more discussions of the results in Table seven and Table eight.\n8mp0 | A. TMA-related\n```",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1724700848,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_3aa7262c27",
  "usage": {
    "completion_tokens": 1541,
    "prompt_tokens": 3325,
    "total_tokens": 4866
  }
}