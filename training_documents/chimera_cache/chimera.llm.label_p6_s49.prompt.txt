You will receive raw text from an OCR scan of a document or web page. Each line of input represents one textbox from the document.

Your task is to label each textbox with exactly one of the following labels:
  title: the primary document title, no more than one per document
  heading: the name of the following chapter or section (should never start in the middle of the previous paragraph)
  subheading: a secondary heading, often following another heading (should never start in the middle of the previous paragraph)
  body: main paragraph content: may start or end in the middle of a word, use context to follow paragraphs that are split across textboxes (if a textbox contains both heading and body text, label it "body")
  math: textbox primarily containing math
  imageDescription: standalone callout that only describes an image, chart, table, or similar (typically begins with "Table...", "Figure...", "Fig. ...", "Above...", "Left...", etc.)
  authors: names of contributors to this document
  institutions: list of universities, business, and other institutions affiliated with this document
  publisher: info about the publisher, or provided by the publisher, including copyright info
  pagenum: the page number
  headerOrFooter: boilerplate text page number, or redundant headings at the top/bottom of the page
  toc: the table of contents
  references: bibliographic references and resources
  acknowledgements: thanks and credits to contributors
  appendix: index, appendix, or any other end matter not part of the main thread (including related headings)
  table: tabular data, label, title, or subheading for a grid or list of data
  datum: numeric data or data label from a chart or list
  advertisement: promotional ads and marketing
  layout: non-content related to the layout of the page, e.g. sidebar navigation
  callout: pull quote or long block text that stands apart from the main text
  footnote: footnotes and endnotes near the bottom of the page
  keywords: list of keywords or tags
  url: web address or email
  other: any other text that does not belong to the main thread

Each line of input is prefixed with a textbox ID in the format `id | Text content`. The output must be in the format `id | label`.

Note that body text may be split across multiple textboxes, and textboxes may start or end in the middle of a sentence or word. Because the text was extracted from a web page or document, paragraphs of body text may be interrupted by multiple textboxes of redundant headers, footnotes, page numbers, tables, images, etc. For example, a real heading will never interrupt the middle of a sentence. Use context clues like this to carefully label each textbox.

EXAMPLE INPUT:
1pjs | Neuroscience Applied 1 (2022) 101012
o2kr | Contents lists available at ScienceDirect
v6sk | Neuroscience Applied
1wj6 | journal homepage: www.journals.elsevier.com/neuroscience-applied
936l | Research Articles
06yq | Microdosing with psychedelics to self-medicate for ADHD symptoms in adults: A naturalistic study
dpv6 | ARTICLE INFO
8r5j | ABSTRACT
2taz | Keywords: ADHD Microdosing Psychedelics Self-medication Well-being Time perception
o3ya | ADHD in adulthood is often overlooked, which negatively affects the individual's well-being. First-line phar- macological interventions are effective in many ADHD patients, relieving symptoms rapidly. However, there seems to be a proportion of individuals who discontinue, or fail to respond to these treatments. For these in- dividuals, alternative treatment options should be explored.
r5il | 1. Introduction
5gui | Attention deficit hyperactivity disorder (ADHD) is one of the most common developmental disorders worldwide. Prevalence research indicates that 2.6% of the adult population has persistent ADHD. ADHD in adults is often overlooked because of the high comorbidity rate and lack of knowledge about how ADHD is expressed in adulthood (Kooij et al.,
9d4o | Fig. 1. Flowchart of included participants for each time point.
82qc | 2019). In addition, ADHD is associated with deficits in various domains of cogni- tive functioning. Twenty-five percent of ADHD cases suffered from ADHD symptoms purely because of de- ficiencies in temporal processing (Sonuga-Barke et al., 2010).
ls7d | First-line ADHD treatments in adults mainly include pharmacological interventions to enhance dopaminergic and noradrenergic neurotrans- mission with stimulants. Overall, they have been proven to work effectively in adults with ADHD, inducing fast symptom relief and thereby enhancing the person's quality of life. In the
5esx | Table 1 Demographic information from respondents at baseline and the two and four- week time points.
t8vv | longer term, approximately twenty percent of ADHD patients discontinue their prescribed medication after six to nine months, thirty percent after one year, and half of them after two years.
xrd1 | 2. Material and methods
5746 | 2.1. Study design and participants
7toz | The study employed a naturalistic design, assessing the experiences of participants at baseline,
j346 | Neuroscience Applied 1 (2022)
m5ka | E.C.H.M. Haijen et al.
kx2c | REVIEW
v8uq | before they start MD on their own initiative, and at two and four weeks after MD initiation. The target population included adults diagnosed with ADHD and individuals who experienced ADHD symptoms to the extent that these interfered with their daily lives and who had not been diag- nosed with ADHD before. To be included in the analyses, participants needed to score above a cut-off point on at least one of the subscales of the Conner's Adult ADHD Rating Scale (CAARS-S:SV).
2zmw | Fig. 2. Mean scores of the CAARS-S:SV DSM-IV total symptoms T-scores at baseline (0W) and two (2W) and four weeks (4W) after MD (A) of the whole sample, and (B) per conventional ADHD medication use. Error bars represent mean error.
sdk3 | <LATEX>\lim _ { x ightarrow \infty } rac { 6 x ^ { 2 } + 1 0 0 } { 7 x ^ { 2 } - 1 0 } =</LATEX>
x972 | 2.2. Study procedure
k221 | Mean performance measures of the CAARS-

EXAMPLE OUTPUT:
1pjs | headerOrFooter
o2kr | publisher
v6sk | publisher
1wj6 | publisher
936l | headerOrFooter
06yq | title
dpv6 | other
8r5j | heading
2taz | keywords
o3ya | body
r5il | heading
5gui | body
9d4o | imageDescription
82qc | body
ls7d | body
5esx | imageDescription
t8vv | body
xrd1 | heading
5746 | subheading
7toz | body
j346 | headerOrFooter
m5ka | authors
kx2c | headerOrFooter
v8uq | body
2zmw | imageDescription
sdk3 | math
x972 | heading
k221 | body




d6r4 | To select the best configuration of <LATEX>W</LATEX> and <LATEX>D ,</LATEX> we build a perfor- mance model to predict the runtime of a single training iteration (represented by <LATEX>\left. T \right)</LATEX> for each available configuration. For the compu-
9mrz | we measure the runtime of the forward pass on a <LATEX>F _ { t } .</LATEX> The runtime of backward pass (represented as
pk06 | two times of the forward pass if no activation recomputation is used, and three times otherwise. We define the critical path as a series of pipeline stages with dependency that determine the overall computation overhead of a single training iteration. An example of critical path is shown in Figure 6. Let <LATEX>C _ { f }</LATEX> and <LATEX>C _ { b }</LATEX> denote the number of forward passes and backward passes on the critical path of the pipeline, respectively. For the example shown in Figure 6, <LATEX>C _ { f } = 6</LATEX> <LATEX>C _ { b } = 1 0 .</LATEX> The total computation overhead is
7wn9 | <LATEX>F _ { t } C _ { f } + B _ { t } C _ { b } .</LATEX> To model the communication overhead, we assume bidirectional and direct point-to-point communication between the compute nodes, and use the classic Latency-Bandwidth <LATEX>\mathrm { d t h } \left( \alpha - \beta \right) .</LATEX> The cost of sending a message of size L is <LATEX>\alpha + \beta I ,</LATEX> <LATEX>\alpha</LATEX> (latency) and <LATEX>\beta</LATEX> (the transfer time per word) can be measured using micro benchmarks. As discussed in Section 3.2, Chimera has two types of communication: <LATEX>p 2 p</LATEX> communication <LATEX>\left( C o m m _ { p 2 p } \right)</LATEX> between stages and allreduce <LATEX>\left( \mathrm { C o m m } _ { a l l r e d u c e } \right)</LATEX> for gradient synchronization. <LATEX>C o m m _ { p 2 p }</LATEX> can be simply modelled by the <LATEX>\alpha - \beta</LATEX> cost model. The total <LATEX>p 2 p</LATEX> communication cost is <LATEX>\left( C _ { f } + C _ { b } \right) C o m m _ { p 2 p } .</LATEX> Note that <LATEX>C o m m _ { P } 2 p</LATEX> can be partially overlapped by the intermediate bubbles if there are any, but to simplify the model we do not consider that effect.
wcd0 | For Commallreduce, we assume its implementation makes use of Rabenseifner's algorithm [42, 53], whose cost is
caos | <LATEX>= 2 \left( \log _ { 2 } r \right) \alpha + 2 \left( r - 1 \right) \beta L / r</LATEX> where L is the size of gradients to be synchronized and <LATEX>r</LATEX> is the num- ber of stage replicas. Note that Rabenseifner's algorithm reaches the lower bound on the bandwidth term for host-based allreduce, and therefore works best for large models. We model the effect of com- munication overlapping (discussed in Section 3.2) for Comm allreduce. Figure 6 shows an example of the free regions (i.e., exceeding which will increase the total runtime) utilized in Chimera to over- lap the gradient synchronization. Note that there are two stage replicas on each worker. Regions (a) and (b) can be utilized to overlap the gradient synchronization for the first stage replica (the one with a larger stage ID), and region (c) can be utilized to overlap the gradient synchronization for both stage replicas. Let <LATEX>\mathrm { C o m m } _ { \mathrm { u n o v e r l a p p e d } } \left( i \right)</LATEX> represent the portion of <LATEX>\mathrm { C o m m } _ { a l l r e d u c e }</LATEX> which can not be overlapped by the free regions on worker <LATEX>i</LATEX> i, and then the max of <LATEX>\mathrm { C o m m } _ { u n o v e r l a p p e d } \left( i \right)</LATEX> among the <LATEX>D</LATEX> workers con- tributes to the total runtime.
qrvy | Overall, the runtime of <LATEX>a</LATEX> single training iteration is modelled as
pn9j | <LATEX>T = \left( F _ { t } + C o m m _ { p 2 p } \right) C _ { f } + \left( B _ { t } + C o m m _ { p 2 p } \right) C _ { b } +</LATEX> <LATEX>m a x \left\{ C o m m _ { u n o v e r l a p p e d } \left( i \right) : i \in \left[ 0 , D - 1 \right] \right\} .</LATEX> (1)
1pmc | We use this model to select the best configuration of <LATEX>W</LATEX> and <LATEX>D</LATEX> (see Section 4.2.2).
5z6q | 3.5 Scale to More Micro-Batches
yeoh | For a large <LATEX>\widehat { \mathcal{B} } ,</LATEX> there may be more than <LATEX>D</LATEX> micro-batches in a training iteration for each worker (i.e., <LATEX>\left. N > D \right) ,</LATEX> especially when the compute resources are limited. To scale to a large <LATEX>\widehat { B } ,</LATEX> we first choose the
b8hf | <LATEX>B</LATEX> <LATEX>D</LATEX> <LATEX>a n d \quad s c h e d u l e \quad t h e s e \quad D \quad m i c r o - b a t c h e s \quad u s i n g \quad b i d i r e c t i o n a l \quad p i p e l i n e s \quad a s</LATEX> as a basic scheduling unit, and scale to a large <LATEX>\widehat { B }</LATEX> by concatenating K <LATEX>\left( K = N / D \right.</LATEX> and <LATEX>\left. N = \widehat { B } / W / B \right)</LATEX> basic units together. Figure 7(a) presents an example with N=2D micro-batches in a training iteration for
hntf | Figure 8: Chimera with a combination of four 8-stage pipelines.
99fo | each worker, which has two basic scheduling units (i.e., <LATEX>\left. K = 2 \right) .</LATEX> We propose three methods to concatenate multiple basic units. (1) Direct concatenation, as shown in Figure 7(b). The bubbles at the end of the first basic unit can be occupied by the forward passes at the beginning of the second basic unit. If the backward pass has the same workload as the forward pass, basic units can be concatenated seamlessly. However, backward pass has about two times workload of the forward pass, which results in intermediate bubbles.
y5r7 | To remove the intermediate bubbles of direct concatenation, we propose (2) forward doubling (shown in Figure 7(d)) and (3) back- ward halving, in which the key idea is to equalize the workloads of forward and backward passes. In forward doubling, we increase the number of micro-batches for each forward pass to two, and then
lrry | backward passes, each <LATEX>o f \quad w h i c h \quad h a s \quad o n l y \quad o n e \quad m i c r o - b a t c h , a s \quad s h o w n \quad i n \quad F i g u r e \quad 7 \left( c \right) . T h e n ,</LATEX> we fine-tune the schedule to remove 50% bubbles at the beginning
hvhm | of the pipeline, as shown in Figure 7(d). Forward doubling removes the intermediate bubbles, but it leads to two times activation mem- ory consumption and therefore may exceed the device memory capacity. We resort to activation recomputation to reduce memory overhead. Note that recomputation increases the workload of the backward pass, but the p2p communication overhead in the forward passes is also doubled because of the outputs for two micro-batches; therefore, we still treat the forward pass (with two micro-batches) and the backward pass (with one micro-batch and recompute) have approximately equal workload. Forward doubling prefers large mod- els in which even <LATEX>B = 1</LATEX> exceeds the device memory capacity, since in such case activation recomputation must be used. For smaller models which has a larger <LATEX>B ,</LATEX> we propose to use backward halving, which uses the same schedule as forward doubling, except that rather than executing two micro-batches in the forward pass but to halve the micro-batch size of the backward pass. Backward halving does not increase the activation memory (thus no activation recom- putation), but it may lower the computational using a sub-max B. To select the best of the <LATEX>\begin{array}{} \text { d efficiency because of } \\ \text { inree methods is not a } \end{array}</LATEX> priori, which we rely on empirical results. Note that both forward doubling and backward halving have total <LATEX>D - 2</LATEX> bubbles (D/2-1 in the forward passes and D/2-1 in the backward passes), as shown in Figure 7(d), which is about a 50% reduction compared with DAPPLE and GPipe. For <LATEX>K > 2 ,</LATEX> we use the schedule of 2D micro-batches as a basic scheduling unit (as shown in Figure 7(c)) for forward doubling
863b | and backward halving, and concatenate <LATEX>\left\lfloor k / 2 \right\rfloor</LATEX> basic units and the residual D micro-batches if <LATEX>K</LATEX> is odd.
fj24 | One more benefit for both forward doubling and backward halv- ing is that they have more space to overlap <LATEX>p 2 p</LATEX> communication (in the forward passes) than the classic 1F1B schedule [38, 39]. In Figure 7(d), taking the forward pass on micro-batch 1 as an example, the <LATEX>p 2 p</LATEX> communication from P1 to P2 can be overlapped by the intermediate forward pass computation, while for 1F1B there may be not enough computation to overlap <LATEX>p 2 p</LATEX> communication.
uz3c | 3.6 Generalize to More than Two Pipelines
d5bi | So far we have only discussed the case that two pipelines (one down and one <LATEX>\left. u p \right)</LATEX> are combined together in Chimera. Yet, Chimera can be generalized to combine more than two pipelines for an even number of pipeline stages (i.e., <LATEX>D</LATEX> is even). For <LATEX>Q = D / 2 ,</LATEX> let <LATEX>F</LATEX> denote the set of <LATEX>\mathrm { a l l }</LATEX> the divisors of <LATEX>Q ,</LATEX> including 1 and <LATEX>\mathcal{Q}</LATEX> itself. For any <LATEX>f \in F ,</LATEX> we can generate a scheme for Chimera, which combines <LATEX>f</LATEX> down pipelines and <LATEX>f</LATEX> up pipelines together and each pipeline has <LATEX>D / 2 f</LATEX> micro-batches scheduled by the 1F1B strategy. Figure 8 gives an example in which Chimera combines four pipelines with eight stages (i.e., <LATEX>D = 8</LATEX> and <LATEX>\left. f = 2 \right) .</LATEX> For the down pipeline <LATEX>i \left( i \in \left[ 0 , f - 1 \right] \right) ,</LATEX> the <LATEX>D</LATEX> stages are mapped to the <LATEX>D</LATEX> workers in turn with the first stage (i.e., stage0) being mapped to the worker <LATEX>i * \left( D / f \right) .</LATEX> For example, for the down pipeline1 in Figure 8, stages [0,1,2,3,4,5,6,7] are mapped to workers [4,5,6,7,0,1,2,3], respectively. For the <LATEX>f</LATEX> up pipelines, the <LATEX>D</LATEX> stages are mapped to the <LATEX>D</LATEX> workers in a completely reverse order of the corresponding down pipeline. It can be easily demonstrated that the schedules of the <LATEX>2 f</LATEX> pipelines can be overlaid without conflict.
ba4l | <LATEX>\left[ \left( D - D / 2 f + 1 \right) M _ { a } \right. ,</LATEX> <LATEX>\left. D * M _ { a } \right]</LATEX> For any <LATEX>f \in F ,</LATEX> Chimera can scale to more micro-batches (i.e., <LATEX>\left. N > D \right)</LATEX> using the methods discussed in Section 3.5. For a given <LATEX>f ,</LATEX> Chimera incurs <LATEX>2 \left( D / f / 2 - 1 \right)</LATEX> bubbles, but has to maintain <LATEX>2 f</LATEX> model replicas and synchronize the gradients of 2f stages on each worker.