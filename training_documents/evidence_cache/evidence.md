## Experimental Evidence on the Productivity Effects of Generative Artificial Intelligence

We examine the productivity effects of a generative artificial intelligence technology-the assistive chatbot ChatGPT-in the context of mid-level professional writing tasks. In a preregistered online experiment, we assign occupation-specific, incentivized writing tasks to four hundred forty-four college-educated professionals, and randomly expose half of them to ChatGPT. Our results show that ChatGPT substantially raises average productivity: time taken decreases by zero point eight standard deviations and output quality rises by zero point four standard deviations. Inequality between workers decreases, as ChatGPT compresses the productivity distribution by benefiting low-ability workers more. ChatGPT mostly substitutes for worker effort rather than complementing worker skills, and restructures tasks towards idea-generation and editing and away from rough-drafting. Exposure to ChatGPT increases job satisfaction and self-efficacy and heightens both concern and excitement about automation technologies.


## One Introduction

Recent advances in generative artificial intelligence may have widespread implications for production and labor markets. New generative AI systems like ChatGPT or DALL-E, which can be prompted to create novel text or visual outputs from large amounts of training data, are qualitatively unlike most historical examples of automation technologies. Previous waves of automation predominantly impacted "routine" tasks consisting of explicit sequences of steps that could be easily codified and programmed into a computer. Creative, difficult-to-codify tasks (such as writing and image generation) largely avoided automation-a pattern that scholars noted might change with the advent of the deep learning techniques that now underpin generative AI systems.

The emergence of powerful generative AI technologies reintroduces a host of classic questions in a new context. Automation technologies-by definition-perform specific tasks in place of humans. But, more broadly, they may either displace humans completely from certain occupations or complement existing human workers and increase their productivity. Insofar as automation technologies mostly displace human workers, they can increase unemployment, while their impacts on aggregate productivity may be small or nonexistent to the degree that they mainly serve to redistribute income from workers to owners of capital. Insofar as automation complements existing workers, it can simultaneously benefit workers, capital owners, and consumers by raising productivity and wages and lowering prices.

For example, a potent generative writing tool like ChatGPT might entirely replace certain kinds of writers, such as grant writers or marketers, by letting companies directly automate the creation of grant applications and press releases with minimal human oversight. This might not increase the quality of the resulting written output, but would let companies save on wage costs by eliminating human labor. Alternatively, a tool like ChatGPT could substantially raise the productivity of grant writers and marketers, for example by automating relatively routine, time-consuming subcomponents of their writing tasks, such as translating ideas into an initial rough draft. In this case, demand for these services could expand, resulting in higher employment and wages as well as greater productivity for companies and cheaper products for consumers. Inequalities between workers could also be affected: inequality could decrease if lower-ability workers are helped more by ChatGPT, or increase if higher-ability workers have the skills necessary to take advantage of the new technology.

Which of these eventualities will generative AI systems bring about? The answer depends on a host of questions: how do generative AI systems affect workers' productivity in existing tasks? Do they affect productivity largely by substituting for worker effort or complementing worker skills? Do they differentially affect low- or high-ability workers, or workers with different skill profiles? Do they affect workers' enjoyment of their work?

This paper takes the first step towards answering these questions. In an online experiment, we recruit four hundred forty-four experienced, college-educated professionals and assign each to complete two occupation-specific, incentivized writing tasks. The occupations we draw on are marketers, grant writers, consultants, data analysts, human resource professionals, and managers. The tasks, which include writing press releases, short reports, analysis plans, and delicate emails, comprise twenty-to thirty-minute assignments designed to resemble real tasks performed in these occupations; indeed, most of our participants report completing similar tasks before and rate the assigned tasks as realistic representations of their everyday work. Participants face high-powered incentives, in the form of large bonus payments, to produce high-quality work. Quality is assessed by experienced professionals working in the same occupations. Evaluators are asked to treat the output as if encountered in a work setting and are incentivized to grade outputs carefully. Evaluators assign an overall grade as well as separate grades for writing quality, content quality, and originality. Each piece of output is seen by three evaluators, with an average within-essay cross-evaluator correlation of zero point four-four.

A randomly-selected fifty percent of our participants-the treatment group-are instructed to sign up for ChatGPT between the first and second task, are walked through how to use it, and are told they are permitted to use it on the second task if they find it useful. The control group is instead instructed to sign up for the LaTeX editor Overleaf. This design allows us to estimate the causal effects of ChatGPT using a combination of within-person and between-person variation, and performance on the first task serves as a measure of baseline ability that enables our inequality analyses.

We collect participants' output and elicit total time taken, time taken on various subcomponents of the task, job satisfaction, self-efficacy, and beliefs about automation. We also take a snapshot of each participant's output each minute while they perform the task, to construct an objective measure of time active on the task and to detect ChatGPT usage in the control group and on the pre-treatment task.

A complete description of our experimental design, a copy of relevant survey questionnaires, and additional figures validating our central measures and extending our main results are included in the Online Appendix. Descriptive statistics about the sample, as well as balance and selective attrition tests, are available in Table One. The attrition rate is five percent in the control group and ten percent in the treatment group. Balance tests indicate that across thirteen pre-treatment characteristics, the treatment and control group exhibit a small significant difference only for only two characteristics (employment status and being an HR professional). Our partly within-person design, which controls for performance on the pre-treatment task, should eliminate any influence of selective attrition on our results; in the Online Appendix, we also report Lee two thousand nine bounds on our main results and versions of our results controlling for employment status and occupation, which confirm that our results are highly robust to selective attrition.


## Two Results

Two point one Takeup of ChatGPT

In the treatment group, ninety-two percent of treated participants successfully sign up for ChatGPT, and eighty-one percent choose to use it on the second task, giving it an average self-assessed usefulness score of four point four out of five.

Prior to treatment, about seventy percent of our participants had heard of ChatGPT and thirty percent had used it before. Self-reported and objective measures indicate only ten to twenty percent of the control group use ChatGPT on the tasks, meaning there is at least a sixty percentage point experimentally-induced gap in usage between our treatment and control groups on the second task. The fact that some control participants are using the tool means our estimates provide lower bounds on the effects of ChatGPT usage on productivity.


## Two point two Productivity

We measure productivity as earnings per minute. Figure one shows that the experimental intervention increases this outcome dramatically. In the treatment group, time taken on the post-treatment task drops by ten minutes, thirty-seven percent, relative to the control group, who take an average of twenty-seven minutes. Average evaluator grades in the treatment group increase by zero point four five standard deviations, with roughly similar increases for overall grades and specific grades for writing quality, content quality, and originality.

Figure one Panels (c) and (d) show that these effects are not limited to specific pockets of the time or grade distributions: the entire time distribution shifts to the left (faster work) and the entire grade distribution shifts to the right (higher quality). At the individual worker level, Figure two shows that treated workers who received a low grade on the first task experience both increases in grades and decreases in time spent, while workers who received a high grade maintain their grade level while substantially reducing their time spent.

These results are virtually identical across our two main incentive schemes, covering eighty percent of respondents: a "linear" scheme in which respondents are paid one dollar for each point they receive on each submission (each of which is graded on a one-to-seven point scale), and a "convex" scheme in which respondents are additionally paid three dollars for earning a grade of six or seven, giving them an extra incentive to produce high-quality output.

Two supplementary interventions allow us to probe further. In one arm involving twenty percent of participants, we require participants in both the treatment and control group to spend exactly fifteen minutes on each task. This holds effort fixed across the treatment and control groups, allowing us to interpret any difference in grades as a pure effect of ChatGPT access on productive capacity. In this arm, the treatment increases grades by a similar zero point three nine standard deviations, albeit imprecisely estimated and with a slight imbalance in pre-treatment outcomes.

In another arm involving thirty percent of the treatment group, after completing the second task, respondents are shown their first-task output and given the opportunity to edit or replace it using ChatGPT if they wish. Twenty-three percent choose to replace their response with ChatGPT's output and twenty-five percent use ChatGPT to edit their original response, suggesting that participants view ChatGPT as a way to improve output quality in addition to a convenient way to save time.


## Two point three Productivity Inequality

The control group exhibits persistent productivity inequality: participants who score well on the first task also tend to score well on the second task. As Figure two Panel (a) shows, there is a correlation of zero point four nine between a control participant's average grade on the first task and their average grade on the second task.

In the treatment group, initial inequalities are half-erased by the treatment: the correlation between first-task and second-task grades is only zero point two five. This reduction in inequality is driven by the fact that participants who scored lower on the first round benefit more from ChatGPT access, as the figure shows: the gap between the treatment and control lines is much larger at the left-hand end of the x-axis.


## Two point four Human-Machine Complementarity

ChatGPT could increase workers' productivity in two ways. On the one hand, it could substitute for worker effort by quickly producing output of satisfactory quality that workers directly submit, letting them reduce the time they spend on the task. On the other hand, it could complement workers' skills: humans and ChatGPT working together could produce more than the sum of their parts, for example if ChatGPT aids with the brainstorming process, or quickly produces a rough draft and humans then edit and improve on the draft. In our experiment, evidence for the complementarity story could come in two forms: (a) we could observe treatment-group participants choosing to expend significant time editing ChatGPT's output or repeatedly prompting ChatGPT in anticipation of earning higher grades, and (b) we could observe that treatment participants' essays receive higher grades than ChatGPT's raw output, suggesting that human input adds value.

We observe neither of these pieces of evidence, suggesting that ChatGPT is increasing productivity primarily by substituting for worker effort. Sixty-eight percent of treated participants report submitting ChatGPT's initial output without editing it, and on average treated participants are active on the task for only three minutes after we first observe them pasting in a large quantity of text (presumably from ChatGPT). There is also no correlation between how long a participant is active after pasting in the ChatGPT text and the grade they ultimately receive, and treated respondents do not receive higher average grades than raw ChatGPT output that we give to evaluators to grade, meaning we find no evidence that human editing is improving the ChatGPT output. This is true even when participants are given strong pecuniary incentives to do so, in the convex incentives group.


## Two point five Task Structure

As suggested by the preceding discussion, ChatGPT substantially changes the structure of writing tasks. Figure three Panel A shows that prior to the treatment, participants spend about twenty-five percent of their time brainstorming, fifty percent writing a rough draft, and twenty-five percent editing. Post-treatment, the share of time spent writing a rough draft falls by more than half and the share of time spent editing more than doubles.


## Two point six Skill Demand

If ChatGPT is especially helpful to those with poor writing and communication skills relative to their other skills, it could have major labor market implications by expanding the available occupational choices and raise the earnings of individuals with strong idea-generation skills who struggle to effectively get those ideas onto paper.

We perform several tests of this hypothesis. We construct two measures of a person's relative writing skills. First, at the beginning of the experiment, we ask participants to rank from one to three their skills at communication (writing and speaking), problem solving, and creativity. Second, in addition to assigning overall grades, evaluators separately assess each piece of output based on writing quality, content quality, and originality; the gap between a person's first-task overall score and their writing score affords another measure.

Similarly, we construct two measures of the individual-level benefits of ChatGPT. First, at the end of the experiment, we ask treatment-group participants how much they would be willing to pay on a monthly basis to access ChatGPT in their jobs. Second, we measure how much each treatment participant's grade increases from the first to the second task. We find no clear evidence for the aforementioned hypothesis. Figure three Panel B shows that average willingness to pay for ChatGPT is flat across the terciles of both our measures of writing skill: respondents, regardless of their writing skills, are willing to pay about zero point five percent of their monthly salary for a monthly subscription to ChatGPT. Grade gains from ChatGPT are also roughly flat across both measures of relative writing skills: people with comparatively poor writing skills do not experience unusually large grade gains.


## Two point seven Job Satisfaction and Self-Efficacy

Access to ChatGPT could affect job satisfaction. For example, it could make participants happier by automating tedious or annoying components of the task or allowing them to finish more quickly. Alternatively, it could make the experience less enjoyable by quickly automating the most fun parts of the task. It could similarly either boost self-efficacy by giving participants access to a complex and powerful tool that enhances their capabilities, or it could lower it by making participants feel superfluous. We measure job satisfaction with a question, after each task, about how much participants enjoyed the task, and self-efficacy with a question about how skilled or effective they felt while completing the task, both on one to ten Likert scales.


## Two point eight Beliefs About Automation

Many of our treated participants had never heard of or never used ChatGPT before participating in the experiment. Hence, most essentially encounter the technology for the first time and receive a crash course on its usefulness for writing tasks. How are their beliefs about future waves of automation affected by this encounter?

After respondents complete their second task, we elicit three beliefs, each on a one to ten scale: how worried they are about workers in their occupation being replaced by artificial intelligence; how optimistic they are that artificial intelligence will make workers in their occupation more productive; and, overall, how optimistic or pessimistic they feel about future advances in artificial intelligence. The effects of treatment on these outcomes are displayed in Figure four Panel C; worry about automation increases by zero point two six standard deviations, excitement by zero point three nine, and net optimism increases by about zero point two zero.

Zero point zero three seven.


## Two point nine Two-Week Followup Survey

One indication of the value of ChatGPT to participants is whether they continue to use it after the experiment. To track whether participants are subsequently using ChatGPT in their real jobs, we resurvey them two weeks after their completion of the initial survey. This followup is still in progress, with an eighty-two percent response rate among the four hundred twenty-three respondents who have been invited so far, and no evidence of differential response rates by treatment status.

Thirty-three percent of former treatment group participants have used ChatGPT in their job in the past week, relative to eighteen percent of control group participants. Restricting to workers who had not previously used ChatGPT when they participated in our main experiment, twenty-six percent of treated and nine percent of control workers are now using ChatGPT in their jobs. Users give it an average usefulness score of three point six five out of five point zero zero, somewhat lower than in our main experiment, likely owing to the greater length and complexity of real-world tasks. The range of tasks they report using it for is broad: generating recommendation letters for employees, responding to customer service requests, brainstorming, search-engine requests, rough-drafting emails, and so on.

Respondents who are not using ChatGPT in their jobs mostly report that this is because the chatbot lacks context-specific knowledge that forms an important part of their writing. For example, they report that their writing is "very specifically tailored to their customers and involves real-time information" or "unique and specific to their company products."

These comments point to an important and inherent limitation of our experiment: it involves relatively small, self-contained tasks that lack much context-specific knowledge beyond what we stipulate in the task prompts. However, our core result, that ChatGPT can increase productivity on many mid-level professional writing tasks, is supported by the fact that many respondents choose to use it in their real jobs. The fact that the treatment group is substantially more likely to use ChatGPT than the control group also suggests that the dissemination of ChatGPT into real professional activity is still in its very early stages, with many people not using it due to a lack of knowledge about or experience with the technology.

There is no difference between the former treatment and control in their overall satisfaction with their job in the followup survey. That said, many of our respondents only started using ChatGPT in their job in the past week or two, and it may take longer for access to ChatGPT to affect overall job satisfaction.


## Three Discussion

College-educated professionals performing mid-level professional writing tasks experience substantial increases in productivity when given access to ChatGPT. The generative writing tool increases the output quality of low-ability workers while reducing their time spent, and it allows high-ability workers to maintain their quality standards while becoming significantly faster. At the aggregate level, ChatGPT substantially compresses the productivity distribution, reducing inequality. It is also already being used by many workers in their real jobs. The experimental evidence suggests that ChatGPT largely substitutes for worker effort rather than complementing workers' skills, potentially causing a decrease in demand for workers, with adverse distributional effects as capital owners gain at the expense of workers.

The experiment has several important limitations worth enumerating. First, the tasks are relatively short, self-contained, and lack a dimension of context-specific knowledge, which may inflate our estimates of ChatGPT's usefulness. The results on job satisfaction and self-efficacy are similarly limited, reflecting enjoyment of a small task rather than feelings about a worker's whole job, as evidenced by the fact that there is no difference between the treatment and control groups in real job satisfaction after two weeks. Second, an experiment, by its nature, captures only direct, immediate effects of ChatGPT on the selected occupations. There will be many indirect, reinforcing, or counteracting "general-equilibrium" effects as labor markets and production systems adapt to the advent of technologies like ChatGPT. The effects of ChatGPT will also likely vary by occupation, task, and skill level.

Only time and future research will fully reveal how ChatGPT and its successors will affect labor markets. For now, the evidence we provide suggests that generative AI technologies will-and have already begun-to noticeably impact workers.

Fourteen