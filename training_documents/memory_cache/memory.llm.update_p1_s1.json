{
  "id": "chatcmpl-A3UDtyrxfb1HzJtB2wZ42ANInMgNT",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "```plaintext\n90no | Working Memory Capacity of ChatGPT: An Empirical Study\njt0y | Abstract\nqb7o | One Working memory is a critical aspect of both human intelligence and artificial intelligence, serving as a workspace for the temporary storage and manipulation of information. In this paper, we systematically assess the working memory capacity of ChatGPT, a large language model developed by OpenAI, by examining its performance in verbal and spatial n-back tasks under various\noae5 | conditions. Our experiments reveal that ChatGPT experiences significant declines in performance as n increases (which necessitates more information to be stored in working memory), suggesting a limit to the working memory capacity strikingly similar to that of humans. Furthermore, we investigate the impact of different instruction strategies on ChatGPT's performance and observe that the fundamental\ns0fl | patterns of a capacity limit persist. From our empirical findings, we propose that\n43ia | n-back tasks may serve as tools for benchmarking the working memory capacity\n3g2y | of large language models and hold potential for informing future efforts aimed\ngc7v | at enhancing AI working memory and deepening our understanding of human working memory through AI models.\nwqfv | One Introduction\nj30a | The advent of large language models like ChatGPT and GPT-four has propelled the\n7bdk | pursuit of artificial general intelligence and unveiled human-level abilities that warrant further\nfim0 | exploration. Among these abilities is the capacity to retain contextual information while\nojro | engaging in multi-turn conversations, suggesting the presence of working memory in these LLMs.\nl6t9 | In cognitive sciences, working memory is usually defined as the ability to temporarily store and manipulate information in mind. It is widely regarded as a critical element of human intelligence, as it underlies various higher-order cognitive processes such as reasoning, problem-solving, and\n1ujy | language comprehension.\nqzor | Studies on human participants have revealed a fundamental capacity limit in working memory.\na0by | However, there has not been a consensus on why and how working memory capacity is limited.\nu1ky | Among many theories, the executive attention hypothesis suggests that working memory\nhtm9 | requires using attention to maintain or suppress information, and the constraint on working memory\n8hul | capacity is not really about memory storage per se, but about the capacity for controlled, sustained\n87vz | Supporting evidence of the executive attention hypothesis includes results from the n-back task,\nym66 | which is arguably the current gold standard measure of working memory capacity in the cognitive\nl0a3 | neuroscience literature. The n-back task, initially developed by Kirchner,\nu8nw | requires participants to monitor a continuous stream of stimuli, and to decide for each stimulus\nl2v9 | whether it matches the one n steps back in the stream. The participants in this task must, therefore, continuously update their mental\nafbe | representation of the target items while also dropping now irrelevant items from consideration. So,\n320g | some executive attention processes are required in addition to storage. Typical human performance\ntp8g | (measured by accuracy) as a function of n is shown in Figure two, where we used the data presented in\n40 | In humans, working memory capacity has proved to be closely\ns2v1 | related with fluid intelligence or general intelligence, placing working memory at the core of human intelligence.\njkp9 | However, in artificial intelligence, there has not been a consensus as to which metrics should be accepted as an intelligence index when evaluating and comparing cognitive abilities of LLMs. In the current\n9g22 | study, we define working memory of LLMs as an emergent ability\n41fx | to selectively maintain and manipulate information for ongoing\n7um8 | cognitive processes, echoing the executive attention hypothesis in\nuow2 | cognitive sciences. We propose that the performance of LLMs\niat7 | on n-back tasks can be a reliable metric for assessing their working\nywwr | memory capacity, which in turn might reflect the general intelligence\n0i6j | To demonstrate this, we used ChatGPT as a representative\n87cq | of LLMs, and designed two categories of the n-back task to eval-\n0qs3 | uate its working memory capacity. Our results revealed strikingly\nz8f7 | consistent patterns of a capacity limit across multiple experimental\n7w7j | conditions, hinting at possibly similar mechanisms of working mem-\n51vd | ory in humans and LLMs. We believe this finding is important for both cognitive scientists and LLM\nbaj1 | researchers, and hope that this could guide future endeavors of better understanding why human\nn1wo | working memory capacity is limited and building more intelligent LLMs with better working memory capacity.\n5bq3 | Two Related Works\nezla | Working memory has long been a subject of study in human and animal cognition. Unlike long-term memory, which is stored in long-term synaptic weights in the neural system, working\nrsbu | memory is believed to be maintained by sustained activations of neurons in prefrontal cortex.\nmagd | This working mechanism bears striking resemblance to the in-context learning ability found in LLMs.\ntb74 | However, the investigation of working memory in LLMs remains largely unexplored. Therefore,\n8l69 | exploring the working memory capacity of LLMs holds great interest and significance, as it can contribute to the development of more powerful models.\n1jyv | Large language models have played a crucial role in achieving impressive performance across a wide\nolce | range of downstream tasks. While fine-tuning has emerged as a popular approach for transferring to\nerx1 | new tasks, it can be impractical to apply this method to extremely large models and/or\nb3bt | Note that both verbal and spatial tasks are compatible with these variants; we illustrate using verbal tasks without loss of generality.\n0w8m | showcasing the remarkable few-shot learning capabilities of large language models without requiring\nvcu3 | weight updates through gradient descent. Since its introduction, research on in-context learning in\nhjzj | language models has garnered significant attention from both academia and industry. Previous studies\nj79f | have presented various approaches to leverage the in-context learning ability of language models,\n6tbv | including selecting labeled examples for demonstrations, meta-training with an explicit\nxb13 | in-context learning objective, and exploring the variant of in-context learning that involves\ncxqi | learning to follow instructions.\nqfmx | However, relatively less work has been done to calibrate the working memory capacity of LLMs and understand the limitation of in-context learning ability. To the best of our knowledge, this paper is\nrut3 | the first that provides an empirical analysis from the neuroscience view that investigates the working memory ability of LLMs.\nm3cp | Three Methods\nmq4s | We devised two categories of n-back tasks involving verbal and spatial working memory respectively, and prompted ChatGPT to complete the\na0a5 | tasks in a trial-by-trial manner. For both categories, we have a base version task, and several variants derived from the base version to further test the model's performance under different conditions.\ngocl | Three point one Verbal n n-back experiments\nob1z | In the base version of the verbal n-back task, for n equals one, two, three, respectively, we generated fifty blocks of letter sequences using an alphabet commonly found in the literature (\"bcdfghjklnpqrstvwxyz').\nm4cb | Each block contained a sequence of twenty-four letters, which are presented one at a time as user input to the\nm61y | API. We included eight match trials and sixteen nonmatch trials in each block. The LLM was instructed to\n```",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725394525,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 1614,
    "prompt_tokens": 3693,
    "total_tokens": 5307
  }
}