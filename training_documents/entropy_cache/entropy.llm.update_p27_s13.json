{
  "id": "chatcmpl-A3UG0N5Hm7CVDM6iR2xunyEDGXt1p",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "zn89 | The principle of maximum entropy is widely used outside physics, though still controversial. But I think we should use it to figure out some basic properties of a gas-like its energy or entropy per molecule, as a function of pressure and temperature.\nb264 | To do this, we should generalize Shannon entropy to 'Gibbs entropy', replacing the sum by an integral. Or else we should 'discretize' the gas, assuming each molecule has a finite set of states. It sort of depends on whether you prefer calculus or programming. Either approach is okay if we study our gas using classical statistical mechanics.\njv66 | Quantum statistical mechanics gives a more accurate answer. It uses a more general\n5507 | definition of entropy-but the principle of maximum entropy still applies!\n74me | I won't dive into any calculations just yet. Before doing a gas, we should do some simpler examples-like the die whose average roll is five. But I can't resisting mentioning one philosophical point. In the box above I was hinting that maximum entropy works when your 'prior' is uniform:\nexc2 | Suppose there are N possible outcomes. At first you have no reason to think any is more probable than any other.\n8y62 | This is an important assumption: when it's not true, the principle of maximum entropy as we've stated it does not apply. But what if our set of events is something like a line? There's no obvious best probability measure on the line! And even good old Lebesgue measure D X depends on our choice of coordinates. To handle this, we need a generalization of the principle of maximum entropy, called the principle of maximum relative entropy.\n4fig | In short, a deeper treatment of the principle of maximum entropy pays more attention to our choice of 'prior': what we believe before we learn new facts. And it brings in the concept of 'relative entropy': entropy relative to that prior. But we won't get into this here, because we will always be using a very bland prior, like assuming that each of finitely many outcomes is equally likely.\nsksb | ADMITTING YOUR IGNORANCE\nop72 | Suppose you describe your knowledge of a system with N states using a probability distribution P one, P two, dot, dot, dot, P N. Then the Shannon information\nim7i | H equals negative summation P I log P I measures your ignorance of the system's state.\nrmuk | So, choosing the maximum-entropy probability distribution consistent with the facts you know amounts to not pretending to know more than you do.\nkadj | Remember: if we describe our knowledge using a probability distribution, its Shannon entropy says how much we expect to learn when we find out what's really going on. We can roughly say it measures our 'ignorance'-though ordinary language can be misleading here.\n5kvy | At first you think this ordinary six-sided die is fair. But then you learn that no, the average of the numbers that come up is five. What are the probabilities P one, dot, dot, dot, P N for the different faces to come up?\n8wop | This is tricky: you can imagine different answers!\nvcpg | You could guess the die lands with five up every time. In other words, P five equals one. This indeed gives the correct average. But the entropy of this probability distribution is zero. So you're claiming to have no ignorance at all of what happens when you roll the die!\nij2y | Next you might guess that it lands with four up half the time and six up half the time. In other words, P four equals P six equals point five. This probability distribution has one bit of entropy. Now you are admitting more ignorance. But how can you be so sure that five never comes up?\nw7j4 | Next you might guess that P four equals P six equals point four and P five equals point two. We can compute the entropy of this probability distribution. It's higher: one point five bits. Good, you're being more honest now! But how can you be sure that one, two, or three never come up? You are still pretending to know stuff!\nq8xw | Keep improving your guess, finding probability distributions with mean five with bigger and bigger entropy. The bigger the entropy gets, the more you're admitting your ignorance! If you do it right, your guess will converge to the unique maximum-entropy solution.\n1m55 | But there's a more systematic way to solve this problem.\n2zaa | THE BOLTZMANN DISTRIBUTION\n4iwf | Suppose you want to maximize the Shannon entropy\ngrmv | negative summation P I log P I\nmrk0 | of a probability distribution P one, dot, dot, dot, P N, subject to the constraint that the expected value of some quantity A I is some number C:\nbash | summation P I A I equals C (*) for I equals one to N\nxseo | Then try the Boltzmann distribution:\n3oxf | P I proportional to exp(negative beta A I)\ngvhw | where beta is some parameter.\n4ta1 | If you can find beta that makes (*) hold, this is the answer you seek!\njokf | How do you actually use the principle of maximum entropy?\nx0bn | If you know the expected value of some quantity and want to maximize entropy given this, there's a great formula for the probability distribution that usually does the job! It's called the 'Boltzmann distribution'. In physics it also goes by the names 'Gibbs distribution' or 'canonical ensemble', and in statistics it's called an 'exponential family'.\n049a | In the Boltzmann distribution, the probability P I is proportional to exp(negative beta A I) where A is the quantity whose expected value you know. Since probabilities must sum to one, we must have\nebc8 | summation exp(negative beta A I) for I equals one to N in the denominator.\nidjw | P I equals this expression scaled so that the total probability is one.\n6561 | It is then easy to find the expected value of A as a function of the number beta: just plug these probabilities into the formula\n2hrf | expected value of A equals summation A I P I for I equals one to N.\nmpyx | The hard part is inverting this process and finding beta if you know what you want the expected value of A to be.\nqlzy | When and why does the Boltzmann distribution actually work? That's a bit of a long story, so I'll explain it later. First, let's use the Boltzmann distribution to solve the puzzle I mentioned last time:\nsa6v | At first you think this ordinary six-sided die is fair. But then you learn that no, the average of the numbers that come up is five. What are the probabilities P one, dot, dot, dot, P N for the different faces to come up? You can use the Boltzmann distribution to solve this puzzle!\ngneb | To do it, take one less than or equal to I less than or equal to six and A I equals I. Stick the Boltzmann distribution P I into the formula summation A I P I equals five and get a polynomial equation for exp(negative beta). You can solve this with a computer and get exp(negative beta) approximately one point eight seven seven.\n1oqy | So, the probability of rolling the die and getting the number one through six is proportional to exp(negative beta I) approximately one point eight seven seven to the power I. You can figure out the constant of proportionality by demanding that the probabilities sum to one-or just look at the formula for the Boltzmann distribution. You should get these probabilities:\nopno | You can compute the entropy of this probability distribution, and you get roughly one point nine seven bits. You'll remember that last time we got entropies up to one point five bits just by making some rather silly guesses.\n0ahx | So, using the Boltzmann distribution, you can find the maximum-entropy die that rolls five on average. Later, we'll see how the same math lets us find the maximum-entropy state of a box of gas that has some expected value of energy!\nx5cc | MAXIMIZATION SUBJECT TO A CONSTRAINT\nc7cg | To maximize a smooth function F of several variables subject to a constraint on some smooth function G, look for a point where\nfiyl | the gradient of F is equal to lambda times the gradient of G for some number lambda.",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725394656,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 1759,
    "prompt_tokens": 3386,
    "total_tokens": 5145
  }
}