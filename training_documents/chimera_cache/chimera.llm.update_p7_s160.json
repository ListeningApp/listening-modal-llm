{
  "id": "chatcmpl-A3UD8mGQ80SVMQkQT2wS37RYhoj0R",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "0i1k | each worker, which has two basic scheduling units (i.e., K equals two). We propose three methods to concatenate multiple basic units. One. Direct concatenation, as shown in Figure seven (b). The bubbles at the end of the first basic unit can be occupied by the forward passes at the beginning of the second basic unit. If the backward pass has the same workload as the forward pass, basic units can be concatenated seamlessly. However, backward pass has about two times workload of the forward pass, which results in intermediate bubbles.\ndqlg | To remove the intermediate bubbles of direct concatenation, we propose two. forward doubling (shown in Figure seven (d)) and three. backward halving, in which the key idea is to equalize the workloads of forward and backward passes. In forward doubling, we increase the number of micro-batches for each forward pass to two, and then\np6pq | backward passes, each of which has only one micro-batch, as shown in Figure seven (c). Then, we fine-tune the schedule to remove fifty percent bubbles at the beginning\nzse6 | of the pipeline, as shown in Figure seven (d). Forward doubling removes the intermediate bubbles, but it leads to two times activation memory consumption and therefore may exceed the device memory capacity. We resort to activation recomputation to reduce memory overhead. Note that recomputation increases the workload of the backward pass, but the p two p communication overhead in the forward passes is also doubled because of the outputs for two micro-batches; therefore, we still treat the forward pass (with two micro-batches) and the backward pass (with one micro-batch and recompute) have approximately equal workload. Forward doubling prefers large models in which even B equals one exceeds the device memory capacity, since in such case activation recomputation must be used. For smaller models which has a larger B, we propose to use backward halving, which uses the same schedule as forward doubling, except that rather than executing two micro-batches in the forward pass but to halve the micro-batch size of the backward pass. Backward halving does not increase the activation memory (thus no activation recomputation), but it may lower the computational using a sub-max B. To select the best of the begin array text d efficiency because of text inree methods is not a end array priori, which we rely on empirical results. Note that both forward doubling and backward halving have total D minus two bubbles (D divided by two minus one in the forward passes and D divided by two minus one in the backward passes), as shown in Figure seven (d), which is about a fifty percent reduction compared with DAPPLE and GPipe. For K greater than two, we use the schedule of two D micro-batches as a basic scheduling unit (as shown in Figure seven (c)) for forward doubling\np2n3 | and backward halving, and concatenate floor k divided by two basic units and the residual D micro-batches if K is odd.\nohh7 | One more benefit for both forward doubling and backward halving is that they have more space to overlap p two p communication (in the forward passes) than the classic one F one B schedule. In Figure seven (d), taking the forward pass on micro-batch one as an example, the p two p communication from P one to P two can be overlapped by the intermediate forward pass computation, while for one F one B there may be not enough computation to overlap p two p communication.\nvbni | Three point six Generalize to More than Two Pipelines\ndgcw | So far we have only discussed the case that two pipelines (one down and one up) are combined together in Chimera. Yet, Chimera can be generalized to combine more than two pipelines for an even number of pipeline stages (i.e., D is even). For Q equals D divided by two, let F denote the set of all the divisors of Q, including one and Q itself. For any f in F, we can generate a scheme for Chimera, which combines f down pipelines and f up pipelines together and each pipeline has D divided by two f micro-batches scheduled by the one F one B strategy. Figure eight gives an example in which Chimera combines four pipelines with eight stages (i.e., D equals eight and f equals two). For the down pipeline i (i in [zero, f minus one]), the D stages are mapped to the D workers in turn with the first stage (i.e., stage zero) being mapped to the worker i times (D divided by f). For example, for the down pipeline one in Figure eight, stages [zero, one, two, three, four, five, six, seven] are mapped to workers [four, five, six, seven, zero, one, two, three], respectively. For the f up pipelines, the D stages are mapped to the D workers in a completely reverse order of the corresponding down pipeline. It can be easily demonstrated that the schedules of the two f pipelines can be overlaid without conflict.\ngvwz | (D minus D divided by two f plus one) M a, D times M a. For any f in F, Chimera can scale to more micro-batches (i.e., N greater than D) using the methods discussed in Section three point five. For a given f, Chimera incurs two (D divided by f divided by two minus one) bubbles, but has to maintain two f model replicas and synchronize the gradients of two f stages on each worker.",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725394478,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 1139,
    "prompt_tokens": 3305,
    "total_tokens": 4444
  }
}