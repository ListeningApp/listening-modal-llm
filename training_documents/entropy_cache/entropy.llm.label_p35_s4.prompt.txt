You will receive raw text from an OCR scan of a document or web page. Each line of input represents one textbox from the document.

Your task is to label each textbox with exactly one of the following labels:
  title: the primary document title, no more than one per document
  heading: the name of the following chapter or section (should never start in the middle of the previous paragraph)
  subheading: a secondary heading, often following another heading (should never start in the middle of the previous paragraph)
  body: main paragraph content: may start or end in the middle of a word, use context to follow paragraphs that are split across textboxes (if a textbox contains both heading and body text, label it "body")
  math: textbox primarily containing math
  imageDescription: standalone callout that only describes an image, chart, table, or similar (typically begins with "Table...", "Figure...", "Fig. ...", "Above...", "Left...", etc.)
  authors: names of contributors to this document
  institutions: list of universities, business, and other institutions affiliated with this document
  publisher: info about the publisher, or provided by the publisher, including copyright info
  pagenum: the page number
  headerOrFooter: boilerplate text page number, or redundant headings at the top/bottom of the page
  toc: the table of contents
  references: bibliographic references and resources
  acknowledgements: thanks and credits to contributors
  appendix: index, appendix, or any other end matter not part of the main thread (including related headings)
  table: tabular data, label, title, or subheading for a grid or list of data
  datum: numeric data or data label from a chart or list
  advertisement: promotional ads and marketing
  layout: non-content related to the layout of the page, e.g. sidebar navigation
  callout: pull quote or long block text that stands apart from the main text
  footnote: footnotes and endnotes near the bottom of the page
  keywords: list of keywords or tags
  url: web address or email
  other: any other text that does not belong to the main thread

Each line of input is prefixed with a textbox ID in the format `id | Text content`. The output must be in the format `id | label`.

Note that body text may be split across multiple textboxes, and textboxes may start or end in the middle of a sentence or word. Because the text was extracted from a web page or document, paragraphs of body text may be interrupted by multiple textboxes of redundant headers, footnotes, page numbers, tables, images, etc. For example, a real heading will never interrupt the middle of a sentence. Use context clues like this to carefully label each textbox.

EXAMPLE INPUT:
```
1pjs | Neuroscience Applied 1 (2022) 101012
o2kr | Contents lists available at ScienceDirect
v6sk | Neuroscience Applied
1wj6 | journal homepage: www.journals.elsevier.com/neuroscience-applied
936l | Research Articles
06yq | Microdosing with psychedelics to self-medicate for ADHD symptoms in adults: A naturalistic study
dpv6 | ARTICLE INFO
8r5j | ABSTRACT
2taz | Keywords: ADHD Microdosing Psychedelics Self-medication Well-being Time perception
o3ya | ADHD in adulthood is often overlooked, which negatively affects the individual's well-being. First-line phar- macological interventions are effective in many ADHD patients, relieving symptoms rapidly. However, there seems to be a proportion of individuals who discontinue, or fail to respond to these treatments. For these in- dividuals, alternative treatment options should be explored.
r5il | 1. Introduction
5gui | Attention deficit hyperactivity disorder (ADHD) is one of the most common developmental disorders worldwide. Prevalence research indicates that 2.6% of the adult population has persistent ADHD. ADHD in adults is often overlooked because of the high comorbidity rate and lack of knowledge about how ADHD is expressed in adulthood (Kooij et al.,
9d4o | Fig. 1. Flowchart of included participants for each time point.
82qc | 2019). In addition, ADHD is associated with deficits in various domains of cogni- tive functioning. Twenty-five percent of ADHD cases suffered from ADHD symptoms purely because of de- ficiencies in temporal processing (Sonuga-Barke et al., 2010).
ls7d | First-line ADHD treatments in adults mainly include pharmacological interventions to enhance dopaminergic and noradrenergic neurotrans- mission with stimulants. Overall, they have been proven to work effectively in adults with ADHD, inducing fast symptom relief and thereby enhancing the person's quality of life. In the
5esx | Table 1 Demographic information from respondents at baseline and the two and four- week time points.
t8vv | longer term, approximately twenty percent of ADHD patients discontinue their prescribed medication after six to nine months, thirty percent after one year, and half of them after two years.
xrd1 | 2. Material and methods
5746 | 2.1. Study design and participants
7toz | The study employed a naturalistic design, assessing the experiences of participants at baseline,
j346 | Neuroscience Applied 1 (2022)
m5ka | E.C.H.M. Haijen et al.
kx2c | REVIEW
v8uq | before they start MD on their own initiative, and at two and four weeks after MD initiation. The target population included adults diagnosed with ADHD and individuals who experienced ADHD symptoms to the extent that these interfered with their daily lives and who had not been diag- nosed with ADHD before. To be included in the analyses, participants needed to score above a cut-off point on at least one of the subscales of the Conner's Adult ADHD Rating Scale (CAARS-S:SV).
2zmw | Fig. 2. Mean scores of the CAARS-S:SV DSM-IV total symptoms T-scores at baseline (0W) and two (2W) and four weeks (4W) after MD (A) of the whole sample, and (B) per conventional ADHD medication use. Error bars represent mean error.
sdk3 | <LATEX>\lim _ { x ightarrow \infty } rac { 6 x ^ { 2 } + 1 0 0 } { 7 x ^ { 2 } - 1 0 } =</LATEX>
x972 | 2.2. Study procedure
k221 | Mean performance measures of the CAARS-
```

EXAMPLE OUTPUT:
```
1pjs | headerOrFooter
o2kr | publisher
v6sk | publisher
1wj6 | publisher
936l | headerOrFooter
06yq | title
dpv6 | other
8r5j | heading
2taz | keywords
o3ya | body
r5il | heading
5gui | body
9d4o | imageDescription
82qc | body
ls7d | body
5esx | imageDescription
t8vv | body
xrd1 | heading
5746 | subheading
7toz | body
j346 | headerOrFooter
m5ka | authors
kx2c | headerOrFooter
v8uq | body
2zmw | imageDescription
sdk3 | math
x972 | heading
k221 | body
```

INPUT:
```
sy14 | In summary, we've seen that if there exists a probability distribution <LATEX>p</LATEX> that max- imizes the Shannon entropy among probability distributions with <LATEX>\langle A \rangle = c ,</LATEX> and <LATEX>i f</LATEX> all the <LATEX>p _ { i }</LATEX> are positive, then <LATEX>p</LATEX> must be a Boltzmann distribution. But this raises other questions. When does such a probability distribution exist? If it exists, is it unique? And what if not all the <LATEX>p _ { i }</LATEX> are positive?
ghss | In what follows we'll dive down this rabbit hole and get to the bottom of it. I'll just state some facts-you may enjoy trying to see if you can prove them. First, there exists a probability distribution <LATEX>p _ { 1 } , \ldots , p _ { n }</LATEX> with <LATEX>\langle A \rangle = c</LATEX> if and only if
3s61 | <LATEX>A _ { \min } \leq c \leq A _ { \max }</LATEX> where <LATEX>A _ { \min }</LATEX> is the minimum value and <LATEX>A _ { \max }</LATEX> is the maximum value of the numbers <LATEX>A _ { 1 } , \ldots , A _ { n } .</LATEX> Second, whenever
s3rm | <LATEX>A _ { m i n } < c < A _ { \max } ,</LATEX> there exists a unique probability distribution <LATEX>p _ { 1 } , \ldots , p _ { n }</LATEX> maximizing Shannon entropy subject to the constraint <LATEX>\langle A \rangle = c .</LATEX> Third, this unique maximizer <LATEX>p</LATEX> has <LATEX>p _ { i } > 0</LATEX> for <LATEX>\mathrm { a l l } i</LATEX> i, and is thus a Boltzmann distribution, whenever
pqxb | <LATEX>A _ { \min } < c < A _ { \max } .</LATEX> When <LATEX>c = A _ { \min } ,</LATEX> the unique maximizer assigns the same probability <LATEX>p _ { i }</LATEX> to each outcome <LATEX>i</LATEX> with <LATEX>A _ { i } = A _ { \min } ,</LATEX> while <LATEX>p _ { i } = 0</LATEX> for all other outcomes. Similarly, when <LATEX>c = A _ { \max } ,</LATEX> the unique maximizer assigns the same probability <LATEX>p _ { i }</LATEX> to each outcome <LATEX>i</LATEX> with <LATEX>A _ { i } = A _ { \max } ,</LATEX> while <LATEX>p _ { i } = 0</LATEX> for all other outcomes.
08qc | It's good to look at an example:
hdj2 | Puzzle 24. Suppose there are only two outcomes, with <LATEX>A _ { 1 } = - 1</LATEX> and <LATEX>A _ { 2 } = 1 .</LATEX> Work out the Boltzmann distribution <LATEX>p</LATEX> maximizing Shannon entropy subject to the constraint <LATEX>\langle A \rangle = c</LATEX> for <LATEX>- 1 < < <</LATEX> 1. Show that as <LATEX>\lambda \rightarrow + \infty</LATEX> this Boltzmann distribution has
a57g | <LATEX>p _ { 1 } \rightarrow 1 , p _ { 2 } \rightarrow 0</LATEX> while as <LATEX>\lambda \rightarrow - \infty</LATEX> it has
g8m3 | <LATEX>p _ { 1 } \rightarrow 0 , p _ { 2 } \rightarrow 1 .</LATEX> Show the probability distribution <LATEX>p _ { 1 } = 1 ,</LATEX> <LATEX>p _ { 2 } = 0</LATEX> maximizes Shannon entropy subject to the constraint <LATEX>\langle A \rangle = - 1 ,</LATEX> while <LATEX>p _ { 1 } = 0 ,</LATEX> <LATEX>p _ { 2 } = 1</LATEX> maximizes it subject to the constraint <LATEX>\langle A \rangle = 1 .</LATEX> Show these two probability distributions are not Boltzmann distributions.
a9av | 28
73l3 | THERMAL EQUILIBRIUM
ornm | Suppose a system has finitely many states i = 1, . .. , n with energies Ej. If the probability p; that it's in the ith state maximizes entropy subject to a constraint on its expected energy:
u2z1 | (E) = p; E; i=1 n
atmh | we say it is in thermal equilibrium. In this case pi is given by a Boltzmann distribution
7hgg | pi =
xwsv | >
756b | exp(-E;)
6bx7 | at least if all the probabilities p; are positive.
spl6 | Don't worry: the substance of what I'm saying here is almost the same as in the last box. I'm merely attaching new words to the concepts, to make them sound more like physics:
7wzm | · Before I said we had a set of n 'outcomes' numbered 1, 2, ... , n. Now I'm talking about 'states'. If we have a system with n states, it means there are n outcomes when we do a measurement to completely determine which state it's in. A 'state' is some way for a physical system to be that's vague but it's all we can say until we consider some specific kind of system. In classical physics the states form a set, usually infinite but sometimes finite.
e9xj | · Before I said we had a 'quantity' A that depends on the outcome, taking the value Ai in the ith outcome. Now I'm calling this quantity the 'energy' E. Energy is a particularly interesting quantity in physics, so we'll focus on that, without demanding that you know anything about it: for our present purposes, we can take any quantity and dub it 'energy'.
9vl6 | · Before I called the Lagrange multiplier . Now I'm calling it ß, because that's what physicists do in this particular context.
c2fd | When a system maximizes entropy subject to a constraint on the expected value of its energy, and perhaps also some other quantities, we say the system is in thermal equilibrium. This is meant to suggest that an object just sitting there, not heating up or cooling down, is often best modeled this way.
jzvp | You may have noticed the annoying clause "at least if all the probabilities pi are positive". I only said that because I cannot tell a lie. In Puzzle 24 we saw that as 3 -> too, the Boltzmann distribution can converge to a non-Boltzmann probability distribution where some of the probabilities pi vanish. This still counts as thermal equilibrium, because it's still maximizing entropy subject to a constraint on expected energy. We'll learn more about this when we study the concept of 'absolute zero'.
f71y | 29
31e1 | COOLNESS
40ry | If a probability distribution p; maximizes entropy sub- ject to a constraint on the expected value of the energy Ej, then
d1j2 | Pi xe BEi
wstc | where ß is the coolness, inversely proportional to tem- perature. So:
ba1e | The cooler a system is, the less likely it is to be in a high-energy state!
en95 | Say a system with finitely many states maximizes entropy subject to a constraint on the expected value of some quantity E that we choose to call 'energy'. Then its probability of being in the ith state is proportional to exp(-BE;) for some number .
l3do | When ß is big and positive, the probability of being in a state of high energy is tiny, since exp(-ßE;) gets very small for large energies Ej. This means our system is cold.
u9bo | Conversely when ß is small and positive, exp(-ßE;) drops off very slowly as the energy E; gets bigger. So, high-energy states become quite likely when ß is small and positive. This means our system is hot.
7avo | It turns out 3 is inversely proportional to the temperature more about that later. But in modern physics ß is just as important as temperature. It comes straight from the principle of maximum entropy!
uzo6 | So ß deserves a name. And its name is 'coolness'.
89b4 | By the way, the formula
5f7e | Pi xe BEi
h83u | is only strictly true when ß is finite. There's also a limiting case ß -> +0, when pi = 0 except for states of the very lowest energy. And there's a limiting case ß -> -0, where Pi = 0 where except for states of the very highest energy. I'll say a bit about these oddities later. First I'll say more about what coolness has to do with temperature.
kjoa | 30
wb1c | COOLNESS VERSUS TEMPERATURE
ak13 | Coolness 3 is inversely proportional to temperature T:
s7el | where k is Boltzmann's constant.
0lfu | Coolness is measured in joules-1, temperature is measured in kelvin, and Boltzmann's constant is a conversion factor:
cnjl | k =1.380649.10-23 joules kelvin
rtdl | In statistical mechanics, coolness is inversely proportional to temperature. But coolness has units of energy-1, not temperature-1. So we need a constant to convert between coolness and inverse temperature! And this constant is very interesting.
norp | Remember: when a system maximizes entropy with a constraint on its expected energy, the probability of it having energy E is proportional to exp(-ßE) where ß is its coolness. But we can only exponentiate dimensionless quantities! (Why?) So ß has dimensions of 1/energy.
chx3 | Since coolness is inversely proportional to temperature, we must have ß = 1/kT where k is some constant with dimensions of energy/temperature. This constant k is called 'Boltzmann's constant'. It's tiny:
sfcf | k = 1.380649 . 10-23 joules/kelvin.
nuxs | This is mainly because we use units of energy, joules, suited to macroscopic objects like a cup of hot water. Boltzmann's constant being tiny reveals that such things have enormously many microscopic states!
umx0 | Later we'll see that a single classical point particle, in empty space, has energy 3kT/2 when it's maximizing entropy at temperature T. The 3 here is because the atom can move in 3 directions, the 1/2 because we integrate x2 to get this result. The important part is kT. The kT says: if an ideal gas is made of atoms, each atom contributes just a tiny bit of energy per kelvin, or degree Celsius: roughly 10-23 joules. So a little bit of gas, like a gram of hydrogen, must have roughly 1023 atoms in it. This is a very rough estimate, but it's a big deal.
o306 | Indeed, the number of atoms in a gram of hydrogen is about 6 . 1023. You may have heard of Avogadro's number-this is quite close to that. So Boltzmann's constant gives a hint that matter is made of atoms-and even better, a nice rough estimate of how many per gram!
qm1b | Later we will see that Boltzmann's constant has another important meaning: it's a fundamental unit of entropy, a nat, expressed in joules/kelvin.
n9ms | 31
icdp | TEMPERATURE
wtse | If a system has finitely many states with energies Ei, in thermal equilibrium at temperature T the probability that it's in the ith state is Pi xx exp(-Ei/kT)
bm7g | where k is Boltzmann's constant and T can be positive, negative, or even infinite:
jr68 | T = 00
axix | T < 0
05y4 | T> 0
2fb0 | A system with finitely many states can be pretty weird. It can have negative tem- perature! Even weirder: as you heat it up, its temperature becomes large and positive, then it reaches infinity, and then it 'wraps around' and become large and negative.
s9fn | The reason: coolness is more fundamental than temperature. The coolness ß is inversely proportional to the temperature T:
k5r6 | 3 = 1/kT.
z9v9 | When the temperature goes up to infinity and then suddenly becomes a large nega- tive number, it's really just the coolness going down to zero and becoming negative. Temperatures 'wrap around' infinity, as shown in the picture.
gpu9 | A system with finitely many states can have negative or infinite temperature because in thermal equilibrium, its probability of being in the ith state is
u5ws | exp(-ßE;) n Lexp(-BEi) i=1
onkh | Pi
8t55 | where E; is the energy of the ith state, and this makes sense for any ß E R. Moreover, the probability pi depends continously on , even as ß passes through zero. This means a large positive temperature is almost like a large negative temperature!
7imw | But the circle of temperature can be misleading. Temperatures wrap around T = 00 but not T = 0. A system with a small positive temperature is very different from one with a small negative temperature! That's because pi for B >> 0 is very different than it is for ß < 0.
lf1t | 32
tze7 | For a system with finitely many states we can take the limit of the Boltzmann distribution when ß -> +0; then the system will only occupy its lowest-energy state or states. We can also take the limit when 3 -> -co; then the system will only occupy its highest-energy state or states. In terms of temperature, this means that the limit where T approaches zero from above is very different than the limit where T approaches zero from below.
ryvv | So, for a system with finitely many states, the best picture of possible thermal equilibria is not a circle of temperatures but a closed interval of coolness: the coolness ß can be anything in [-0, +x], which topologically is a closed interval. In terms of coolness, too is different from -0, but approaching 0 from above is the same as approaching it from below. But in terms of temperature, approaching 0 from above is different from approaching 0 from below, while a temperature of too is the same as a temperature of -0.
zdrh | Now, if all this seems very weird, here's why: we often describe physical systems using infinitely many states, with a lowest possible energy but no highest possible energy. In this case the sum in the Boltzmann distribution can't converge for ß < 0, so negative temperatures are ruled out.
osmo | However, some physical systems can be approximately described using a finite set of states (or in quantum theory, a finite-dimensional Hilbert space of states). Then the things I just said hold true! And people enjoy studying these systems, and their strange properties, in the lab.
0b8v | It's good to look at a simple example, and work everything out explicitly:
fucs | Puzzle 25. Suppose a system has two states with energies E1 / E2. Compute the probabilities pi that it is in either of these states in thermal equilibrium as a function of the coolness B. Then express these probabilities as a function of the temperature T. Using these functions pi(T):
yply | . Show that when 0 < T < +oo the system is more likely to be in the lower-energy state: P1(T) > p2(T).
75n5 | . Show that when -> < T < 0 the system is more likely to be in the higher-energy state: P1(T) < p2(T).
ec91 | · Show that
```

OUTPUT:
```
