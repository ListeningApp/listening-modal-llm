{
  "id": "chatcmpl-A3UD9r1unDms1sKSXcat2oveKncE2",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "sk5m | GEMS is a memory-efficient pipeline approach which schedules micro-batches among two model replicas. Since GEMS is mainly designed for small B and has at most two active micro-batches at\nqws8 | the same time, its bubble ratio is much higher than the other approaches and cannot be alleviated by a larger N. Asynchronous approaches do not have periodic pipeline flushes, so they do not have bubble problem but with stale weights.\n877u | Memory consumption. Memory overhead mainly comes from two aspects: the weight parameters and the activations (the intermediate results of the forward pass required in the backward pass to compute gradients). For GPipe and DAPPLE, each worker maintains the weights of one pipeline stage. For GEMS and Chimera (with the default setting), each worker maintains the weights of two pipeline stages since there are two pipelines in two directions. PipeDream updates the model after each backward pass on a micro-batch N equals one; therefore, to ensure weight version consistency between forward and backward passes, it requires to stash up to D versions of weights on a worker, which has the same memory cost as pure data parallelism. By using gradient accumulation N is greater than or equal to D, PipeDream-2BW reduces the number of weight versions to be stashed to two.\nrzhj | GEMS injects only one micro-batch at the beginning of the pipeline, and thus the activations of the forward pass on one micro-batch are stored. However, this leads to low pipeline efficiency as discussed previously. Since GPipe injects N micro-batches N is greater than or equal to D to fully utilize the pipeline into the pipeline concurrently, the activations memory consumption is proportional to N, which does not scale well to large mini-batches. Using the classic or a variant of one-F-one-B schedule, PipeDream, PipeDream-2BW, DAPPLE, and Chimera inject up to D micro-batches at the beginning of the pipeline, which scale well to large mini-batches. By counting the number of injected micro-batches on each worker of Chimera, we can observe that Chimera has an extra benefit of a more balanced activations memory consumption among the workers than PipeDream, PipeDream-2BW, and DAPPLE, and therefore a better memory resources utilization. Note that the activations memory consumption can be reduced using the technique of activation recomputation, but this is at a cost of about one-third more computation overhead.\nbfax | ZeRO removes the memory redundancies by partitioning the three model states i.e., optimizer states, gradients, and parameters across data-parallel processes, with a modest increasement to the communication volume. Note that our pipeline approach is orthogonal to ZeRO. To further reduce the memory consumption of our pipeline approach is an interesting future work.\nf2y8 | Convergence friendliness. By periodic pipeline flushes, synchronous approaches ensure that the same version of weights is used across all stages and all micro-batches in a training iteration, without introducing staleness. From the algorithmic perspective, synchronous approaches are equivalent to the standard and well-proved mini-batch SGD, and therefore, guarantee convergence.\no7qm | To remove pipeline flushes, asynchronous approaches either aggressively lead to weight versions mismatch between forward and backward passes, or conservatively introduce staleness to the weights while ensuring weight versions consistency. Although they empirically show promising convergence results, the generality is lack of proof. More recent work observes that asynchronous training algorithms may result in lower convergence performance.\nd52l | For the model accuracy, all the synchronous pipeline approaches such as Chimera, DAPPLE, GPipe and GEMS are guaranteed to achieve the same accuracy as the standard mini-batch SGD algorithm. For the asynchronous approaches such as PipeDream-2BW and PipeDream, it is not safe to achieve the ideal accuracy as the standard algorithm because of the introduced weight staleness, and the convergence quality may exhibit variance on different neural networks and tasks.\nqlbb | Overall, Chimera achieves the best balance of all aspects, as presented in Table Two. We will discuss the implementation details of Chimera in the following section.\nnhsg | THE SCHEME OF CHIMERA Three\nhy0l | Three point one Bidirectional Pipelines\n4jmz | We consider large-scale models with repetitive structures i.e., the same block repeated multiple times, such as Bert and GPT- Two/Three, which can be partitioned into balanced stages with an equal number of blocks. The feature of repetitive structures is also exploited in PipeDream-2BW. How to generally partition any model into stages with efficiency is not the topic of this paper and has been well studied in recent work.\nb7cq | The key idea of Chimera is to combine two pipelines in different directions we call them down and up pipelines, respectively together. Figure Three shows an example with four pipeline stages i.e., D equals four. Here we assume there are D micro-batches executed by each\n0b39 | worker within a training iteration, namely N equals D, which is the minimum to keep all the stages active. How to scale to more than D micro-batches i.e., for N is greater than D will be discussed in Section 3.5. In the down pipeline, stage zero through stage three are mapped to PO through P three linearly, while in the up pipeline the stages are mapped in a completely opposite order. The N assuming an even number micro-batches are equally partitioned among the two pipelines. Each pipeline schedules N divided by two micro-batches using one-F-one-B strategy, as shown in the left part of Figure Three. Then, by merging these two pipelines together, we obtain the pipeline schedule of Chimera upper right of Figure Three. Given an even number of stages D which can be easily satisfied in practice, it is guaranteed that there is no conflict i.e., there is at most one micro-batch occupies the same time slot on each worker during merging. We can see that the number of bubbles is reduced to D divided by two minus one in the forward and backward passes, respectively. By considering the uneven workloads between forward and backward passes, we get a more practical schedule of Chimera bottom right of Figure Three.\nqecd | For the models which have to use a small B hat to guarantee convergence, there maybe less than D micro-batches a training iteration i.e., N is less than D. Chimera also supports the cases of N is less than D by simply partitioning the N micro-batches among the two pipelines as evenly as possible, with an extreme case that N equals one where only one micro-batch runs on a single pipeline.",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725394479,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 1366,
    "prompt_tokens": 3333,
    "total_tokens": 4699
  }
}