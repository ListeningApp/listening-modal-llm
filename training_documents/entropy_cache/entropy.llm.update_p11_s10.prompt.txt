You will receive raw text from an OCR scan of a document or web page. Each line of input represents one textbox from the document. Your task is to faithfully reproduce the text for our text-to-speech engine so that it is easily read aloud.

Note that the text might not start at the beginning and paragraphs may be split acrosss multiple textboxes. Textboxes may start or end in the middle of a word which is acceptable: do not combine text between textboxes and leave broken words at the start and end of a textbox unchanged.

You will receive one textbox per-line in the format `id | Textbox content`. The output must be in the format `id | Clean content`.

RULES:
 • Remove inline citations and references e.g. "(Author, 2021)" or "(https://wikipedia.org/article.html)" or "[ECMOS 35b, 47, 49]".
 • Spell out numbers, dates, and chemical formulas for the TTS engine e.g. "-2.5" -> "negative two point five" or "Jan. 2020" -> "January twenty twenty".
 • Spell out LaTeX formulas for the TTS engine e.g. "<LATEX>p = 1</LATEX>" -> "P equals one".
 • Rejoin hyphenated word fragments within the same textbox e.g. "synth -esize" -> "synthesize".
 • Fix simple typos and OCR errors e.g. "O nce upon a 1ime." -> "Once upon a time.".
 • Delete nonsequiter textboxes that interrupt the middle of a sentence if they obviously do not belong.

Otherwise, the output should exactly match the input text:
 • Do not combine text across lines or attempt to fix words broken across lines.
 • Ensure each ID from the input is included in the output.
 • Make as few changes as possible while respecting the above rules.

EXAMPLE INPUT:
```
ccpt | stigate the effectiveness of psychedelics for ADHD symp- toms in adults.
0fpw | 2. Materia1 and methods
dqn8 | 2.1. Study design and participants
cop0 | The study employed a naturalistic design, assessing the experiences of participants at bas-,
lhkq | eline before they start, and at 2 and 4 weeks after initiation. The target population included adults with ADHD symptoms who had not been diag- nosed with ADHD before. To be included, participants needed to score on Conner's Adult ADHD Rating Scale (CAARS-S:SV). This cut-off was indicative of clinically elevated symptoms (Conners et al., 1999) (see section 2.3).
82ju | 2.2. Study procedure
y2qo | The online advertisement was placed on a website providing information about psychedelics (www.microdo sing.nl). Interested participants were redirected to information explaining the study rationale and procedure. The baseline survey took 20 min to complete. If the survey had not been completed after 24 h, a reminder was sent. Each of the surveys at the 2- and 4-week time points took about 15 min to complete. Par- ticipants were able to pause the surveys. Data collection occurred between Nov. 2020 and Jul. 2021. The study was approved by the Ethics Review Committee of Psy- chology and Neuroscience at Maastricht University (ERCPN- 215_05_11_2019_A1).
1piq | 2.3. Measures
7fqw | 2.3.1. Demographic information and history of substance use
rhaz | At baseline, demographic information was collected. History of substance use assessed experience with psyche- delics (i.e., ayahuasca, DMT, 5-MeO-DMT, LSD, novel lysergamides (e.g., 1P-LSD, ALD-52), psilocybin, salvia divinorum, ibogaine, and mescaline) in both full (psychedelic) doses and microdoses.
wak1 | 2.3.2. Psychiatric and physiological diagnoses
wtfz | Participants were asked whether they had a current diagnosis of a disorder. These answer options were chosen because most of the listed diagnoses are often reported to co-occur with ADHD (Kooij et al., 2019), or because these diagnoses were reported to be common in people who microdose (Fadiman and Korb, 2019). We 
3j2l | REVIEW ARTICLE Frontiers in Psychiatry | www.frontiersin.org
03k3 | 2
1k9e | constructed a variable Comorbidity alongside ADHD, differenti- ating respondents with and without a comorbid diagnosis alongside ADHD (<LATEX>p < . 0 0 1 =</LATEX> only ADHD or no ADHD diagnosis; <LATEX>p > . 0 1 =</LATEX> ADHD and at least one other diagnosis). We constructed a variable Medication use alongside microdosing, differentiating respondents who were and were not using conven- tiona1 ADHD medication alongside microdosing (<LATEX>0 =</LATEX> only microdosing; <LATEX>1 =</LATEX> conven-
```

EXAMPLE OUTPUT:
```
ccpt | stigate the effectiveness of psychedelics for ADHD symptoms in adults.
0fpw | Two. Material and methods
dqn8 | Two point one. Study design and participants
cop0 | The study employed a naturalistic design, assessing the experiences of participants at bas-,
lhkq | eline before they start, and at two and four weeks after initiation. The target population included adults with ADHD symptoms who had not been diagnosed with ADHD before. To be included, participants needed to score on Conner's Adult ADHD Rating Scale. This cut-off was indicative of clinically elevated symptoms (see section two point three).
82ju | Two point two. Study procedure
y2qo | The online advertisement was placed on a website providing information about psychedelics. Interested participants were redirected to information explaining the study rationale and procedure. The baseline survey took twenty minutes to complete. If the survey had not been completed after twenty-four hours, a reminder was sent. Each of the surveys at the two and four-week time points took about fifteen minutes to complete. Participants were able to pause the surveys. Data collection occurred between November twenty twenty and July twenty twenty-one. The study was approved by the Ethics Review Committee of Psychology and Neuroscience at Maastricht University.
1piq | Two point three. Measures
7fqw | Two point three point one. Demographic information and history of substance use
rhaz | At baseline, demographic information was collected. History of substance use assessed experience with psychedelics (i.e., ayahuasca, DMT, five-MeO-DMT, LSD, novel lysergamides (e.g., one-P-LSD, ALD-fifty-two), psilocybin, salvia divinorum, ibogaine, and mescaline) in both full (psychedelic) doses and microdoses.
wak1 | Two point three point two. Psychiatric and physiological diagnoses
wtfz | Participants were asked whether they had a current diagnosis of a disorder. These answer options were chosen because most of the listed diagnoses are often reported to co-occur with ADHD, or because these diagnoses were reported to be common in people who microdose. We
3j2l |
03k3 |
1k9e | constructed a variable Comorbidity alongside ADHD, differentiating respondents with and without a comorbid diagnosis alongside ADHD (P is less than point zero zero one equals only ADHD or no ADHD diagnosis; P is greater than point zero one equals ADHD and at least one other diagnosis). We constructed a variable Medication use alongside microdosing, differentiating respondents who were and were not using conventional ADHD medication alongside microdosing (zero equals only microdosing; one equals conven-
```

INPUT:
```
hrlm | Later von Neumann generalized Gibbs' formula for entropy from classical to quantum statistical mechanics! He replaced the probability distribution p by a so-called density matrix p, and the integral by a trace.
wfjt | Later Shannon invented information theory, and a formula for the entropy of a probability distribution on a set (often a finite set). This is often called 'Shannon entropy'. It's just a special case of Gibbs' formula for entropy in classical statistical mechanics, but without the Boltzmann's constant.
g779 | Later still, Kolmogorov invented a formula for the entropy of a specific string of symbols. It's just the length of the shortest program, written in bits, that prints out this string. It depends on the computer language, but not too much.
by5l | There's a network of results connecting all these 5 concepts of entropy. I will first explain Shannon entropy, then entropy in classical statistical mechanics, and then en- tropy in thermodynamics. While this is the reverse of the historical order, it's the easiest way to go.
wye3 | I will not explain entropy in quantum statistical mechanics: for that I would feel compelled to teach you quantum mechanics first. Nor will I explain algorithmic entropy.
oc7w | FROM PROBABILITY TO INFORMATION
k5k4 | How much information do you get when you learn an event of prob- ability <LATEX>p</LATEX> has happened? It's
wngq | <LATEX>- \log p</LATEX> where we can use any base for the logarithm, usually <LATEX>e</LATEX> or 2.
udft | Example: Suppose I flip 3 coins that you know are fair. I tell you the outcome: "heads, tails, heads". That's an event of probability <LATEX>1 / 2 ^ { 3 } ,</LATEX> so the information you get is
foay | <LATEX>- \log \left( \frac { 1 } { 2 ^ { 3 } } \right) = 3 \log 2</LATEX> or "3 bits" for short, since log 2 of information is called a bit.
43mx | Here is the simplest link between probability and information: when you learn that an event of probability <LATEX>p</LATEX> has happened, how much information do you get? We say it's <LATEX>- \log p .</LATEX> We take a logarithm so that when you multiply probabilities, information adds. The minus sign makes information come out positive.
ld7j | Beware: when I write 'log' I don't necessarily mean the logarithm base 10. I mean that you can use whatever base for the logarithm you want; this choice is like a choice of units. Whatever base <LATEX>b</LATEX> you decide to use, I'll call <LATEX>\log _ { b } 2</LATEX> a 'bit'. For example, if I flip a single coin that you know is fair, and you see that it comes up heads, you learn of an event that's of probability <LATEX>1 / 2 ,</LATEX> so the amount of information you learn is
mf5v | <LATEX>- \log _ { b } \frac { 1 } { 2 } = \log _ { b } 2 .</LATEX> That's one bit! Of course if you use base <LATEX>b = 2</LATEX> then this logarithm actually equals 1, which is nice.
f31w | To understand the concept of information it helps to do some puzzles.
yofz | Puzzle 1. First I flip 2 fair coins and tell you the outcome. Then I flip 3 more and tell you the outcome. How much information did you get?
4mdw | Puzzle 2. I roll a fair 6-sided die and tell you the outcome. Approximately how much information do you get, using logarithms base 2?
49f1 | Puzzle 3. When you flip 7 fair coins and tell me the outcome, how much information do I get?
csae | Puzzle 4. Every day I eat either a cheese sandwich, a salad, or some fried rice for lunch- each with equal probability. I tell you what I had for lunch today. Approximately how many bits of information do you get?
c29v | Puzzle 5. I have a trick coin that always lands heads up. You know this. I flip it 5 times and tell you the outcome. How much information do you receive?
gkgm | Puzzle 6. I have a trick coin that always lands heads up. You believe it's a fair coin. I flip it 5 times and tell you the outcome. How much information do you receive?
swmm | Puzzle 7. I have a trick coin that always lands with the same face up. You know this, but you don't know which face always comes up. I flip it 5 times and tell you the outcome. How much information do you receive?
dak5 | These puzzles raise some questions about the nature of probability, like: is it sub- jective or objective? People like to argue about those questions. But once we get a probability p, we can convert it to information by computing - log p.
57sj | UNITS OF INFORMATION
qdqk | An event of probability <LATEX>1 / 2</LATEX> carries one bit of information. An event of probability <LATEX>1 / e</LATEX> carries one nat of information. An event of probability <LATEX>1 / 3</LATEX> carries one trit of information. An event of probability <LATEX>1 / 4</LATEX> carries one crumb of information. An event of probability <LATEX>1 / 1 0</LATEX> carries one hartley of information. An event of probability <LATEX>1 / 1 6</LATEX> carries one nibble of information. An event of probability <LATEX>1 / 2 5 6</LATEX> carries one byte of information. An event of probability <LATEX>1 / 2 ^ { 8 1 9 2 }</LATEX> carries one kilobyte of information.
emd4 | There are many units of information. Using information <LATEX>= - \log p</LATEX> we can relate these to probabilities. For example if you see a number in base 10, and each digit shows up with probability <LATEX>1 / 1 0 ,</LATEX> the amount of information you get from each digit is one ‘hartley'.
m9fu | How many bits are in a hartley? Remember: no matter what base you use, I call <LATEX>\log 1 0</LATEX> a hartley and log 2 a bit. There are <LATEX>\log 1 0 / \log 2</LATEX> bits in a hartley. This number has the same value no matter what base you use for your logarithms! If you use base 2, it's
b3ph | <LATEX>\log _ { 2 } 1 0 / \log _ { 2 } 2 = \log _ { 2 } 1 0 \approx 3 . 3 2 .</LATEX> So a hartley is about 3.32 bits.
9j3h | If you flip 8 fair coins and tell me what answers you got, I've learned of an event that has probability <LATEX>1 / 2 ^ { 8 } = 1 / 2 5 6 .</LATEX> We say I've received a 'byte' of information. This equals 8 bits of information. Similarly, if you flip <LATEX>1 0 2 4 \times 8</LATEX> fair coins and tell me the outcome, I receive a kilobyte of information.
```

OUTPUT:
```
