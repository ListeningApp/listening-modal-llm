{
  "id": "chatcmpl-A3UELSqs3M9CLwedIJgZkzwrNjppz",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "v24x | For projected Newton-CG, we use the parameter settings from the experiments in Xie and Wright. Specifically, in the notation of Xie and Wright, we set the accuracy parameter and backtracking parameter to zeta equals theta equals zero point five and the step acceptance parameter to eta equals zero point two. Furthermore, following the algorithmic to have equivalent termination conditions, we modify the gradient negativity check description of Xie and Wright, and from g sub L superscript i is less than negative epsilon sub L to the three halves to g sub k superscript i is less than negative epsilon sub k for this method.\n26hh | For projected gradient and Newton-MR TMP, we set the scaling parameter in Algorithms five and six to zeta equals zero point five and the sufficient decrease parameter to rho equals ten to the negative four. All line searches are initialised from alpha sub zero equals one. We note that, for both FISTA and PGM, we terminate the iterations when absolute value of left parenthesis f left parenthesis x sub k right parenthesis plus lambda norm x sub k norm sub one minus left parenthesis f left parenthesis x sub k minus one right parenthesis plus lambda norm x sub k minus one norm sub one right parenthesis is less than ten to the negative eight on the L one problem and absolute value of f left parenthesis x sub k right parenthesis minus f left parenthesis x sub k minus one right parenthesis is less than ten to the negative eight otherwise. We set the momentum term in PGM to beta equals zero point nine and select the fixed step size by starting from alpha equals one and successively shrinking the step size by a factor of ten until the iterates are stable for the duration of the experiment, i.e., no divergence or large scale oscillations. This procedure resulted in a step size of alpha equals ten to the negative three for the L one MLP (Figure three) and alpha equals one for the NNMF problems (Figures four and five).\nw533 | We now give a more complete description of each of the objective functions.\nfj70 | Multinomial Regression We first consider is the problems of multinomial regression on C classes. Specifically, consider a set of data items left brace a sub i, b sub i right brace sub i equals one to n subset of real numbers to the d times left brace one, up to C right brace. Denote the weights of each class as x sub one, up to, x sub C and define x equals left bracket x sub one, up to, x sub C minus one right bracket. We are free to take x sub C equals zero as class C is identifiable from the weights of the other classes. The objective, f, is given by\n04yk | f left parenthesis x right parenthesis equals one over n summation from i equals one to n summation from c equals one to C minus one negative one left parenthesis b sub i equals c right parenthesis log left parenthesis softmax left parenthesis x sub c, a sub i right parenthesis right parenthesis,\n5l8x | where one left parenthesis dot right parenthesis is the indicator function and\nh425 | softmax left parenthesis x sub c, a sub i right parenthesis equals the fraction of exp left parenthesis inner product of x sub c, a sub i right parenthesis over summation from c equals one to C exp left parenthesis inner product of x sub c, a sub i right parenthesis. In this case, the objective is convex. We allow for a constant term in each set of weights, x sub c, which we do not apply the L one penalisation to.\nxfce | All methods for this example are initialised from\ntrvq | x sub zero equals zero. Neural Network Again, suppose we have a set of data items left brace a sub i, b sub i right brace sub i equals one to n subset of real numbers to the d times left brace one, up to C right brace. We consider a small two layer network with a smooth activation function. Specifically, we consider the sigmoid weighted linear unit (SiLU) activation defined by\nztx4 | sigma left parenthesis x right parenthesis equals the fraction of x over one plus e to the negative x. We note that the SiLU activation is similar to ReLU and is the product of a linear activation with a standard sigmoid activation. We define a network, h left parenthesis dot semicolon x right parenthesis parameterised by the weights, x, with the following architecture",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725394553,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 930,
    "prompt_tokens": 3309,
    "total_tokens": 4239
  }
}