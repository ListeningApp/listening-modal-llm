You will receive raw text from an OCR scan of a document or web page. Each line of input represents one textbox from the document. Your task is to faithfully reproduce the text for our text-to-speech engine so that it is easily read aloud.

Note that the text might not start at the beginning and paragraphs may be split acrosss multiple textboxes. Textboxes may start or end in the middle of a word which is acceptable: do not combine text between textboxes and leave broken words at the start and end of a textbox unchanged.

You will receive one textbox per-line in the format `id | Textbox content`. The output must be in the format `id | Clean content`.

RULES:
 • Remove inline citations and references e.g. "(Author, 2021)" or "(https://wikipedia.org/article.html)" or "[ECMOS 35b, 47, 49]".
 • Spell out numbers, dates, and chemical formulas for the TTS engine e.g. "-2.5" -> "negative two point five" or "Jan. 2020" -> "January twenty twenty".
 • Spell out LaTeX formulas for the TTS engine e.g. "<LATEX>p = 1</LATEX>" -> "P equals one".
 • Rejoin hyphenated word fragments within the same textbox e.g. "synth -esize" -> "synthesize".
 • Fix simple typos and OCR errors e.g. "O nce upon a 1ime." -> "Once upon a time.".
 • Delete nonsequiter textboxes that interrupt the middle of a sentence if they obviously do not belong.

Otherwise, the output should exactly match the input text:
 • Do not combine text across lines or attempt to fix words broken across lines.
 • Ensure each ID from the input is included in the output.
 • Make as few changes as possible while respecting the above rules.

EXAMPLE INPUT:
ccpt | stigate the effectiveness of psychedelics for ADHD symp- toms in adults.
0fpw | 2. Materia1 and methods
dqn8 | 2.1. Study design and participants
cop0 | The study employed a naturalistic design, assessing the experiences of participants at bas-,
lhkq | eline before they start, and at 2 and 4 weeks after initiation. The target population included adults with ADHD symptoms who had not been diag- nosed with ADHD before. To be included, participants needed to score on Conner's Adult ADHD Rating Scale (CAARS-S:SV). This cut-off was indicative of clinically elevated symptoms (Conners et al., 1999) (see section 2.3).
82ju | 2.2. Study procedure
y2qo | The online advertisement was placed on a website providing information about psychedelics (www.microdo sing.nl). Interested participants were redirected to information explaining the study rationale and procedure. The baseline survey took 20 min to complete. If the survey had not been completed after 24 h, a reminder was sent. Each of the surveys at the 2- and 4-week time points took about 15 min to complete. Par- ticipants were able to pause the surveys. Data collection occurred between Nov. 2020 and Jul. 2021. The study was approved by the Ethics Review Committee of Psy- chology and Neuroscience at Maastricht University (ERCPN- 215_05_11_2019_A1).
1piq | 2.3. Measures
7fqw | 2.3.1. Demographic information and history of substance use
rhaz | At baseline, demographic information was collected. History of substance use assessed experience with psyche- delics (i.e., ayahuasca, DMT, 5-MeO-DMT, LSD, novel lysergamides (e.g., 1P-LSD, ALD-52), psilocybin, salvia divinorum, ibogaine, and mescaline) in both full (psychedelic) doses and microdoses.
wak1 | 2.3.2. Psychiatric and physiological diagnoses
wtfz | Participants were asked whether they had a current diagnosis of a disorder. These answer options were chosen because most of the listed diagnoses are often reported to co-occur with ADHD (Kooij et al., 2019), or because these diagnoses were reported to be common in people who microdose (Fadiman and Korb, 2019). We 
3j2l | REVIEW ARTICLE Frontiers in Psychiatry | www.frontiersin.org
03k3 | 2
1k9e | constructed a variable Comorbidity alongside ADHD, differenti- ating respondents with and without a comorbid diagnosis alongside ADHD (<LATEX>p < . 0 0 1 =</LATEX> only ADHD or no ADHD diagnosis; <LATEX>p > . 0 1 =</LATEX> ADHD and at least one other diagnosis). We constructed a variable Medication use alongside microdosing, differentiating respondents who were and were not using conven- tiona1 ADHD medication alongside microdosing (<LATEX>0 =</LATEX> only microdosing; <LATEX>1 =</LATEX> conven-

EXAMPLE OUTPUT:
ccpt | stigate the effectiveness of psychedelics for ADHD symptoms in adults.
0fpw | Two. Material and methods
dqn8 | Two point one. Study design and participants
cop0 | The study employed a naturalistic design, assessing the experiences of participants at bas-,
lhkq | eline before they start, and at two and four weeks after initiation. The target population included adults with ADHD symptoms who had not been diagnosed with ADHD before. To be included, participants needed to score on Conner's Adult ADHD Rating Scale. This cut-off was indicative of clinically elevated symptoms (see section two point three).
82ju | Two point two. Study procedure
y2qo | The online advertisement was placed on a website providing information about psychedelics. Interested participants were redirected to information explaining the study rationale and procedure. The baseline survey took twenty minutes to complete. If the survey had not been completed after twenty-four hours, a reminder was sent. Each of the surveys at the two and four-week time points took about fifteen minutes to complete. Participants were able to pause the surveys. Data collection occurred between November twenty twenty and July twenty twenty-one. The study was approved by the Ethics Review Committee of Psychology and Neuroscience at Maastricht University.
1piq | Two point three. Measures
7fqw | Two point three point one. Demographic information and history of substance use
rhaz | At baseline, demographic information was collected. History of substance use assessed experience with psychedelics (i.e., ayahuasca, DMT, five-MeO-DMT, LSD, novel lysergamides (e.g., one-P-LSD, ALD-fifty-two), psilocybin, salvia divinorum, ibogaine, and mescaline) in both full (psychedelic) doses and microdoses.
wak1 | Two point three point two. Psychiatric and physiological diagnoses
wtfz | Participants were asked whether they had a current diagnosis of a disorder. These answer options were chosen because most of the listed diagnoses are often reported to co-occur with ADHD, or because these diagnoses were reported to be common in people who microdose. We
3j2l | REVIEW ARTICLE Frontiers in Psychiatry: www.frontiersin.org
03k3 | Two
1k9e | constructed a variable Comorbidity alongside ADHD, differentiating respondents with and without a comorbid diagnosis alongside ADHD (P is less than point zero zero one equals only ADHD or no ADHD diagnosis; P is greater than point zero one equals ADHD and at least one other diagnosis). We constructed a variable Medication use alongside microdosing, differentiating respondents who were and were not using conventional ADHD medication alongside microdosing (zero equals only microdosing; one equals conven-




llcu | It's a kind of average of A where each value A(i) is 'weighted', i.e. multiplied, by the probability of the ith outcome. We saw an example in the last tweet: the expected amount of rainfall.
4vdi | We've seen that if you believe the ith outcome has probability pi, the amount of information you learn if the ith outcome actually occurs is - log pi. Thus, the expected amount of information you learn is
kl0p | (-log p) = - pi log pi. iEX
czi9 | And this is the Shannon entropy! We denote it by H, or more precisely H (p), so
a3sj | H(p) = - pi log pi. iEX In the box above I was taking X to be the set {1, ... , n}. This is often a good thing to do when there are finitely many outcomes.
fhkc | Let's get to know the Shannon entropy a little better.
5hcr | Puzzle 21. Let <LATEX>X = 1 , 2</LATEX> so that we know a probability distribution <LATEX>p</LATEX> on <LATEX>X</LATEX> if we know <LATEX>p _ { 1 } ,</LATEX> since <LATEX>p _ { 2 } = 1 - p _ { 1 } .</LATEX> Graph the Shannon entropy <LATEX>H \left( p \right)</LATEX> as a function of <LATEX>p _ { 1 } .</LATEX> Show that it has a maximum at <LATEX>p _ { 1 } = \frac { 1 } { 2 }</LATEX> and minima at <LATEX>p _ { 1 } = 0</LATEX> and
em55 | <LATEX>p _ { 1 } = 1 .</LATEX> This makes sense: if you believe <LATEX>p _ { 1 } = 1</LATEX> then you learn nothing when an outcome happens chosen according to the probability distribution <LATEX>p :</LATEX> you are sure outcome 1 will occur, and it does (with probability 1). Similarly, if you believe <LATEX>p _ { 1 } = 0</LATEX> you learn nothing when an outcome happens according to this probability distribution, since you are sure outcome 2 will occur. On the other hand, if <LATEX>p _ { 1 } = \frac { 1 } { 2 } \mathrm { y o u }</LATEX> are maximally undecided about what will happens, and you learn 1 bit of information when it does.
8x5q | Puzzle 22. Let <LATEX>X = 1 , 2 , 3 .</LATEX> Draw the set of probability distributions on <LATEX>X</LATEX> as an equilateral triangle whose corners are the probability distributions <LATEX>\left( 1 , 0 , 0 \right) ,</LATEX> <LATEX>\left( 0 , 1 , 0 \right) ,</LATEX> and <LATEX>\left( 0 , 0 , 1 \right) .</LATEX> Sketch contour lines of <LATEX>H \left( p \right)</LATEX> as a function on this triangle. Show it has a maximum at <LATEX>p = \left( \frac { 1 } { 3 } , \frac { 1 } { 3 } , \frac { 1 } { 3 } \right)</LATEX> and minima at the corners of the triangle.
wv87 | Again this should make intuitive sense. Here is a harder puzzle along the same lines:
97h8 | Puzzle 23. Let <LATEX>X = \left\{ 1 , \ldots , n \right\} .</LATEX> Show that <LATEX>H \left( p \right)</LATEX> has a maximum at <LATEX>p = \left( \frac { 1 } { n } , \ldots , \frac { 1 } { n } \right)</LATEX> and minima at the probability distributions where <LATEX>p _ { i } = 1</LATEX> for some particular
cj29 | <LATEX>i \in X .</LATEX> Here is one of the big lessons from all this:
ekqn | Shannon entropy is larger for probability distributions that are more spread out, and smaller for probability distributions that are more sharply peaked.
hi3b | Indeed, you can think of Shannon entropy as a measure of how spread out a proba- bility distribution is! The more spread out it is, the more you learn when an outcome occurs, drawn from that distribution.
3my4 | THE PRINCIPLE OF MAXIMUM ENTROPY
wizx | Suppose there are n possible outcomes. At first you have no reason to think any is more probable than any other.
7kbg | Then you learn some facts about the correct probability distribution-but not enough to determine it uniquely! Which probability distribution P1, . .. , Pn should you choose?
8wpy | The principle of maximum entropy says:
lev6 | Of all the probability distributions consistent with the facts you've learned, choose the one with the largest Shannon entropy
cnqv | H = - > i=1 n Pi log på
848r | What's Shannon entropy good for? For starters, it gives a principle for choosing the 'best' probability distribution consistent with what you know. Choose the one that maximizes the Shannon entropy!
o1ho | This is called the 'principle of maximum entropy'. This principle first arose in statistical mechanics, which is the application of probability theory to physics-but we can use it elsewhere too.
gd2a | For example: suppose you have a die with faces numbered 1,2,3,4,5,6. At first you think it's fair. But then you somehow learn that the average of the numbers that comes up when you roll it is 5. Given this, what's the probability that if you roll it, a 6 comes up?
z373 | Sounds like an unfair question! But you can figure out the probability distribution on {1, 2, 3, 4, 5, 6} that maximizes Shannon entropy subject to the constraint that the mean is 5. According to the principle of maximum entropy, you should use this to answer my question!
38ni | But is this correct?
effn | The problem is figuring out what 'correct' means! But in statistical mechanics we use the principle of maximum entropy all the time, and it seems to work well. The brilliance of E. T. Jaynes was to realize it's a general principle of reasoning, not just for physics.
ud13 | The principle of maximum entropy is widely used outside physics, though still con- troversial. But I think we should use it to figure out some basic properties of a gas-like its energy or entropy per molecule, as a function of pressure and temperature.
aoj8 | To do this, we should generalize Shannon entropy to 'Gibbs entropy', replacing the sum by an integral. Or else we should 'discretize' the gas, assuming each molecule has a finite set of states. It sort of depends on whether you prefer calculus or programming. Either approach is okay if we study our gas using classical statistical mechanics.
1std | Quantum statistical mechanics gives a more accurate answer. It uses a more general
m2cm | definition of entropy-but the principle of maximum entropy still applies!
2syo | I won't dive into any calculations just yet. Before doing a gas, we should do some simpler examples-like the die whose average roll is 5. But I can't resisting mentioning one philosophical point. In the box above I was hinting that maximum entropy works when your 'prior' is uniform:
dxsw | Suppose there are n possible outcomes. At first you have no reason to think any is more probable than any other.