You will receive raw text from an OCR scan of a document or web page. Each line of input represents one textbox from the document.

Your task is to label each textbox with exactly one of the following labels:
  title: the primary document title, no more than one per document
  heading: the name of the following chapter or section (should never start in the middle of the previous paragraph)
  subheading: a secondary heading, often following another heading (should never start in the middle of the previous paragraph)
  body: main paragraph content: may start or end in the middle of a word, use context to follow paragraphs that are split across textboxes (if a textbox contains both heading and body text, label it "body")
  math: textbox primarily containing math
  imageDescription: standalone callout that only describes an image, chart, table, or similar (typically begins with "Table...", "Figure...", "Fig. ...", "Above...", "Left...", etc.)
  authors: names of contributors to this document
  institutions: list of universities, business, and other institutions affiliated with this document
  publisher: info about the publisher, or provided by the publisher, including copyright info
  pagenum: the page number
  headerOrFooter: boilerplate text page number, or redundant headings at the top/bottom of the page
  toc: the table of contents
  references: bibliographic references and resources
  acknowledgements: thanks and credits to contributors
  appendix: index, appendix, or any other end matter not part of the main thread (including related headings)
  table: tabular data, label, title, or subheading for a grid or list of data
  datum: numeric data or data label from a chart or list
  advertisement: promotional ads and marketing
  layout: non-content related to the layout of the page, e.g. sidebar navigation
  callout: pull quote or long block text that stands apart from the main text
  footnote: footnotes and endnotes near the bottom of the page
  keywords: list of keywords or tags
  url: web address or email
  other: any other text that does not belong to the main thread

Each line of input is prefixed with a textbox ID in the format `id | Text content`. The output must be in the format `id | label`.

Note that body text may be split across multiple textboxes, and textboxes may start or end in the middle of a sentence or word. Because the text was extracted from a web page or document, paragraphs of body text may be interrupted by multiple textboxes of redundant headers, footnotes, page numbers, tables, images, etc. For example, a real heading will never interrupt the middle of a sentence. Use context clues like this to carefully label each textbox.

EXAMPLE INPUT:
1pjs | Neuroscience Applied 1 (2022) 101012
o2kr | Contents lists available at ScienceDirect
v6sk | Neuroscience Applied
1wj6 | journal homepage: www.journals.elsevier.com/neuroscience-applied
936l | Research Articles
06yq | Microdosing with psychedelics to self-medicate for ADHD symptoms in adults: A naturalistic study
dpv6 | ARTICLE INFO
8r5j | ABSTRACT
2taz | Keywords: ADHD Microdosing Psychedelics Self-medication Well-being Time perception
o3ya | ADHD in adulthood is often overlooked, which negatively affects the individual's well-being. First-line phar- macological interventions are effective in many ADHD patients, relieving symptoms rapidly. However, there seems to be a proportion of individuals who discontinue, or fail to respond to these treatments. For these in- dividuals, alternative treatment options should be explored.
r5il | 1. Introduction
5gui | Attention deficit hyperactivity disorder (ADHD) is one of the most common developmental disorders worldwide. Prevalence research indicates that 2.6% of the adult population has persistent ADHD. ADHD in adults is often overlooked because of the high comorbidity rate and lack of knowledge about how ADHD is expressed in adulthood (Kooij et al.,
9d4o | Fig. 1. Flowchart of included participants for each time point.
82qc | 2019). In addition, ADHD is associated with deficits in various domains of cogni- tive functioning. Twenty-five percent of ADHD cases suffered from ADHD symptoms purely because of de- ficiencies in temporal processing (Sonuga-Barke et al., 2010).
ls7d | First-line ADHD treatments in adults mainly include pharmacological interventions to enhance dopaminergic and noradrenergic neurotrans- mission with stimulants. Overall, they have been proven to work effectively in adults with ADHD, inducing fast symptom relief and thereby enhancing the person's quality of life. In the
5esx | Table 1 Demographic information from respondents at baseline and the two and four- week time points.
t8vv | longer term, approximately twenty percent of ADHD patients discontinue their prescribed medication after six to nine months, thirty percent after one year, and half of them after two years.
xrd1 | 2. Material and methods
5746 | 2.1. Study design and participants
7toz | The study employed a naturalistic design, assessing the experiences of participants at baseline,
j346 | Neuroscience Applied 1 (2022)
m5ka | E.C.H.M. Haijen et al.
kx2c | REVIEW
v8uq | before they start MD on their own initiative, and at two and four weeks after MD initiation. The target population included adults diagnosed with ADHD and individuals who experienced ADHD symptoms to the extent that these interfered with their daily lives and who had not been diag- nosed with ADHD before. To be included in the analyses, participants needed to score above a cut-off point on at least one of the subscales of the Conner's Adult ADHD Rating Scale (CAARS-S:SV).
2zmw | Fig. 2. Mean scores of the CAARS-S:SV DSM-IV total symptoms T-scores at baseline (0W) and two (2W) and four weeks (4W) after MD (A) of the whole sample, and (B) per conventional ADHD medication use. Error bars represent mean error.
sdk3 | <LATEX>\lim _ { x ightarrow \infty } rac { 6 x ^ { 2 } + 1 0 0 } { 7 x ^ { 2 } - 1 0 } =</LATEX>
x972 | 2.2. Study procedure
k221 | Mean performance measures of the CAARS-

EXAMPLE OUTPUT:
1pjs | headerOrFooter
o2kr | publisher
v6sk | publisher
1wj6 | publisher
936l | headerOrFooter
06yq | title
dpv6 | other
8r5j | heading
2taz | keywords
o3ya | body
r5il | heading
5gui | body
9d4o | imageDescription
82qc | body
ls7d | body
5esx | imageDescription
t8vv | body
xrd1 | heading
5746 | subheading
7toz | body
j346 | headerOrFooter
m5ka | authors
kx2c | headerOrFooter
v8uq | body
2zmw | imageDescription
sdk3 | math
x972 | heading
k221 | body




g8m3 | An independent test performed by systems integrators in 1995 showed the superiority of this system over other commercial courtesy amount reading systems. The system was integrated in NCR's line of check reading systems. It has been fielded in several banks across the United States since June 1996, and it has been reading millions of checks per day since then.
a9av | XI. CONCLUSIONS
73l3 | During the short history of automatic pattern recognition, increasing the role of learning seems to have invariably improved the overall performance of recognition systems. The systems described in this paper are more evidence to this fact. Convolutional NN's have been shown to eliminate the need for hand-crafted feature extractors. GTN's have been shown to reduce the need for hand-crafted heuristics, manual labeling, and manual parameter tuning in document recognition systems. As training data becomes plentiful, as computers get faster, and as our understanding of learning algorithms improves, recognition systems will rely more and more of learning and their performance will improve.
ornm | Just as the back-propagation algorithm elegantly solved the credit assignment problem in multilayer NN's, the gradient-based learning procedure for GTN's introduced in this paper solves the credit assignment problem in systems whose functional architecture dynamically changes with each new input. The learning algorithms presented here are in a sense nothing more than unusual forms of gradient descent in complex, dynamic architectures, with efficient back-propagation algorithms to compute the gradient. The results in this paper help establish the usefulness and relevance of gradient-based minimization methods as a general organizing principle for learning in large systems.
u2z1 | It was shown that all the steps of a document analysis system can be formulated as GT's through which gradi-
atmh | ents can be back propagated. Even in the nontrainable parts of the system, the design philosophy in terms of graph transformation provides a clear separation between domain-specific heuristics (e.g., segmentation heuristics) and generic, procedural knowledge (the generalized trans- duction algorithm)
7hgg | It is worth pointing out that data generating models (such as HMM's) and the maximum likelihood principle were not called upon to justify most of the architectures and the train- ing criteria described in this paper. Gradient-based learning applied to global discriminative loss functions guarantees optimal classification and rejection without the use of "hard to justify" principles that put strong constraints on the system architecture, often at the expense of performances.
xwsv | More specifically, the methods and architectures pre- sented in this paper offer generic solutions to a large number of problems encountered in pattern recognition systems.
756b | 1) Feature extraction is traditionally a fixed transform, and it is generally derived from some expert prior knowledge about the task. This relies on the probably incorrect assumption that the human designer is able to capture all the relevant information in the input. We have shown that the application of gradient-based learning to convolutional NN's allows us to learn ap- propriate features from examples. The success of this approach was demonstrated in extensive comparative digit recognition experiments on the NIST database.
6bx7 | 2) Segmentation and recognition of objects in images cannot be completely decoupled. Instead of taking hard segmentation decisions too early, we have used HOS to generate and evaluate a large number of hypotheses in parallel, postponing any decision until the overall criterion is minimized.
spl6 | 3) Hand-truthing images to obtain segmented characters for training a character recognizer is expensive and does not take into account the way in which a whole document or sequence of characters will be recog- nized (in particular, the fact that some segmentation candidates may be wrong, even though they may look like true characters). Instead we train multimodule systems to optimize a global measure of performance, which does not require time consuming detailed hand- truthing and yields significantly better recognition performance because it allows to train these modules to cooperate toward a common goal.
7wzm | 4) Ambiguities inherent in the segmentation, character recognition, and linguistic model should be inte- grated optimally. Instead of using a sequence of task- dependent heuristics to combine these sources of in- formation, we have proposed a unified framework in which generalized transduction methods are applied to graphs representing a weighted set of hypotheses about the input. The success of this approach was demonstrated with a commercially deployed check- reading system that reads millions of business and personal checks per day: the generalized transduction engine resides in only a few hundred lines of code.
e9xj | 5) Traditional recognition systems rely on many hand- crafted heuristics to isolate individually recognizable objects. The promising SDNN approach draws on the robustness and efficiency of convolutional NN's to avoid explicit segmentation altogether. Simultaneous automatic learning of segmentation and recognition can be achieved with gradient-based learning meth- ods.
9vl6 | This paper presents a small number of examples of GT modules, but it is clear that the concept can be applied to many situations where the domain knowledge or the state information can be represented by graphs. This is the case in many audio signal recognition tasks, and visual scene analysis applications. Future work will attempt to apply GT networks to such problems, with the hope of allowing more reliance on automatic learning and less on detailed engineering.
c2fd | APPENDIX A PRECONDITIONS FOR FASTER CONVERGENCE
jzvp | As seen before, the squashing function used in our convolutional networks is <LATEX>f \left( a \right) = A \tanh \left( S a \right) .</LATEX> Symmetric functions are believed to yield faster convergence, although the learning can become extremely slow if the weights are too small. The cause of this problem is that in weight space the origin is a fixed point of the learning dynamics and, although it is a saddle point, it is attractive in almost all directions [116]. For our simulations, we use <LATEX>A =</LATEX> 1.7159 and <LATEX>S = 2 / 3</LATEX> (see [20], [34]). With this choice of parameters, the equalities <LATEX>f \left( 1 \right) = 1</LATEX> and <LATEX>f \left( - 1 \right) = - \quad 1</LATEX> are satisfied. The rationale behind this is that the overall gain of the squashing transformation is around one in normal operating conditions, and the interpretation of the state of the network is simplified. Moreover, the absolute value of the second derivative of <LATEX>f</LATEX> is a maximum at +1 and -1, which improves the convergence toward the end of the learning session. This particular choice of parameters is merely a convenience, and does not affect the result.
f71y | Before training, the weights are initialized with random values using a uniform distribution between <LATEX>- 2 . 4 / F _ { i }</LATEX> and <LATEX>2 . 4 / F _ { i } ,</LATEX> where <LATEX>F ;</LATEX> is the number of inputs (fan-in) of the unit which the connection belongs to. Since several connections share a weight, this rule could be difficult to apply, but in our case all connections sharing a same weight belong to units with identical fan-ins. The reason for dividing by the fan-in is that we would like the initial standard deviation of the weighted sums to be in the same range for each unit and to fall within the normal operating region of the sigmoid. If the initial weights are too small, the gradients are very small and the learning is slow. If they are too large, the sigmoids are saturated and the gradient is also very small. The standard deviation of the weighted sum scales like the square root of the number of inputs when the inputs are independent, and it scales linearly with the number of inputs if the inputs are highly correlated. We chose to assume the second hypothesis since some units receive highly correlated signals.
31e1 | APPENDIX B
40ry | STOCHASTIC GRADIENT VERSUS BATCH GRADIENT
d1j2 | Gradient-based learning algorithms can use one of two classes of methods to update the parameters. The first method, dubbed "batch gradient," is the classical one: the gradients are accumulated over the entire training set, and the parameters are updated after the exact gradient has been so computed. In the second method, called "stochastic gradient," a partial, or noisy, gradient is evaluated on the basis of one single training sample (or a small number of samples), and the parameters are updated using this approximate gradient. The training samples can be selected randomly or according to a properly randomized sequence. In the stochastic version the gradient estimates are noisy, but the parameters are updated much more often than with the batch version. An empirical result of considerable practical importance is that on tasks with large, redundant data sets, the stochastic version is considerably faster than the batch version, sometimes by orders of magnitude [117]. Although the reasons for this are not totally understood theoretically, an intuitive explanation can be found in the following extreme example. Let us take an example where the training database is composed of two copies of the same subset. Then accumulating the gradient over the whole set would cause redundant computations to be performed. On the other hand, running Stochastic Gradient once on this training set would amount to performing two complete learning iterations over the small subset. This idea can be generalized to training sets where there exist no precise repetition of the same pattern but where some redundancy is present. In fact stochastic update must be better when there is redundancy, i.e., when a certain level of generalization is expected.
wstc | Many authors have claimed that second-order methods should be used in lieu of gradient descent for NN training. The literature abounds with recommendations [118] for classical second-order methods such as the Gauss-Newton or Levenberg-Marquardt algorithms for quasi-Newton methods such as Broyden-Fletcher-Goldfarb-Shanno, limited-storage Broyden-Fletcher-Goldfarb-Shanno, or for various versions of the conjugate gradients method. Unfortunately, all of the above methods are unsuit- able for training large NN's on large data sets. The Gauss-Newton and Levenberg-Marquardt methods require O(N3) operations per update, where N is the number of parameters, which makes them impractical for even moderate size networks. Quasi-Newton methods require "only" O(N2) operations per update, but that still makes them impractical for large networks. Limited-storage Broy- den-Fletcher-Goldfarb-Shanno's and conjugate gradients require only O(N) operations per update so they would appear appropriate. Unfortunately, their convergence speed relies on an accurate evaluation of successive "conjugate descent directions" which only makes sense in "batch" mode. For large data sets, the speed-up brought by these methods over regular batch gradient descent cannot match the enormous speed up brought by the use of
ba1e | stochastic gradient. Several authors have attempted to use conjugate gradient with small batches or batches of increasing sizes [119], [120], but those attempts have not yet been demonstrated to surpass a carefully tuned stochastic gradient. Our experiments were performed with a stochastic method that scales the parameter axes so as to minimize the eccentricity of the error surface.
en95 | APPENDIX C STOCHASTIC DIAGONAL LEVENBERG-MARQUARDT
l3do | Owing to the reasons given in Appendix B, we prefer to update the weights after each presentation of a single pattern in accordance with stochastic update methods. The patterns are presented in a constant random order, and the training set is typically repeated 20 times.
u9bo | Our update algorithm is dubbed the stochastic diagonal Levenberg-Marquardt method where an individual learning rate (step size) is computed for each parameter (weight) before each pass through the training set [20], [34], [121]. These learning rates are computed using the diagonal terms of an estimate of the Gauss-Newton approximation to the Hessian (second derivative) matrix. This algorithm is not believed to bring a tremendous increase in learning speed but it converges reliably without requiring extensive adjustments of the learning parameters. It corrects major ill-conditioning of the loss function that are due to the peculiarities of the network architecture and the training data. The additional cost of using this procedure over standard stochastic gradient descent is negligible.
7avo | At each learning iteration a particular parameter wk is updated according to the following stochastic update rule:
uzo6 | (18)
89b4 | Wk + Wk - th awk
5f7e | where EP is the instantaneous loss function for pattern p. In convolutional NN's, because of the weight sharing, the partial derivative JEP /Owk is the sum of the partial derivatives with respect to the connections that share the parameter wk
h83u | JEP OWk (i,j)EVk
kjoa | Ouij (19)
wb1c | where uij is the connection weight from unit j to unit i, Vk is the set of unit index pairs (i, j) such that the connection between i and j share the parameter wk, i.e.,
ak13 | Wij = wk V(i, j) E Vk- (20)
s7el | As stated previously, the step sizes Ex are not constant but are function of the second derivative of the loss function along the axis Wk
0lfu | (21)
cnjl | Ek = Į+hkk
rtdl | where u is a hand-picked constant and hkk is an estimate of the second derivative of the loss function E with respect to Wk. The larger hkk is, the smaller the weight update. The parameter µ prevents the step size from becoming too
norp | large when the second derivative is small, very much like the "model-trust" methods, and the Levenberg-Marquardt methods in nonlinear optimization [8]. The exact formula to compute hkk from the second derivatives with respect to the connection weights is
chx3 | hkk = (i,j)EVk (k,l)EVk
sfcf | JulijOukl . (22)
nuxs | However, we make three approximations. The first approx- imation is to drop the off-diagonal terms of the Hessian with respect to the connection weights in (22)
umx0 | 02 E
o306 | hkk =
qm1b | (23)
n9ms | (i,j)EVk
icdp | Naturally, the terms 02 E/Ou2; are the average over the training set of the local second derivatives
wtse | p=1 P 32 EP
bm7g | @2E 1 HR
jr68 | (24)
axix | Those local second derivatives with respect to connection weights can be computed from local second derivatives with respect to the total input of the downstream unit
05y4 | 82 EP 02 EP
2fb0 | (25)
s9fn | where x; is the state of unit j and 02 EP /da? is the second derivative of the instantaneous loss function with respect to the total input to unit i (denoted a;). Interestingly, there is an efficient algorithm to compute those second derivatives which is very similar to the back-propagation procedure used to compute the first derivatives [20], [21]
k5r6 | k 02 EP daž + f" (i) on; . aEP
z9v9 | da? = f'(a)2 Z uk 32 EP
gpu9 | (26
u5ws | Unfortunately, using those derivatives leads to well-known problems associated with every Newton-like algorithm: these terms can be negative and can cause the gradient algorithm to move uphill instead of downhill. Therefore, our second approximation is a well-known trick called the Gauss-Newton approximation, which guarantees that the second derivative estimates are nonnegative. The Gauss-Newton approximation essentially ignores the nonlinearity of the estimated function (the NN, in our case), but not that of the loss function. The back propagation equation for Gauss-Newton approximations of the second derivatives is
onkh | 82 EP = f'(ai)} } Uni k 02 EP daž
8t55 | (27)
7imw | This is very similar to the formula for back propagating the first derivatives, except that the sigmoid's derivative and the weight values are squared. The right-hand side is a sum of products of nonnegative terms, therefore the left-hand side term is nonnegative.