{
  "id": "chatcmpl-A0ZlZBifEkhWkmuK6Aikj7Sqnu6iH",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "```\nu8nw | Text Aligner. The text aligner A generates an alignment dalign between mel-spectrograms and phonemes. We train a text aligner A alongside the decoder G during the reconstruction phase. Modeled after the decoder of Tacotron two with attention, A is initially trained on an automatic speech recognition task using the LibriSpeech corpus and then fine-tuned concurrently with our decoder. We call an aligner with this setup a Transferable Monotonic Aligner.\nl2v9 | Style Encoder. Given an input mel-spectrogram X, our encoder derives a style vector s equals E of X. With various reference audios, E can generate diverse style representations, allowing the decoder G to create speech that mirrors the style s of a reference audio X. E consists of four residual blocks followed by an averaging pooling layer along the time axis.\nafbe | Pitch Extractor. As in FastPitch, we extract pitch F zero directly in Hertz without further processing, providing a more straightforward representation and allowing enhanced control of speech pitch. Instead of using the acoustic periodicity detection method employed in FastPitch to estimate the ground truth pitch, we train a pitch extractor F end-to-end with our decoder G for a more accurate estimation. Our pitch extractor F is a JDC network, pre-trained on LibriSpeech with ground truth F zero estimated using YIN. This extractor is\n78r8 | fine-tuned with the decoder to predict pitch p X equals F of X for the reconstruction of X.\n320g | Decoder. Our decoder G is trained to reconstruct the input mel-spectrogram X, represented by c equals G of h text. dalign, s, p X, ne. Here, h text. dalign is aligned hidden representation of phonemes, s equals E of s is the style vector of X, p X is pitch contour of X, and ne is energy represented by the log norm of X per frame. The decoder is comprised of seven residual blocks with AdaIN, defined as follows:\ntp8g | AdaIN of c, s equals La of s times c minus mu of c over sigma of c plus Lu of s.\ngxfd | where c is a single channel of the feature maps, s is the style vector, u of dot and o of dot denote the channel mean and standard deviation and Lo and Lu are learned linear projections for computing the adaptive gain and bias using the style vector s. The use of AdaIN is one of the major differences between our model and other TTS models with style encoders. The advantages of AdaIN for diverse speech synthesis are further discussed in Appendix A-B.\ns2v1 | To prevent dilution of import features, we concatenate the pitch p X, energy ne, and residual phoneme features h res and deliver them to subsequent residual blocks after AdaIN. This process is further detailed in Table, and its effectiveness is discussed in Section four-D.\njkp9 | Discriminator. We include a discriminator D to facilitate training of our decoder for better sound quality. The discriminator shares the same architecture as our style encoder.\nz8tt | Duration Predictor. Our duration predictor consists of a three-layer bidirectional LSTM R with adaptive layer normalization module followed by a linear projection L, where instance normalization is replaced by layer normalization in equation one. We use adaptive layer normalization because R takes discrete tokens similar to those in NLP applications, where layer normalization is preferred. R is shared with the prosody predictor P through h prosody equals R of h text as input to P.\n9g22 | Prosody Predictor. Our prosody predictor P predicts both the pitch p X and energy ne with given text and style vector. The aligned shared representation h prosody of at is processed through a shared bidirectional LSTM layer to generate h prosody, which is then fed into two sets of three residual blocks with AdaIN and a linear projection layer, one for the pitch output and another for the energy output.\n41fx | B. Training Objectives\n7um8 | Our model training process is divided into two stages to allow the integration of duration-invariant prosody data augmentation, a key contribution of our work. During the first stage, the model learns to reconstruct the mel-spectrogram from the text, pitch, energy, and style. The second stage fixes all modules except the duration and prosody predictors, which are trained to predict the duration, pitch, and energy from the given text. One First Stage Objectives:\nuow2 | Mel reconstruction. Given a mel-spectrogram X belonging to X prime and its corresponding text t belonging to T, we train our decoder under the L one reconstruction loss\niat7 | C mel equals expectation of X, t absolute value X minus G of h text dot align, s, p X, ne.\nywwr | a Mel-spectrogram reconstruction\n0i6j | where h text equals T of t is the encoded phoneme representation, align equals A of X, t is the attention alignment from the text aligner, s equals E of X is the style vector of X, p X equals F of X is the pitch F zero of X and ne is the energy of X. For end-to-end training with the decoder and the text aligner, we apply a novel fifty percent-fifty percent strategy: half the time, we use the raw attention output as the alignment, which allows gradient backpropagation through the text aligner; for the other half, we use the non-differentiable monotonic version of alignment through dynamic programming algorithms to train the decoder for generating intelligible speech from monotonic hard alignment during inference. This innovative approach effectively fine-tunes the pre-trained text aligner to produce the optimal alignments for speech reconstruction, thus enhancing the sample quality of generated speech. The effectiveness of this strategy is analyzed in section four-D.\n87cq | TMA objectives. We fine-tune our text aligner with the original sequence-to-sequence automatic speech recognition objectives L s two s to ensure that correct attention alignment is kept during the end-to-end training:\n0qs3 | L s two s equals expectation of X, t sum from i equals one to N cross-entropy of t i, t i.\nz8f7 | three\n7w7j | where N is the number of phonemes in t, t i is the i-th phoneme token of t, t i is the i-th predicted phoneme token, and cross-entropy of dot is the cross-entropy loss function.\n51vd | Since this alignment is not necessarily monotonic, we use a simple L one loss L mono that forces the soft attention alignment to be close to its non-differentiable monotonic version:\nbaj1 | L mono equals expectation of X, t absolute value align minus hard one, where align equals A of X, t is the attention alignment and hard is the monotonic hard alignment obtained through dynamic programming algorithms.\nn1wo | b Duration and prosody prediction\n5bq3 | Adversarial objectives. We employ two adversarial loss functions, the original cross-entropy loss function L A D V for adversarial training and the additional feature-matching loss L f m, to improve the sound quality of the reconstructed mel-spectrogram:\nezla | L A D V equals expectation of X, t log D of X plus log one minus D of X.\nrsbu | five\nmagd | one sum from l equals one to T N vertical absolute value D prime of X minus D prime of X vertical one.\ntb74 | L f m equals expectation of X, t\n8l69 | where and is the reconstructed mel-spectrogram by G, T is the total number of layers in D and D prime denotes the output feature map of l-th layer with N number of features. The feature matching loss can be seen as a reconstruction loss of hidden layer features of real and generated speech as judged by the discriminator.\n```",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1724700849,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_a2ff031fb5",
  "usage": {
    "completion_tokens": 1620,
    "prompt_tokens": 3367,
    "total_tokens": 4987
  }
}