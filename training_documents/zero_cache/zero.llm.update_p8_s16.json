{
  "id": "chatcmpl-A3UAG5kf9b8ehLn4DlQ6pC6hlrcXY",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "oqod | Sampling from LMs Pre-trained LMs are sensitive to sampling parameters and the specific example given in the prompt. For all evaluated methods, we perform hyperparameter search over various sampling parameters, and for methods using a fixed prompt example, we report metrics averaged across three randomly chosen examples. To select the best run for each method, we rank the runs by the sum of LCS and executability, each normalized by human-expert scores. Further details are in Appendix A point one.\ntey0 | Model Choices For Planning LM, we evaluate a representative set of causal language models. For Translation LM, we mainly use Sentence-Roberta-three hundred fifty-five M and provide relevant ablations in Section five point three. GPT-three and Codex are accessed using OpenAI API, and the remaining models are accessed through open-source packages, Hugging Face Transformers and SentenceTransformers, all without additional training (except for the fine-tuning baseline).\nghiv | Four point one Do LLMs contain actionable knowledge for high-level tasks?\no5ts | We first investigate whether LLMs can generate sensible action plans expressed in free-form language. We use the approach described in Section three point one to query pre-trained LLMs. To evaluate the correctness of generated action plans, we conduct human evaluations. For each model, we ask ten human annotators to determine - by answering \"Yes\" or \"No\" - whether each task can be completed using provided action steps. To provide a reference of how humans might rate the action plans provided by other humans, we also ask annotators to rate the human-written action plans included in the VirtualHome dataset for the same set of tasks. In contrast to the free-form text output by LLMs, humans wrote the plans using a graphical programming interface that enforces strict syntax and a chosen set of atomic action vocabulary, which limit the expressivity and the completeness of their answers. More details of our human evaluation procedure can be found in Appendix A point two.\nhd7d | We show the human evaluation results in Figure one, where the y-axis shows correctness averaged across all tasks and all annotators. Surprisingly, when LLMs are large enough and without imposed syntactic constraints, they can generate highly realistic action plans whose correctness - as deemed by human annotators - even surpasses human-written action plans. We also observe some level of correctness for smaller models such as GPT-two. However, inspection of its produced output indicates\nehux | that it often generates shorter plans by ignoring common-sense actions or by simply rephrasing the given task (e.g. the task \"Go to sleep\" produces only a single step \"Go to bed\"). These failure modes sometimes mislead human annotators to mark them correct as the annotators may ignore common-sense actions in their judgment as well, resulting in a higher correctness rate than the quality of the output shows.\npoxj | Four point two How executable are the LLM action plans?\n2xvb | We analyze the executability of LLM plans by evaluating them in all seven household scenes in VirtualHome. As shown in Table one, we find action plans generated naively by LLMs are generally not very executable. Although smaller models seem to have higher executability, we find that the majority of these executable plans are produced by ignoring the queried task and repeating the given example of a different task. This is validated by the fact that smaller models have lower LCS than larger models despite having high executability, showing that this failure mode is prevalent among smaller models. In contrast, larger models do not suffer severely from this failure mode. Yet as a result of being more expressive, their generated programs are substantially less executable.\nm1hx | Four point three Can LLM action plans be made executable by proposed procedure?\n9psh | We evaluate the effectiveness of our proposed procedure of action translation. We first create a bank of all allowed forty-seven thousand five hundred twenty-two action steps in the environment, including all possible combinations of atomic actions and allowed arguments or objects. Then we use an off-the-shelf Sentence-Roberta as Translation LM to create embeddings for actions and output text. For better computational efficiency, we pre-compute the embeddings for all allowed actions, leaving minor computation overhead for our procedure over the baseline methods at inference time. As shown in Table one, executability of generated programs is significantly improved. Furthermore, we also observe improved LCS because the translated action steps precisely follow the program syntax and thus are more similar to the plans produced by human experts. Sample output is shown in Figure one and a larger random subset of generated samples can be found in Appendix A point five.\naiqv | To validate their correctness, we again perform human evaluations using the same procedure from Section four point one. Results are shown in Table one. We find that despite being more similar to human-written plans as they follow strict syntax, the programs are deemed less correct by humans compared to their vanilla counterparts. By examining the output, we observe two main sources of errors. First, we find Translation LM is poor at mapping compounded instructions to a succinct admissible action, e.g. \"brush teeth with toothbrush and toothpaste\". Second, we find that the generated programs are sometimes terminated too early. This is partly due to the imperfect expressivity of the environment;\n4klz | certain necessary actions or objects are not implemented to fully achieve some tasks, so Translation LM cannot map to a sufficiently similar action. This is also reflected by our human evaluation results of the programs written by other humans, as only seventy percent of the programs are considered complete.\nxz5d | Five Analysis and Discussions\nerwx | Five point one Ablation of design decisions\nie7i | We perform ablation studies for the three components of our proposed procedure, described in Section three point two, three point three, and three point four respectively. As shown in Table two, leaving out any of the three components would all lead to decreased performance in both executability and LCS. An exception is Translated GPT-three without Trajectory Correction, where we observe a slight improvement in LCS at the expense of a considerable drop in executability. Among the three proposed components, leaving out action translation leads to the most significant executability drop, showing the importance of action translation in extracting executable action plans from LLMs.\naksw | Five point two Are the generated action plans grounded in the environment?\n0fsl | Since successful execution of correct action plans directly measures grounding, we calculate the percentage of generated action plans that are both correct and executable. We deem an action plan to be correct if seventy percent or more human annotators decide it is correct. Human-written plans are one hundred percent executable, of which sixty-five point nine one percent are deemed correct. Results for LMs are shown in Figure four.\n0m3g | Although smaller LMs such as GPT-two can generate highly executable action plans as shown in Table one, these executable plans mostly are not correct, as they often repeat the given example or do not contain all necessary steps. Increasing model parameters can lead to some improvement in generating plans that are both executable and correct, yet it scales poorly with the parameter count. In the meantime, action translation offers a promising way towards grounding actionable knowledge by producing executable and correct plans, though a large gap remains to be closed to reach human-level performance.",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725394300,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 1489,
    "prompt_tokens": 3216,
    "total_tokens": 4705
  }
}