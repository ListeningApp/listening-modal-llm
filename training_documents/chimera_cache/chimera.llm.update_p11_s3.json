{
  "id": "chatcmpl-A3UD9Zd33T1tvnRnC9Lsk7lojl1dm",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "kg3t | Note that we use the same model partition method as the default setting in PipeDream-twobee doubleyou, namely evenly partitioning the basic layers among the workers. Other model partition methods trying to balance the weights among the workers may help to reduce the peak memory consumption of PipeDream-twobee doubleyou, but this is outside the scope of this paper. Generally, Chimera is on par with PipeDream-twobee doubleyou (the latest asynchronous approach) in terms of training throughput, but more convergence-friendly since there is no stale weights in Chimera.\nlvae | We also conduct the evaluation on the cluster with four by eight equals thirty-two V-one hundred GPUs connected by NVLink (intra-node) and Infiniband (inter-node). Experimental results for Bert-fourty-eight are shown in Figure sixteen. On thirty-two V-one hundred GPUs, Chimera improves the throughput by one point ten X to two point thirty-nine X and one point zero five X to one point eighty-nine X over the synchronous and asynchronous pipeline approaches, respectively, which demonstrates that the same conclusions hold on newer machines.\no9sc | Four point three Scale to Large Mini-Batches on a Given Number of Machines\n121u | In this section, we evaluate the training throughput when there are a large number of micro-batches available for each worker within a training iteration. In which case the bubble problem of all synchronous approaches is alleviated.\nzh03 | Figure seventeen presents the throughput of Bert-fourty-eight on thirty-two GPU nodes when scaling to large mini-batches. Different from the other approaches, PipeDream updates the model after training on each micro-batch, and therefore its B stops scaling after reaching the memory limit. Consistently, we search the parameter space and present the best performance for each baseline. For example, to achieve the best performance, DAPPLE and GPipe switch from B equals four to B equals eight when B is greater than or equal to one thousand twenty-four for higher computational efficiency, in which case the bubble problem is less important. PipeDream-twobee doubleyou also uses D equals four, and the B increase from sixteen to thirty-two when B is greater than or equal to one thousand twenty-four. Recall that in Section three point five we discuss three methods for scaling Chimera to large mini-batches. Forward doubling with B equals eight and backward halving with B equals four aim at solving the intermediate bubbles problem. However, the former suffers from recomputation overhead while the latter suffers from lower computational efficiency. Direct concatenation with B equals eight achieves the best performance among these\n1mvw | three methods on Bert-fourty-eight, which can be explained by the fact that the intermediate bubbles caused by the uneven workloads between forward and backward passes can be utilized to accommodate the point-to-point communication between pipeline stages. For B less than or equal to two thousand forty-eight where bubbles still matter, we observe significant improvement of Chimera (direct) over all the synchronous approaches. Overall, for B greater than or equal to one thousand twenty-four, Chimera (direct) is very close to PipeDream-twobee doubleyou (asynchronous using stale weights), and achieves on average one point thirteen X, two point zero seven X, and one point zero six X speedup over GPipe (suffering from recomputation), GEMS (suffering from high bubble ratio), and DAPPLE, respectively.\n6d4s | Figure eighteen presents the throughput of GPT-two on five hundred twelve GPU nodes when scaling to large mini-batches. For GPT-two, Chimera with D equals eight with forward doubling outperforms direct concatenation, since activation recomputation is required in both methods but the former removes intermediate bubbles. Note that GPipe outperforms DAPPLE when scaling to large mini-batches in GPT-two, this is because both approaches require recomputation but the pipeline scheduling of GPipe is more regular and better to overlap the point-to-point communication. Benefiting from the sophisticated (less bubbles and more communication overlap as discussed in Section three point five) pipeline scheduling of Chimera with forward doubling, our approach outperforms all the baselines, and achieves on average one point thirteen X, one point eighteen X, two point sixty X, and one point thirty-four X speedup over PipeDream-twobee doubleyou, GPipe, GEMS, and DAPPLE, respectively. These results demonstrate that Chimera with forward doubling efficiently scales to large mini-batches for the large models where activation recomputation is commonly required.\nmqah | Four point four Chimera with More than Two Pipelines\nxxr8 | Figure nineteen presents the throughput of Chimera with more than two pipelines, combined together on a thirty-two-layer GPT-two model. For the case of one pipeline, we use one F one B scheduling with pipeline flushing. With D equals thirty-two, four pipelines achieve the best performance because it hits the sweet spot of the trade-off between less bubbles and higher allreduce overhead. However, as the pipeline stages get coarser by decreasing D to sixteen, four pipelines performs worse then two pipelines because of the increasing of allreduce overhead. Two pipelines (the default setting of Chimera) usually achieve the highest performance among all the configurations. We expect that Chimera with more than two pipelines would further improve the performance on future deep models with deeper pipeline and higher computation density on each stage.\nrl4j | Five CONCLUSION\nvz32 | Chimera brings new insights for efficiently pipelining large neural networks training at scale. Compared with the state-of-the-art pipeline approaches, Chimera achieves the best balance among pipeline efficiency, memory cost, and convergence friendliness. Empirical results for large language models training on up to two thousand forty-eight GPU nodes show that Chimera significantly improves the training throughput over the counterparts. We foresee that our approach will be one of the major solutions for massively scaling deep learning training. To reduce the communication cost of gradient synchronization by exploiting sparsification and quantization in deep learning training is our next step.\nncb0 | A point one SUMMARY OF THE EXPERIMENTS REPORTED\n8mp0 | We evaluated Chimera on the C.S.C.S. Piz Daint supercomputer. Each Cray XC-fifty compute node contains a twelve-core Intel Xeon E-five twenty-ninety CPU with sixty-four gigabytes RAM, and one NVIDIA Tesla P-one hundred with sixteen gigabytes memory. The compute nodes are connected by Cray Aries interconnect in a Dragonfly topology. We used GLOO in PyTorch as the distributed backend. We utilized the GPU for acceleration in all the experiments, as described in the paper. The source code of Chimera is as follows:\nxjyk | A point two BASELINE EXPERIMENTAL SETUP, AND MODIFICATIONS MADE FOR THE PAPER\nmt0f | Relevant hardware details: C.S.C.S. Piz Daint supercomputer. Each Cray XC-fifty compute node contains a twelve-core Intel Xeon E-five twenty-ninety CPU and one NVIDIA Tesla P-one hundred GPU. The filesystem is Lustre.\nawab | Operating systems and versions: SUSE SLES eleven point three Compilers and versions: GCC nine point three point zero\nixcc | Applications and versions: Bert, GPT-two Libraries and versions: PyTorch one point six Key algorithms: stochastic gradient descent Input datasets and versions: Wikipedia dataset, Wiki text-two dataset URL to output from scripts that gathers execution environment information.",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725394479,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 1522,
    "prompt_tokens": 3227,
    "total_tokens": 4749
  }
}