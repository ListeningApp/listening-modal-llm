{
  "id": "chatcmpl-A3UN4rAALYzW37zGvgOPkwYrkBZUq",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "2zpk | However, as the story above illustrates, even concerted effort at the effectiveness end of the trial design spectrum is not enough to guarantee routine usage of a clinical innovation. Moreover, education and monitoring are not sufficient to change provider behavior, as indicated\nepu7 | by a meta-analysis by the Cochrane Collaboration which showed that audit and feedback only increased target provider behaviors by four point three percent. Thus, simply using traditional clinical trials approaches and pushing them further into real-world conditions is not sufficient to guarantee public health impact. Factors contributing to the non-adoption of evidence-based clinical innovations must be addressed directly through promising methods and strategies that can be subjected to rigorous investigation in the emerging field of implementation science.\n7od1 | Three. What is implementation science?\n0bqy | Three point one. Some history\nb780 | While researchers, clinicians, and healthcare leaders have been at least partially and intermittently aware of the problem of non-adoption of evidence-based clinical innovations, the area began to coalesce as a sustained field of study with the publication of Everett Rogers' Diffusion of Innovations. Drawing on examples from agriculture and industry as well as health, Rogers conceptualized the spread, or diffusion, of innovations as a social process with multiple determinants well beyond the evidence supporting the innovation itself. Concurrently, others also called attention to contextual aspects of innovation adoption or non-adoption, including the field of knowledge utilization, which addresses the tenuous link of evidence to policy decisions; and the field of technology transfer, which investigates reasons that knowledge and technology do not flow freely even within single organizations. These areas of study, and others, have provided a collective foundation upon which the field of implementation science emerged.\nccmb | Three point two. Defining implementation science\nma93 | Implementation science has been defined in several ways that are consistent with the definition put forward in the inaugural issue of the journal, Implementation Science: \"the scientific study of methods to promote the systematic uptake of research findings and other evidence-based practice into routine practice and, hence, to improve the quality and effectiveness of health services.\" Hence the goal of implementation science is not to establish the health impact of a clinical innovation, but rather to identify the factors that affect its uptake into routine use.\nc6hh | As outlined above, ample experience indicates that factors which affect the uptake of a clinical innovation differ from those factors that determine whether an innovation works. The crux of implementation science is thus two-fold:\ny7ea | Identify uptake barriers and facilitators across multiple levels of context (individuals in treatment, providers, organization, and other stakeholder groups), and\nnav4 | Develop and apply implementation strategies that overcome these barriers and enhance the facilitators to increase the uptake of evidence-based clinical innovations.\ngem8 | In its focus on changing individual and social unit behavior, implementation science is close kin to the wide spectrum of fields ranging, for example, from health behavior change to organizational psychology.\ne8nr | Four. How implementation science differs from clinical research, quality improvement, and dissemination\nk78w | Four point one. Clinical research versus implementation research\ng9du | Perhaps the major difference between clinical research and implementation research is that the latter actively engages with the context into which a clinical innovation is introduced, rather than controlling, efficacy studies, or simply tolerating, effectiveness studies, that context. Consider a hypothetical trial of intranasal ketamine for depression.\nf682 | Implementation trial designs differ from efficacy and effectiveness clinical trials at a fundamental level, that of the hypothesis. While clinical trials focus on contrasting the health effects of an innovation with a comparison group, implementation trials focus on testing strategies to increase uptake and sustainability of the innovation, in this example intranasal ketamine. Efficacy and effectiveness trials seek to evaluate a clinical innovation, a clinical treatment, test, organization of care, or other initiative to be introduced into a healthcare venue. In contrast, implementation trials seek to evaluate a strategy for increasing uptake of the evidence-based innovation into routine practice, typically an integrated set, bundle, or package of implementation innovations to enhance the usage of the clinical innovation.\nli1k | While the subject population and setting in an implementation trial may resemble those used in effectiveness trials, alternatively the unit of observation in implementation trials may also be providers or clinics or even entire organizations depending on the focus of the implementation effort. Further, as the population becomes more heterogeneous in moving from efficacy to effectiveness designs, outcome measures must become briefer and simpler in order to minimize respondent burden and retain protocol subjects who are less research-tolerant; this is true in implementation trials as well. As one moves from left to right in Table One, intervention clinicians become progressively less highly specialized and skill development efforts more closely resemble training that would typically be feasible under general conditions; measurement of fidelity to the intervention varies similarly.\npbea | Consideration of the context for the research also differs substantively across the efficacy-effectiveness-implementation spectrum. Efficacy trials value conducting a trial that is invariant from beginning to end. This entails sometimes extraordinary efforts at training clinicians and tracking subjects, so that the research team's efforts amount at times to \"crypto-case management,\" aggressive subject engagement and follow-up outreach, to ensure close follow-up and minimal data loss, which is not a problem since the focus of the trial is solely on testing the intervention under optimal conditions. However, since effectiveness studies are more concerned with performance of an efficacious intervention in real-world settings, careful attention is paid to limiting the amount of research-funded effort that is incorporated into the protocol, often \"firewalling\" research effort with carefully and explicitly defined parameters. Further, for implementation trials, trying to optimize the natural context in which uptake is being measured is a threat to the validity of the trial. Thus, researcher involvement at the sites is drastically circumscribed, with\nccpt | some implementation trials even limiting research support to training endogenous clinicians and utilizing remote, \"light-touch\" outcome assessments for patient and provider subjects.",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725395094,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 1209,
    "prompt_tokens": 3105,
    "total_tokens": 4314
  }
}