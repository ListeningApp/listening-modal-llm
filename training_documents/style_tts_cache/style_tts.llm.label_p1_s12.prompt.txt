You will receive raw text from an OCR scan of a document or web page. Each line of input represents one textbox from the document.

Your task is to label each textbox with exactly one of the following labels:
  title: the primary document title, no more than one per document
  heading: the name of the following chapter or section (should never start in the middle of the previous paragraph)
  subheading: a secondary heading, often following another heading (should never start in the middle of the previous paragraph)
  body: main paragraph content: may start or end in the middle of a word, use context to follow paragraphs that are split across textboxes (if a textbox contains both heading and body text, label it "body")
  math: textbox primarily containing math
  imageDescription: standalone callout that only describes an image, chart, table, or similar (typically begins with "Table...", "Figure...", "Fig. ...", "Above...", "Left...", etc.)
  authors: names of contributors to this document
  institutions: list of universities, business, and other institutions affiliated with this document
  publisher: info about the publisher, or provided by the publisher, including copyright info
  pagenum: the page number
  headerOrFooter: boilerplate text page number, or redundant headings at the top/bottom of the page
  toc: the table of contents
  references: bibliographic references and resources
  acknowledgements: thanks and credits to contributors
  appendix: index, appendix, or any other end matter not part of the main thread (including related headings)
  table: tabular data, label, title, or subheading for a grid or list of data
  datum: numeric data or data label from a chart or list
  advertisement: promotional ads and marketing
  layout: non-content related to the layout of the page, e.g. sidebar navigation
  callout: pull quote or long block text that stands apart from the main text
  footnote: footnotes and endnotes near the bottom of the page
  keywords: list of keywords or tags
  url: web address or email
  other: any other text that does not belong to the main thread

Each line of input is prefixed with a textbox ID in the format `id | Text content`. The output must be in the format `id | label`.

Note that body text may be split across multiple textboxes, and textboxes may start or end in the middle of a sentence or word. Because the text was extracted from a web page or document, paragraphs of body text may be interrupted by multiple textboxes of redundant headers, footnotes, page numbers, tables, images, etc. For example, a real heading will never interrupt the middle of a sentence. Use context clues like this to carefully label each textbox.

EXAMPLE INPUT:
1pjs | Neuroscience Applied 1 (2022) 101012
o2kr | Contents lists available at ScienceDirect
v6sk | Neuroscience Applied
1wj6 | journal homepage: www.journals.elsevier.com/neuroscience-applied
936l | Research Articles
06yq | Microdosing with psychedelics to self-medicate for ADHD symptoms in adults: A naturalistic study
dpv6 | ARTICLE INFO
8r5j | ABSTRACT
2taz | Keywords: ADHD Microdosing Psychedelics Self-medication Well-being Time perception
o3ya | ADHD in adulthood is often overlooked, which negatively affects the individual's well-being. First-line phar- macological interventions are effective in many ADHD patients, relieving symptoms rapidly. However, there seems to be a proportion of individuals who discontinue, or fail to respond to these treatments. For these in- dividuals, alternative treatment options should be explored.
r5il | 1. Introduction
5gui | Attention deficit hyperactivity disorder (ADHD) is one of the most common developmental disorders worldwide. Prevalence research indicates that 2.6% of the adult population has persistent ADHD. ADHD in adults is often overlooked because of the high comorbidity rate and lack of knowledge about how ADHD is expressed in adulthood (Kooij et al.,
9d4o | Fig. 1. Flowchart of included participants for each time point.
82qc | 2019). In addition, ADHD is associated with deficits in various domains of cogni- tive functioning. Twenty-five percent of ADHD cases suffered from ADHD symptoms purely because of de- ficiencies in temporal processing (Sonuga-Barke et al., 2010).
ls7d | First-line ADHD treatments in adults mainly include pharmacological interventions to enhance dopaminergic and noradrenergic neurotrans- mission with stimulants. Overall, they have been proven to work effectively in adults with ADHD, inducing fast symptom relief and thereby enhancing the person's quality of life. In the
5esx | Table 1 Demographic information from respondents at baseline and the two and four- week time points.
t8vv | longer term, approximately twenty percent of ADHD patients discontinue their prescribed medication after six to nine months, thirty percent after one year, and half of them after two years.
xrd1 | 2. Material and methods
5746 | 2.1. Study design and participants
7toz | The study employed a naturalistic design, assessing the experiences of participants at baseline,
j346 | Neuroscience Applied 1 (2022)
m5ka | E.C.H.M. Haijen et al.
kx2c | REVIEW
v8uq | before they start MD on their own initiative, and at two and four weeks after MD initiation. The target population included adults diagnosed with ADHD and individuals who experienced ADHD symptoms to the extent that these interfered with their daily lives and who had not been diag- nosed with ADHD before. To be included in the analyses, participants needed to score above a cut-off point on at least one of the subscales of the Conner's Adult ADHD Rating Scale (CAARS-S:SV).
2zmw | Fig. 2. Mean scores of the CAARS-S:SV DSM-IV total symptoms T-scores at baseline (0W) and two (2W) and four weeks (4W) after MD (A) of the whole sample, and (B) per conventional ADHD medication use. Error bars represent mean error.
sdk3 | <LATEX>\lim _ { x ightarrow \infty } rac { 6 x ^ { 2 } + 1 0 0 } { 7 x ^ { 2 } - 1 0 } =</LATEX>
x972 | 2.2. Study procedure
k221 | Mean performance measures of the CAARS-

EXAMPLE OUTPUT:
1pjs | headerOrFooter
o2kr | publisher
v6sk | publisher
1wj6 | publisher
936l | headerOrFooter
06yq | title
dpv6 | other
8r5j | heading
2taz | keywords
o3ya | body
r5il | heading
5gui | body
9d4o | imageDescription
82qc | body
ls7d | body
5esx | imageDescription
t8vv | body
xrd1 | heading
5746 | subheading
7toz | body
j346 | headerOrFooter
m5ka | authors
kx2c | headerOrFooter
v8uq | body
2zmw | imageDescription
sdk3 | math
x972 | heading
k221 | body




1pjs | arXiv:2205.15439v2 [eess.AS] 20 Nov 2023
o2kr | StyleTTS: A Style-Based Generative Model for Natural and Diverse Text-to-Speech Synthesis
v6sk | Abstract-Text-to-Speech (TTS) has recently seen great progress in synthesizing high-quality speech owing to the rapid development of parallel TTS systems. Yet producing speech with naturalistic prosodic variations, speaking styles, and emotional tones remains challenging. In addition, many existing parallel TTS models often struggle with identifying optimal monotonic alignments since speech and duration generation typically occur independently. Here, we propose StyleTTS, a style-based generative model for parallel TTS that can synthesize diverse speech with natural prosody from a reference speech utterance. Using our novel Trans- ferable Monotonic Aligner (TMA) and duration-invariant data augmentation, StyleTTS significantly outperforms other baseline models on both single and multi-speaker datasets in subjective tests of speech naturalness and synthesized speaker similarity. Through self-supervised learning, StyleTTS can generate speech with the same emotional and prosodic tone as the reference speech without needing explicit labels for these categories. In addition, when trained with a large number of speakers, our model can perform zero-shot speaker adaption. The source code and audio samples can be found on our demo page at https://styletts.github.io/.
1wj6 | I. INTRODUCTION
936l | Text-to-speech (TTS), also known as speech synthesis, has made significant strides with the advent of deep learning, producing increasingly human-like synthetic speech [1], [2], [3]. However, synthesizing expressive speech that captures the full range of prosodic, temporal, and spectral features, also known as paralinguistic information, remains a challenge [4]. A single piece of text can be spoken in various ways, influenced by context, emotional tone, and a speaker's unique linguistic habits. Hence, TTS is fundamentally a one-to-many mapping problem requiring innovative approaches.
06yq | While several strategies have been proposed to address this, including methods based on variational inference [1], [5], [6], [7], flow-based modeling [1], [8], [9], controlling pitch, duration and energy [10], [11], and using external prosody encoder [12], [13], [14], the production of synthesized speech still falls short of real human speech. In particular, accurately modeling and incorporating different speakers' speaking styles and emotional tones poses a significant challenge.
dpv6 | Many attempts have been made to integrate style information into TTS models [12], [13], [15], [16]. These approaches are predominantly based on autoregressive models such as Tacotron. Non-autoregressive parallel TTS models, such as Fastspeech [17] and Glow-TTS [9], however, have several advantages over autoregressive models. These models generate speech in parallel, enabling fast speech synthesis, and they are also more robust to longer and out-of-distribution (OOD) utterances. Moreover, because phoneme duration, pitch, and energy are predicted in- dependently from speech, models such as FastSpeech2 [11] and
8r5j | FastPitch [18] allow fully controllable speech synthesis. At the same time, these models have limitations. They predominantly concentrate on speech synthesis from a single target speaker and often achieve multi-speaker extensions by concatenating speaker embeddings with the encoder output. Models that explore speech styles also incorporate styles by concatenating style vectors and phoneme embeddings as input to the decoder. [12], [13], [15], [16]. This approach may not capture the temporal variation of acoustic features in the target speech effectively. In contrast, the domain of style transfer introduces styles through conditional normalization methods like adaptive instance normalization (AdaIN) [19]. This technique has proven effective in neural style transfer [20], [21], [22], generative modeling [23], [24], [25], and neural image editing [26], [27]. Application of these methods in speech synthesis has not been extensively explored yet, restricted primarily to voice conversion and speaker adaptation [28], [29], [30], [31], [32].
2taz | The structure of parallel TTS models allows for the entire speech to be synthesized, presenting an opportunity to leverage the powerful AdaIN module for integrating generalized styles in diverse speech synthesis. Recent state-of-the-art models mostly employ the non-autoregressive parallel framework for TTS, but because they do not directly align the input text and speech like autoregressive models do, an external aligner such as the Montreal Forced Aligner [33] that is pre-trained on a large dataset is often required. Since the external aligner is not trained on the TTS data and objectives, the alignments are not optimally suited for the TTS task. Although training internal aligners alleviates some generalization problems [1], [9], [34], [35], overfitting can occur as the aligners are trained on a smaller TTS dataset with only a mel-reconstruction loss.
o3ya | Here, we introduce StyleTTS, a model that addresses the aforementioned challenges of incorporating diverse speaking styles and learning a reliable monotonic aligner. StyleTTS incorporates style-based generative modeling into a parallel TTS framework to enable natural and expressive speech synthesis. It leverages AdaIN to integrate style vectors derived from reference audio, capturing the full spectrum of a speaker's stylistic features. This allows our model to synthesize speech that emulates the prosodic patterns and emotional tones in the reference audio. With various reference audios, we can synthesize the same text in different speaking styles, effectively realizing the one-to-many mapping that many TTS systems find challenging. In addition, our model employs a novel Transferable Monotonic Aligner (TMA) to find the optimal text alignment, aided by a novel duration-invariant data augmenta- tion scheme to produce naturalistic prosody robust to potentially suboptimal duration predictions. Our model's design is robust against the generalization problems of external aligners and
r5il | overfitting problems that can be caused by internal aligners.
5gui | This paper presents the following novel contributions: (i) the introduction of the Transferable Monotonic Aligner (TMA), a new transfer learning scheme that refines pre-trained text aligners for TTS tasks, (ii) a duration-invariant data augmen- tation method for improving prosody prediction, and (iii) a parallel TTS model that incorporates generalized speech styles for natural and expressive speech synthesis. Together, these contributions pave the way for advanced TTS technologies enhancing human-computer interactions.
82qc | II. METHODS
ls7d | A. Proposed Framework
tzt3 | Given t & T the input phonemes and x 6 X' an arbitrary reference mel-spectrogram, our goal is to train a system that generates the mel-spectrogram & E X' that corresponds to the speech of t and reflects the generalized speech styles of x. Generalized speech styles are defined as any characteristics in the reference audio x except the phonetic content [16], including but not limited to prosodic pattern, lexical stress, formants transition, speaking rate, and speaker identity. Our framework consists of eight modules that can be divided into three major categories: (i) speech generation modules that include the text encoder, style encoder, and decoder, (ii) TTS prediction modules that include the duration and prosody predictor, and (iii) utility modules only used during training that include the discriminator, text aligner, and pitch extractor. An overview of our framework is provided in Figure 1. We detail each of the modules below.
t8vv | Text Encoder. The text encoder T transforms the phonemes t into a hidden representation htext = T(t). It consists of a 3-layer CNN followed by a bidirectional LSTM [36].
qvnu | Text Aligner. The text aligner A generates an alignment dalign between mel-spectrograms and phonemes. We train a text aligner A alongside the decoder G during the reconstruction phase. Modeled after the decoder of Tacorton 2 with attention, A is initially trained on an automatic speech recognition (ASR) task using the LibriSpeech corpus [37] and then fine-tuned concurrently with our decoder. We call an aligner with this setup (pre-trained on large corpora and fine-tuned for TTS tasks) a Transferable Monotonic Aligner (TMA).
kggw | Style Encoder. Given an input mel-spectrogram x, our encoder derives a style vector s = E(x). With various reference audios, E can generate diverse style representations, allowing the decoder G to create speech that mirrors the style s of a reference audio x. E consists of four residual blocks [38] followed by an averaging pooling layer along the time axis.
xrd1 | Pitch Extractor. As in FastPitch [11], we extract pitch F0 directly in Hertz without further processing, providing a more straightforward representation and allowing enhanced control of speech pitch. Instead of using the acoustic periodicity detection method [39] employed in FastPitch to estimate the ground truth pitch, we train a pitch extractor F end-to-end with our decoder G for a more accurate estimation. Our pitch extractor F is a JDC network [40], pre-trained on LibriSpeech with ground truth F0 estimated using YIN [41]. This extractor is
5746 | fine-tuned with the decoder to predict pitch px = F(x) for the reconstruction of x.
7toz | Decoder. Our decoder G is trained to reconstruct the input mel-spectrogram x, represented by c = G (htext . dalign, S, Px, nx). Here, htext . dalign is aligned hidden representation of phonemes, s = E(s) is the style vector of x, Px is pitch contour of x, and ne is energy (represented by the log norm) of x per frame. The decoder is comprised of seven residual blocks with AdaIN [19], defined as follows:
j346 | AdaIN(c, s) = La(s) c - µ(c) σ(c) + Lu(s), (1)
v8uq | where c is a single channel of the feature maps, s is the style vector, u(.) and o(.) denote the channel mean and standard deviation and Lo and Lu are learned linear projections for computing the adaptive gain and bias using the style vector s. The use of AdaIN is one of the major differences between our model and other TTS models with style encoders such as [13] and [16]. The advantages of AdaIN for diverse speech synthesis are further discussed in Appendix A-B.
x972 | To prevent dilution of import features, we concatenate the pitch Px, energy ng, and residual phoneme features hres and deliver them to subsequent residual blocks after AdaIN. This process is further detailed in Table , and its effectiveness is discussed in Section IV-D.
cwrw | Discriminator. We include a discriminator D to facilitate training of our decoder for better sound quality [1]. The discriminator shares the same architecture as our style encoder.
4i0e | Duration Predictor. Our duration predictor consists of a 3- layer bidirectional LSTM R with adaptive layer normalization (AdaLN) module followed by a linear projection L, where instance normalization is replaced by layer normalization in equation 1. We use AdaLN because R takes discrete tokens similar to those in NLP applications, where layer normalization [42] is preferred. R is shared with the prosody predictor P through hprosody = R (htext) as input to P.
h2l3 | Prosody Predictor. Our prosody predictor P predicts both the pitch px and energy nx with given text and style vector. The aligned shared representation hprosody . @ is processed through a shared bidirectional LSTM layer to generate hprosody, which is then fed into two sets of three residual blocks with AdaIN and a linear projection layer, one for the pitch output and another for the energy output (see Appendix D for details).
dfzb | B. Training Objectives
u6ty | Our model training process is divided into two stages to allow the integration of duration-invariant prosody data augmentation, a key contribution of our work. During the first stage, the model learns to reconstruct the mel-spectrogram from the text, pitch, energy, and style. The second stage fixes all modules except the duration and prosody predictors, which are trained to predict the duration, pitch, and energy from the given text. 1) First Stage Objectives:
awvu | Mel reconstruction. Given a mel-spectrogram x E X' and its corresponding text t & T, we train our decoder under the L1 reconstruction loss
on9b | Cmel = Ex,t [lax - G (htext . Malign, S, Pac, na )|] (2)
a8ge | (a) Mel-spectrogram reconstruction
h27a | Fig. 1. Training and inference schemes of StyleTTS. (a) Stage 1 of our training procedures where the decoder is trained to reconstruct input mel-spectrogram using pitch, energy, phonemes, alignment, and style vectors. (b) Stage 2 of training and inference procedures where pitch, energy, and alignment are predicted based on input text, and a style vector is extracted from a reference mel-spectrogram for synthesis. Parameters of modules in blue are fixed during this stage of training while those in orange are tuned.
pdjh | where htext = T(t) is the encoded phoneme representation, @align = A(x, t) is the attention alignment from the text aligner, s = E(x) is the style vector of x, Px = F(x) is the pitch FO of x and ne is the energy of x. For end-to-end (E2E) training with the decoder and the text aligner, we apply a novel 50%-50% strategy: half the time, we use the raw attention output as the alignment, which allows gradient backpropagation through the text aligner; for the other half, we use the non- differentiable monotonic version of alignment through dynamic programming algorithms [9] to train the decoder for generating intelligible speech from monotonic hard alignment during inference. This innovative approach effectively fine-tunes the pre-trained text aligner to produce the optimal alignments for speech reconstruction, thus enhancing the sample quality of generated speech. The effectiveness of this strategy is analyzed in section IV-D.
xmgs | TMA objectives. We fine-tune our text aligner with the original sequence-to-sequence ASR objectives Ls2s to ensure that correct attention alignment is kept during the E2E training:
bd9h | Ls2s = Ex,t i=1 N CE(ti, ti)| tˆi) ,
m047 | (3)
gy8c | where N is the number of phonemes in t, ti is the i-th phoneme token of t, ti is the i-th predicted phoneme token, and CE(.) is the cross-entropy loss function.
y4mj | Since this alignment is not necessarily monotonic, we use a simple L-1 loss Lmono that forces the soft attention alignment to be close to its non-differentiable monotonic version:
vpgq | Lmono = Ex,t [|@align - @hard|1] , (4) where aalign = A(x, t) is the attention alignment and @hard is the monotonic hard alignment obtained through dynamic programming algorithms (see Appendix A-A for details).
ousl | (b) Duration and prosody prediction
m4ju | Adversarial objectives. We employ two adversarial loss functions, the original cross-entropy loss function Lady for adversarial training and the additional feature-matching loss [43] Lfm, to improve the sound quality of the reconstructed mel-spectrogram:
a0mb | Lady = Ex,t [log D(x) + log (1 - D(x))] ,
ki8m | (5)
km8w | 1 l=1 T NĮ | D'(x) - D'(x)|1| , (6)
wzno | Lfm = Ex,t
xagm | where & is the reconstructed mel-spectrogram by G, T is the total number of layers in D and D' denotes the output feature map of l-th layer with Nį number of features. The feature matching loss can be seen as a reconstruction loss of hidden layer features of real and generated speech as judged by the discriminator.