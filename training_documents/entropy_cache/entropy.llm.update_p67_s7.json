{
  "id": "chatcmpl-A3UG3eEOcFhrDFY5iSdO638EBr68m",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "gilw | so its entropy is S equals negative integral E to the power of negative P squared plus two divided by two, ln of E to the power of negative P squared plus two divided by two, dP dQ from negative infinity to infinity.\nx96u | Let's do this integral!\n3txt | Let's compute the Boltzmann distribution P of P and Q and the entropy S. To keep the formulas clean, we'll work in units where M equals K equals K equals H equals one, and compute everything at one special temperature: T equals one.\nqe3d | In this setup H equals two pi, and\nod26 | E to the power of negative BE of P and Q equals E to the power of negative P squared plus Q squared divided by two\nl4k6 | is a beautiful Gaussian with integral\ny4a0 | integral from negative infinity to infinity E to the power of negative P squared plus Q squared divided by two equals two pi.\nbnif | These two factors of two pi cancel when we compute the denominator of the probability distribution P of P and Q:\nq018 | dp dq two pi times two pi equals one.\njbak | Thus, we get simply\ne9cp | P of P and Q equals E to the power of negative P squared plus Q squared divided by two.\n94lh | The entropy of the harmonic oscillator is thus\njxk1 | integral dp dq when K equals K equals H equals T equals one. Next let's do this integral.\nwgvf | S equals negative integral E to the power of negative P squared plus two divided by two ln of E to the power of negative P squared plus two divided by two from negative infinity to infinity.\n3jlo | ENTROPY OF THE HARMONIC OSCILLATOR: Six\nldqe | When M equals K equals K equals H equals T equals one the entropy of a classical harmonic oscillator is\nqt5x | integral from negative infinity to infinity E to the power of negative P squared plus Q squared divided by two ln of E to the power of negative P squared plus Q squared divided by two dP dQ\ncc6l | S equals\nw8g7 | one over two integral zero to infinity R squared times E to the negative R squared divided by two dR dTheta (switching to polar) equals one half times two plus one half R squared\n0lyq | (doing the zero integral)\nmk88 | equals integral U E to the negative U du\nkft0 | (substituting U equals R squared divided by two)\nafqg | equals one\nvz07 | Now let's do the integral to compute the entropy of the harmonic oscillator. We copy a famous trick for computing the integral of a Gaussian. First we switch to polar coordinates in the P Q plane, where\n8wou | R squared equals P squared plus Q squared and dP dQ equals R dR dTheta.\n6x7z | Then we integrate with respect to Theta, which cancels out the factor of one over two pi. Then we do a substitution U equals R squared divided by two. But for us R squared divided by two is minus the logarithm of the Gaussian:\n1q42 | negative P plus zero squared divided by two equals one.\n5ycu | so we're left with\nlhe0 | S equals integral U E to the negative U du\narcp | which we can do with integration by parts.\n8wsg | After all this work, we get an incredibly simple answer:\nm65y | S equals one.\nm652 | So in the special case where M equals K equals K equals H equals T equals one, the entropy of a classical harmonic oscillator in thermal equilibrium is one.\n00j4 | Now let's return to the general case, and finish the job.\nfkpa | ENTROPY OF THE HARMONIC OSCILLATOR: Seven\n7ovw | A classical harmonic oscillator with frequency w has entropy\n7h0s | S equals k ln of A k T divided by H w\n7cdl | for some dimensionless constant A.\n8mej | But when M equals K equals K equals H equals T equals one we have w equals one and S equals one, so we must have\nmlec | and thus finally\nllys | k T divided by H w plus one\nrmu5 | S equals k ln\n3o98 | Knowing the entropy in one special case, we can figure out the constant A in our general formula for the entropy. Our general formula says\nzd6z | S equals k ln of A k T divided by H w.\nk1a2 | But when M equals K equals T equals H equals one we get w equals K divided by M equals one, and we saw last time that in this special case we get\nnlu6 | S equals one.\nsiqn | So A must equal E.\n2ae7 | Thus, the entropy of an oscillator with frequency w at temperature T is\nu9wd | S equals k ln of E k T divided by H w equals k ln of k T plus one.\n6i0o | The extra one here is fascinating to me. If we had slacked off, ignored the possibility of a dimensionless constant A, and crudely used dimensional analysis to guess S approximately the way people often do, we might have gotten\n54u3 | S equals k ln of k T divided by H w\n1x9z | This would be off by one nat.\n4235 | What does the one extra nat mean? It seems pretty mysterious now. But later we'll understand it! I already mentioned that often entropy is roughly k times the logarithm of something called the 'number of accessible states'. But that formula is not exactly right: there's also an extra term related to energy, and that accounts for the one extra nat here. Be patient, and you'll see what I mean.\n8xlb | WHERE ARE WE NOW?\nj21f | The mystery: why does each molecule of hydrogen have approximately twenty-three bits of entropy at standard temperature and pressure?\njftb | The goal: derive and understand the formula for the entropy of a classical ideal monatomic gas:\ngj9e | equals N V to the power of N plus three ln K plus seven\n12a1 | including the mysterious constant gamma.\nucuv | The subgoal: compute the entropy of a single classical particle in a one-dimensional box.\n2tn9 | The sub-subgoal: explain entropy from the ground up, and compute the entropy of a classical harmonic oscillator:\n0g39 | S equals k ln minus k T plus one\n3qn6 | Okay, so we've gotten somewhere! By doing the right integral, we've figured out that the entropy S of a classical harmonic oscillator of frequency w in thermal equilibrium at temperature T is\n65yq | S equals k ln of k T plus one.\np4rq | where k is Boltzmann's constant and h is Planck's constant.\n33o8 | We could compute the entropy of a single particle in a box the same way, and also the entropy of a classical ideal diatomic gas. But the integrals get a bit hairy, so people prefer to use a clever trick called the 'partition function'. It's definitely worth learning. It's not merely a clever trick, it gives new insights on the relation between entropy, energy and temperature. So let's talk about it.\nfopo | THE PARTITION FUNCTION\nb12h | If a system has a set of states X with measure dX and its energy is a function E of X to R, its partition function is\nvk7m | Z of beta equals integral E to the power of negative beta E of X dX\n39f8 | where beta is the coolness.\nupcg | I want to compute the entropy of a particle in a box, and ultimately the entropy of a box of hydrogen. We could do it directly, but that's a bit ugly. It's better to use the 'partition function'. This amazing function knows everything about statistical mechanics. From it you can get the entropy-and much more!\nqzb9 | THE PARTITION FUNCTION AND THE BOLTZMANN DISTRIBUTION\nacan | If a system has a set of states X with measure dX and its energy is E: X to R, in thermal equilibrium at coolness beta its probability distribution of states is the Boltzmann distribution:\nj623 | P of X equals S sub X E to the power of negative beta E of X dX E to the power of negative beta E of X equals E to the power of negative beta E of X Z of beta\nd7ly | where Z of beta equals integral E to the power of negative beta F of X dX\nplf9 | is its partition function.\nvk11 | In fact we've already seen the partition function: it's the thing you have to divide E to the power of negative beta E of X by to get a function whose integral is one. And that function whose integral is one is the Boltzmann distribution: the probability distribution of states in thermal equilibrium at coolness beta. So the partition function is a humble normalizing factor! And yet we'll see that it's incredibly powerful. It's kind of surprising.\njj0l | Like Lagrangians in classical mechanics, it's fairly easy to use partition functions, but it's harder to understand what they 'really mean'. We will try. But first let's see how to use them.\nqdqt | THE PARTITION FUNCTION KNOWS ALL!\n6h7m | If a system has partition function\nb6vb | Z of beta equals integral E to the power of negative beta E of X dX\nc5h2 | then in thermal equilibrium at coolness beta its expected energy is\neccu | E equals negative one over Z ln Z and its entropy is\nrtpq | k times ln Z minus beta d ln Z",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725394659,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 2037,
    "prompt_tokens": 3606,
    "total_tokens": 5643
  }
}