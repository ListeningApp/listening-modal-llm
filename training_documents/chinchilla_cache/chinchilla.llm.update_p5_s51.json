{
  "id": "chatcmpl-A3U6JdDQotEKUDSs8FJozHOCwaSC2",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "a0a5 | For each parameter count N we train four different models, decaying the learning rate by a factor of ten times over a horizon (measured in number of training tokens) that ranges by a factor of sixteen times. Then, for each run, we smooth and then interpolate the training loss curve. From this, we obtain a continuous mapping from FLOP count to training loss for each run. Then, for each FLOP count, we determine which run achieves the lowest loss. Using these interpolants, we obtain a mapping from any FLOP count C, to the most efficient choice of model size N and number of training tokens D such that FLOPs(N,D) equals C. At one thousand five hundred logarithmically spaced FLOP values, we find which model size achieves the lowest loss of all models along with the required number of training tokens. Finally, we fit power laws to estimate the optimal model size and number of training tokens for any given amount of compute, obtaining a relationship Nopt proportional to C to the power of a and Dopt proportional to C to the power of b. We find that a equals zero point five and b equals zero point five-as summarized in Table two. In Section D point four, we show a head-to-head comparison at ten to the power twenty-one FLOPs, using the model size recommended by our analysis and by the analysis of Kaplan et al.-using the model size we predict has a clear advantage.\ngocl | Three point two. Approach Two: IsoFLOP profiles\nob1z | In our second approach we vary the model size for a fixed set of nine different training FLOP counts (ranging from six times ten to the power eighteen to three times ten to the power twenty-one FLOPs), and consider the final training loss for each point. In contrast with Approach One that considered points (N, D, L) along the entire training runs. This allows us to directly answer the question: For a given FLOP budget, what is the optimal parameter count?\nm4cb | For each FLOP budget, we plot the final loss (after smoothing) against the parameter count in Figure Three (left). In all cases, we ensure that we have trained a diverse enough set of model sizes to see a clear minimum in the loss. We fit a parabola to each IsoFLOPs curve to directly estimate at what model size the minimum loss is achieved (Figure Three (left)). As with the previous approach, we then fit a power law between FLOPs and loss-optimal model size and number of training tokens, shown in Figure Three (center, right). Again, we fit exponents of the form Nopt proportional to C to the power of a and Dopt proportional to C to the power of b and we find that a equals zero point four nine and b equals zero point five one-as summarized in Table two.\nm61y | Three point three. Approach Three: Fitting a parametric loss function\nu2ob | Lastly, we model all final losses from experiments in Approach One and Two as a parametric function of model parameter count and the number of seen tokens. Following a classical risk decomposition, we propose the following functional form\n2gnm | L(N, D) equals E plus N to the power of negative a plus D to the power of negative b.\nemgw | The first term captures the loss for an ideal generative process on the data distribution, and should correspond to the entropy of natural text. The second term captures the fact that a perfectly trained transformer with N parameters underperforms the ideal generative process. The final term captures the fact that the transformer is not trained to convergence, as we only make a finite number of optimization steps, on a sample of the dataset distribution.\nxaqt | Model fitting. To estimate (A, B, E, a, b), we minimize the Huber loss between the predicted and observed log loss using the L-BFGS algorithm.\n5rcb | sum over i of Huber of log L(Ni, Di) minus log Li\n524j | \n(\nthree\n)\nxd3m | We account for possible local minima by selecting the best fit from a grid of initializations. The Huber loss is robust to outliers, which we find important for good predictive performance over held-out data points. Section D point two details the fitting procedure and the loss decomposition.\n8qb6 | Efficient frontier. We can approximate the functions Nopt and Dopt by minimizing the parametric loss L under the constraint FLOPs(N, D) proportional to six N D. The resulting Nopt and Dopt balance the two terms in Equation three that depend on model size and data. By construction, they have a power-law form:\nwkbn | Nopt (C) =\n(\nG\n)\nto the power one over (a plus b), Dopt (C) equals (G)\n7xi5 | \nNopt\nto the power a over (a plus b)\nw8r5 | \n, and b equals a over (a plus b).\n(\nfour\n)\nybik | We show contours of the fitted function L, and the closed-form efficient computational frontier in blue. From this approach, we find that a equals zero point four six and b equals zero point five four-as summarized in Table two.\ngii5 | Three point four. Optimal model scaling\nq5eu | We find that the three approaches, despite using different fitting methodologies and different trained models, yield comparable predictions for the optimal scaling in parameters and tokens with FLOPs. All three approaches suggest that as compute budget increases, model size and the amount of training data should be increased in approximately equal proportions. The first and second approaches yield very similar predictions for optimal model sizes, as shown in Figure one and Figure A three. The third approach predicts even smaller models being optimal at larger compute budgets. We note that the observed points (L, N, D) for low training FLOPs (C less than ten to the power twenty-one) have larger residuals L minus L(N, D) than points with higher computational budgets. The fitted model places increased weight on the points with more FLOPs-automatically considering the low-computational budget points as outliers due to the Huber loss. As a consequence of the empirically observed negative curvature in the frontier C to Nopt, this results in predicting a lower Nopt than the two other approaches.\na7ao | In Table three we show the estimated number of FLOPs and tokens that would ensure that a model of a given size lies on the compute-optimal frontier. Our findings suggest that the current generation of\n29a3 | large language models are considerably oversized, given their respective compute budgets, as shown in Figure one. For example, we find that a one hundred seventy-five billion parameter model should be trained with a compute budget of four point four one times ten to the power twenty-four FLOPs and on over four point two trillion tokens. A two hundred eighty billion Gopher-like model is the optimal model to train given a compute budget of approximately ten to the power twenty-five FLOPs and should be trained on six point eight trillion tokens. Unless one has a compute budget of ten to the power twenty-six FLOPs (over two hundred fifty times the compute used to train Gopher), a one trillion parameter model is unlikely to be the optimal model to train. Furthermore, the amount of training data that is projected to be needed is far beyond what is currently used to train large models, and underscores the importance of dataset collection in addition to engineering improvements that allow for model scale. While there is significant uncertainty extrapolating out many orders of magnitude, our analysis clearly suggests that given the training compute budget for many current large language models, smaller models should have been trained on more tokens to achieve the most performant model.\nsb1e | In Appendix C, we reproduce the IsoFLOP analysis on two additional datasets: C four and GitHub code. In both cases we reach the similar conclusion that model size and number of training tokens should be scaled in equal proportions.\nw2yn | Four. Chinchilla",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725394055,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 1657,
    "prompt_tokens": 3398,
    "total_tokens": 5055
  }
}