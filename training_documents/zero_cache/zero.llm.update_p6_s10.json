{
  "id": "chatcmpl-A3UAGUj6Q5sgBEhG7hOcCf0dc4jLi",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "```g5at | ni one j equals one ni log P subscript O (X , X I , less than j) where zero parameterizes the Planning LM. (one) X I\nre3j | argmax ( P subscript O (X) equals greater than\n6z45 | Three point two Admissible Action Parsing by Semantic Translation\n20af | One issue arises when naively following the above approach to generate action plans: the plan expressed in free-form language often cannot be mapped to unambiguous actionable steps and thus is not executable by a robotic agent. Many reasons can cause such failures: one) the output does not follow pre-defined mappings of any atomic action (e.g. \"I first walk to the bedroom\" is not of the format \"walk to PLACE\"), two) the output may refer to atomic action and objects using words unrecognizable by the environment (e.g. \"microwave the chocolate milk\" where \"microwave\" and \"chocolate milk\" cannot be mapped to precise action and object), or three) the output contains lexically ambiguous words (e.g. \"open TV\" should instead be \"switch on TV\").\nhuae | Instead of developing a set of rules to transform the free-form text into admissible action steps, we propose to again leverage world knowledge learned by language models to semantically translate the action. For each admissible environment action A subscript E , we calculate its semantic distance to the predicted action phrase A hat by cosine similarity:\nrblh | C left( F left( A hat right) , F left( A subscript E right) right) equals fraction numerator F left( A hat right) dot F left( A subscript E right) over denominator norm of F left( A hat right) norm of F left( A subscript E right) end fraction where F is an embedding function . (two)\ngl5i | To embed the output action phrase and environment actions, we use a BERT-style LM pre-trained with Sentence-BERT objective, to which we refer as Translation LM cubed. The action embedding is obtained by mean-pooling the last layer hidden states across all tokens in that action phrase. While the set of admissible actions in our environment is discrete and possible to exhaustively enumerate, sampling or projection can be employed in larger discrete or continuous action spaces.\nx0yg | Three point three Autoregressive Trajectory Correction\nmjt8 | Translating each step of the program after the entire program has been synthesized lacks consideration of achievability of individual steps and subjects to compounding errors. In practice, LLMs might output compounded instructions for a single step, even though it cannot be completed using one admissible action in the environment. To this end, we can instead interleave plan generation and action translation to allow for automatic trajectory correction. At each step, we first query Planning LM to generate K samples for a single action left( A subscript one hat, A subscript two hat, ellipsis, A subscript K hat right). For each sample A hat, we consider both its semantic soundness and its achievability in the environment. Specifically, we aim to find admissible environment action A subscript E by modifying the ranking scheme described in Equation one as follows:\nhtcn | argmax subscript A subscript E left bracket max subscript A hat C left( F left( A hat right), F left( A subscript E right) right) plus beta dot P subscript theta left( A hat right) right bracket where beta is a weighting coefficient . (three)\n86d4 | Then we append the translated environment action A subscript E to the unfinished text completion. This way all subsequent steps will be conditioned on admissible actions instead of free-form action phrases generated by Planning LM. Furthermore, we can use Translation LM to detect out-of-distribution actions, those outside the capabilities of a robot, and terminate a program early instead of mapping to a faulty action. This can be achieved by setting a threshold epsilon such that if max subscript A hat comma A subscript E C left( F left( A hat right), F left( A subscript E right) right) plus beta dot P subscript theta left( A hat right) less than epsilon at step T, the program is terminated early. Since we now sample Planning LM for individual steps instead of an entire sequence, another termination condition we consider is when greater than fifty percent of current-step samples are zero-length (excluding leading or trailing non-English text tokens).\nz7qp | Three point four Dynamic Example Selection for Improved Knowledge Extraction\n6t1g | So far in the text, we always give the same example in the prompt for all query tasks. However, consider the task of \"ordering pizza\". Prompting LLMs with this task may give the assumption that the agent is initialized in front of a computer, and the LLMs may guide the agent to search for a pizza store and click \"checkout my cart\". Although these are reasonable and feasible in the real world, such assumption cannot always be made as these interactions may not be supported in simulated environments. In fact, the closest series of actions that human experts give in VirtualHome may be \"walking to a computer\", \"switching on the computer\", and \"typing the keyboard\". Without being fine-tuned on these data, LLMs would often fail at these tasks.\ncpls | To provide weak supervision at inference time, we propose to select the most similar task T and its example plan E from the demonstration set to be used as the example in the prompt. Specifically, we re-use the same Translation LM introduced in Section three point two and select left( T star, E star right) whose high-level task name T star maximizes C left( F left( T right), F left( Q right) right), where Q is the query task. This approach bears resemblance to several recent works. An example is shown in Figure two where \"Shave\" is the most similar to the query task \"Apply lotion\".\nm50n | FINAL METHOD Combining the various improvement discussed above, we refer to the final method as Translated LM, where LM is replaced by specific language model used such as GPT-three.\neztf | Four Results\nbwsu | In this section, we first show that language models can generate sensible action plans for many high-level tasks, even without any additional training. Then we highlight its inadequacy when naively applied to embodied environments and demonstrate how this can be improved by again leveraging world knowledge learned by LLMs. Visualization of generated programs is shown in Figure three.\n```",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725394300,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 1350,
    "prompt_tokens": 3362,
    "total_tokens": 4712
  }
}