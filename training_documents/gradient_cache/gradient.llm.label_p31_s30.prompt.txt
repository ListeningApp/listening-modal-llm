You will receive raw text from an OCR scan of a document or web page. Each line of input represents one textbox from the document.

Your task is to label each textbox with exactly one of the following labels:
  title: the primary document title, no more than one per document
  heading: the name of the following chapter or section (should never start in the middle of the previous paragraph)
  subheading: a secondary heading, often following another heading (should never start in the middle of the previous paragraph)
  body: main paragraph content: may start or end in the middle of a word, use context to follow paragraphs that are split across textboxes (if a textbox contains both heading and body text, label it "body")
  math: textbox primarily containing math
  imageDescription: standalone callout that only describes an image, chart, table, or similar (typically begins with "Table...", "Figure...", "Fig. ...", "Above...", "Left...", etc.)
  authors: names of contributors to this document
  institutions: list of universities, business, and other institutions affiliated with this document
  publisher: info about the publisher, or provided by the publisher, including copyright info
  pagenum: the page number
  headerOrFooter: boilerplate text page number, or redundant headings at the top/bottom of the page
  toc: the table of contents
  references: bibliographic references and resources
  acknowledgements: thanks and credits to contributors
  appendix: index, appendix, or any other end matter not part of the main thread (including related headings)
  table: tabular data, label, title, or subheading for a grid or list of data
  datum: numeric data or data label from a chart or list
  advertisement: promotional ads and marketing
  layout: non-content related to the layout of the page, e.g. sidebar navigation
  callout: pull quote or long block text that stands apart from the main text
  footnote: footnotes and endnotes near the bottom of the page
  keywords: list of keywords or tags
  url: web address or email
  other: any other text that does not belong to the main thread

Each line of input is prefixed with a textbox ID in the format `id | Text content`. The output must be in the format `id | label`.

Note that body text may be split across multiple textboxes, and textboxes may start or end in the middle of a sentence or word. Because the text was extracted from a web page or document, paragraphs of body text may be interrupted by multiple textboxes of redundant headers, footnotes, page numbers, tables, images, etc. For example, a real heading will never interrupt the middle of a sentence. Use context clues like this to carefully label each textbox.

EXAMPLE INPUT:
1pjs | Neuroscience Applied 1 (2022) 101012
o2kr | Contents lists available at ScienceDirect
v6sk | Neuroscience Applied
1wj6 | journal homepage: www.journals.elsevier.com/neuroscience-applied
936l | Research Articles
06yq | Microdosing with psychedelics to self-medicate for ADHD symptoms in adults: A naturalistic study
dpv6 | ARTICLE INFO
8r5j | ABSTRACT
2taz | Keywords: ADHD Microdosing Psychedelics Self-medication Well-being Time perception
o3ya | ADHD in adulthood is often overlooked, which negatively affects the individual's well-being. First-line phar- macological interventions are effective in many ADHD patients, relieving symptoms rapidly. However, there seems to be a proportion of individuals who discontinue, or fail to respond to these treatments. For these in- dividuals, alternative treatment options should be explored.
r5il | 1. Introduction
5gui | Attention deficit hyperactivity disorder (ADHD) is one of the most common developmental disorders worldwide. Prevalence research indicates that 2.6% of the adult population has persistent ADHD. ADHD in adults is often overlooked because of the high comorbidity rate and lack of knowledge about how ADHD is expressed in adulthood (Kooij et al.,
9d4o | Fig. 1. Flowchart of included participants for each time point.
82qc | 2019). In addition, ADHD is associated with deficits in various domains of cogni- tive functioning. Twenty-five percent of ADHD cases suffered from ADHD symptoms purely because of de- ficiencies in temporal processing (Sonuga-Barke et al., 2010).
ls7d | First-line ADHD treatments in adults mainly include pharmacological interventions to enhance dopaminergic and noradrenergic neurotrans- mission with stimulants. Overall, they have been proven to work effectively in adults with ADHD, inducing fast symptom relief and thereby enhancing the person's quality of life. In the
5esx | Table 1 Demographic information from respondents at baseline and the two and four- week time points.
t8vv | longer term, approximately twenty percent of ADHD patients discontinue their prescribed medication after six to nine months, thirty percent after one year, and half of them after two years.
xrd1 | 2. Material and methods
5746 | 2.1. Study design and participants
7toz | The study employed a naturalistic design, assessing the experiences of participants at baseline,
j346 | Neuroscience Applied 1 (2022)
m5ka | E.C.H.M. Haijen et al.
kx2c | REVIEW
v8uq | before they start MD on their own initiative, and at two and four weeks after MD initiation. The target population included adults diagnosed with ADHD and individuals who experienced ADHD symptoms to the extent that these interfered with their daily lives and who had not been diag- nosed with ADHD before. To be included in the analyses, participants needed to score above a cut-off point on at least one of the subscales of the Conner's Adult ADHD Rating Scale (CAARS-S:SV).
2zmw | Fig. 2. Mean scores of the CAARS-S:SV DSM-IV total symptoms T-scores at baseline (0W) and two (2W) and four weeks (4W) after MD (A) of the whole sample, and (B) per conventional ADHD medication use. Error bars represent mean error.
sdk3 | <LATEX>\lim _ { x ightarrow \infty } rac { 6 x ^ { 2 } + 1 0 0 } { 7 x ^ { 2 } - 1 0 } =</LATEX>
x972 | 2.2. Study procedure
k221 | Mean performance measures of the CAARS-

EXAMPLE OUTPUT:
1pjs | headerOrFooter
o2kr | publisher
v6sk | publisher
1wj6 | publisher
936l | headerOrFooter
06yq | title
dpv6 | other
8r5j | heading
2taz | keywords
o3ya | body
r5il | heading
5gui | body
9d4o | imageDescription
82qc | body
ls7d | body
5esx | imageDescription
t8vv | body
xrd1 | heading
5746 | subheading
7toz | body
j346 | headerOrFooter
m5ka | authors
kx2c | headerOrFooter
v8uq | body
2zmw | imageDescription
sdk3 | math
x972 | heading
k221 | body




cuin | VIII. GRAPH TRANSFORMER NETWORKS AND TRANSDUCERS
n1i7 | In Section IV, GTN's were introduced as a general- ization of multilayer, multimodule networks where the state information is represented as graphs instead of fixed- size vectors. This section reinterprets the GTN's in the framework of generalized transduction and proposes a powerful graph composition algorithm.
0r5m | A. Previous Work
ne2o | Numerous authors in speech recognition have used gradient-based learning methods that integrate graph- based statistical models (notably HMM's) with acoustic recognition modules, mainly Gaussian mixture models, but also NN's [67], [78], [98], [99]. Similar ideas have been applied to handwriting recognition (see [38] for a review). However, there has been no proposal for a systematic approach to multilayer graph-based trainable systems. The idea of transforming graphs into other graphs has received considerable attention in computer science
sqxn | through the concept of weighted finite-state transducers [86]. Transducers have been applied to speech recognition [100] and language translation [101], and proposals have been made for handwriting recognition [102]. This line of work has been mainly focused on efficient search algorithms [103] and on the algebraic aspects of combining transducers and graphs (called acceptors in this context), but very little effort has been devoted to building globally trainable systems out of transducers. What is proposed in the following sections is a systematic approach to automatic training in graph-manipulating systems. A different approach to graph-based trainable systems, called input-output HMM, was proposed in [104] and [105].
6pmo | B. Standard Transduction
195r | In the established framework of finite-state transducers [86], discrete symbols are attached to arcs in the graphs. Acceptor graphs have a single symbol attached to each arc whereas transducer graphs have two symbols (an input symbol and an output symbol). A special null symbol is absorbed by any other symbol (when concatenating symbols to build a symbol sequence). Weighted transducers and acceptors also have a scalar quantity attached to each arc. In this framework, the composition operation takes as input an acceptor graph and a transducer graph and builds an output acceptor graph. Each path in this output graph (with symbol sequence Sout) corresponds to one path (with symbol sequence Sin) in the input acceptor graph and one path and a corresponding pair of input-output sequences (Sout, Sin) in the transducer graph. The weights on the arcs of the output graph are obtained by adding the weights from the matching arcs in the input acceptor and transducer graphs. In the rest of the paper, we will call this graph composition operation using transducers the (standard) transduction operation.
kqmi | A simple example of transduction is shown in Fig. 28. In this simple example, the input and output symbols on the transducer arcs are always identical. This type of transducer graph is called a grammar graph. To better understand the transduction operation, imagine two tokens sitting each on the start nodes of the input acceptor graph and the transducer graph. The tokens can freely follow any arc labeled with a null input symbol. A token can follow an arc labeled with a nonnull input symbol if the other token also follows an arc labeled with the same input symbol. We have an acceptable trajectory when both tokens reach the end nodes of their graphs (i.e., the tokens have reached the terminal configuration). This trajectory represents a sequence of input symbols that complies with both the acceptor and the transducer. We can then collect the corresponding sequence of output symbols along the trajectory of the transducer token. The above procedure produces a tree, but a simple technique described in Section VIII-C can be used to avoid generating multiple copies of certain subgraphs by detecting when a particular output state has already been seen.
g8rd | The transduction operation can be performed very ef- ficiently [106], but presents complex bookkeeping prob-
nns9 | lems concerning the handling of all combinations of null and nonnull symbols. If the weights are interpreted as probabilities (normalized appropriately) then an acceptor graph represents a probability distribution over the language defined by the set of label sequences associated to all possible paths (from the start to the end node) in the graph.
9uwy | An example of application of the transduction operation is the incorporation of linguistic constraints (a lexicon or a grammar) when recognizing words or other character strings. The recognition transformer produces the recog- nition graph (an acceptor graph) by applying the NN recognizer to each candidate segment. This acceptor graph is composed with a transducer graph for the grammar. The grammar transducer contains a path for each legal sequence of symbol, possibly augmented with penalties to indicate the relative likelihoods of the possible sequences. The arcs contain identical input and output symbols. Another exam- ple of transduction was mentioned in Section V: the path selector used in the HOS training GTN is implementable by a composition. The transducer graph is linear graph which contains the correct label sequence. The composition of the interpretation graph with this linear graph yields the constrained graph.
srly | C. Generalized Transduction
dl6i | If the data structures associated to each arc took only a finite number of values, composing the input graph and an appropriate transducer would be a sound solution. For our applications however, the data structures attached to the arcs of the graphs may be vectors, images or other
89ub | high-dimensional objects that are not readily enumerated. We present a new composition operation that solves this problem.
dhle | Instead of only handling graphs with discrete symbols and penalties on the arcs, we are interested in considering graphs whose arcs may carry complex data structures, including continuous-valued data structures such as vectors and images. Composing such graphs requires additional information.
2nib | 1) When examining a pair of arcs (one from each input graph), we need a criterion to decide whether to create corresponding arc(s) and node(s) in the output graph, based on the information attached to the input arcs. We can decide to build an arc, several arcs, or an entire subgraph with several nodes and arcs.
naev | 2) When that criterion is met, we must build the corre- sponding arc(s) and node(s) in the output graph and compute the information attached to the newly created arc(s) as a function that the information attached to the input arcs.
7h0h | These functions are encapsulated in an object called a composition transformer. An instance of composition transformer implements the following three methods:
a64c | 1) check(arc1, arc2) compares the data struc- tures pointed to by arcs arc1 (from the first graph) and arc2 (from the second graph) and returns a boolean indicating whether corresponding arc(s) should be created in the output graph;
gi7n | 2) fprop (ngraph, upnode, downnode, arc1, arc2) is called when check(arc1,arc2) re- turns true; this method creates new arcs and nodes between nodes upnode and downnode in the out- put graph ngraph, and computes the information attached to these newly created arcs as a function of the attached information of the input arcs arc1 and arc2;
3pn1 | 3) bprop(ngraph, upnode, downnode, arc1, arc2) is called during training in order to prop- agate gradient information from the output subgraph between upnode and downnode into the data struc- tures on the arc1 and arc2, as well as with respect to the parameters that were used in the fprop call with the same arguments; this method assumes that the function used by fprop to compute the values attached to its output arcs is differentiable.
wjuz | The check method can be seen as constructing a dy- namic architecture of functional dependencies, while the fprop method performs a forward propagation through that architecture to compute the numerical information at- tached to the arcs. The bprop method performs a backward propagation through the same architecture to compute the partial derivatives of the loss function with respect to the information attached to the arcs. This is illustrated in Fig. 28.
3yxv | Fig. 29 shows a simplified generalized graph composition algorithm. This simplified algorithm does not handle null transitions, and it does not check whether the tokens
d8zj | Function generalized_composition (PGRAPH graph1,
ywxn | PGRAPH graph2, PTRANS trans)
uxhs | Returns PGRAPH { // Create new graph PGRAPH ngraph = new_graph ()
ed6e | // Create map between token positions // and nodes of the new graph PNODE map [PNODE, PNODE] = new_empty_map() map [endnode (graph1), endnode(graph2)] = endnode (newgraph)
6qqm | // Recursive subroutine for simulating tokens Function simtokens (PNODE node1, PNODE node2) Returns PNODE {
onum | PNODE currentnode = map[node1, node2] // Check if already visited If (currentnode == nil)
3iir | // Record new configuration currentnode = ngraph->create_node () map [node1, node2] = currentnode // Enumerate the possible non-null // joint token transitions For ARC arc1 in down_arcs (node1) For ARC arc2 in down_arcs (node2) If (trans->check(arc1, arc2)) PNODE newnode = simtokens (down_node (arc1) , down_node (arc2)) trans->fprop(ngraph, currentnode, newnode, arc1, arc2) // Return node in composed graph Return currentnode }
7a0a | // Perform token simulation simtokens (startnode(graph1), startnode(graph2)) Delete map Return ngraph }
o1qp | Fig. 29. Pseudocode for a simplified generalized composition algorithm. For simplifying the presentation, we do not handle null transitions nor implement dead end avoidance. The two main components of the composition appear clearly here: 1) the recursive function simtoken ( ) enumerating the token trajectories and 2) the associative array map used for remembering which nodes of the composed graph have been visited.
jrik | trajectory is acceptable (i.e., both tokens simultaneously reach the end nodes of their graphs). The management of null transitions is a straightforward modification of the token simulation function. Before enumerating the possible nonnull joint token transitions, we loop on the possible null transitions of each token, recursively call the token simulation function, and finally call the method fprop. The safest way for identifying acceptable trajectories con- sists of running a preliminary pass for identifying the token configurations from which we can reach the terminal configuration (i.e., both tokens on the end nodes). This
xv78 | is easily achieved by enumerating the trajectories in the opposite direction. We start on the end nodes and follow the arcs upstream. During the main pass, we only build the nodes that allow the tokens to reach the terminal configuration.
sdks | Graph composition using transducers (i.e., standard trans- duction) is easily and efficiently implemented as a gener- alized transduction. The method check simply tests the equality of the input symbols on the two arcs, and the method fprop creates a single arc whose symbol is the output symbol on the transducer's arc.
y8fb | The composition between pairs of graphs is particularly useful for incorporating linguistic constraints in a handwrit- ing recognizer. Examples of its use are given in the online handwriting recognition system described in Section IX (and in the check reading system described in Section X).
521q | In the rest of the paper, the term composition transformer will denote a GT based on the generalized transductions of multiple graphs. The concept of generalized transduc- tion is a very general one. In fact, many of the GT's described earlier in this paper, such as the segmenter and the recognizer, can be formulated in terms of generalized transduction. In this case, the generalized transduction does not take two input graphs but a single input graph. The method fprop of the transformer may create several arcs or even a complete subgraph for each arc of the initial graph. In fact the pair check, fprop itself can be seen as procedurally defining a transducer.
yogo | In addition, it can be shown that the generalized trans- duction of a single graph is theoretically equivalent to the standard composition of this graph with a particular transducer graph. However, implementing the operation this way may be very inefficient since the transducer can be very complicated.
2285 | In practice, the graph produced by a generalized transduc- tion is represented procedurally in order to avoid building the whole output graph (which may be huge when for example the interpretation graph is composed with the grammar graph). We only instantiate the nodes which are visited by the search algorithm during recognition (e.g., Viterbi). This strategy propagates the benefits of pruning algorithms (e.g., beam search) in all the GTN's.
ys0g | D. Notes on the Graph Structures
7zvp | Section VI discussed the idea of global training by back- propagating gradient through simple GT's. The bprop method is the basis of the back-propagation algorithm for generic GT's. A generalized composition transformer can be seen as dynamically establishing functional relation- ships between the numerical quantities on the input and output arcs. Once the check function has decided that a relationship should be established, the fprop function im- plements the numerical relationship. The check function establishes the structure of the ephemeral network inside the composition transformer.
ww6u | Since fprop is assumed to be differentiable, gradients can be back propagated through that structure. Most param- eters affect the scores stored on the arcs of the successive
hpi6 | graphs of the system. A few threshold parameters may determine whether an arc appears or not in the graph. Since nonexisting arcs are equivalent to arcs with very large penalties, we only consider the case of parameters affecting the penalties.
4uhb | In the kind of systems we have discussed until now (and the application described in Section X), much of the knowledge about the structure of the graph that is produced by a GT is determined by the nature of the GT, but it may also depend on the value of the parameters and on the input. It may also be interesting to consider GT modules which attempt to learn the structure of the output graph. This might be considered a combinatorial problem and not amenable to gradient-based learning, but a solution to this problem is to generate a large graph that contains the graph candidates as subgraphs, and then select the appropriate subgraph.
epyi | E. GTN and HMM's
y51p | GTN's can be seen as a generalization and an extension of HMM's. On the one hand, the probabilistic interpretation can be either kept (with penalties being log-probabilities), pushed to the final decision stage (with the difference of the constrained forward penalty and the unconstrained for- ward penalty being interpreted as negative log-probabilities of label sequences), or dropped altogether (the network just represents a decision surface for label sequences in input space). On the other hand, GTN's extend HMM's by allowing to combine in a well-principled framework multiple levels of processing, or multiple models (e.g., Pereira et al. have been using the transducer framework for stacking HMM's representing different levels of processing in automatic speech recognition [86]).
x95m | Unfolding an HMM in time yields a graph that is very similar to our interpretation graph (at the final stage of processing of the GTN, before Viterbi recognition). It has nodes n(t, i) associated to each time step t and state i in the model. The penalty Ci for an arc from n(t - 1, j) to n(t, i) then corresponds to the negative log-probability of emitting observed data ot at position t and going from state j to state i in the time interval (t- 1,t). With this probabilistic interpretation, the forward penalty is the negative logarithm of the likelihood of whole observed data sequence (given the model).