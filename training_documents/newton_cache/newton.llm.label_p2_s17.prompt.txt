You will receive raw text from an OCR scan of a document or web page. Each line of input represents one textbox from the document.

Your task is to label each textbox with exactly one of the following labels:
  title: the primary document title, no more than one per document
  heading: the name of the following chapter or section (should never start in the middle of the previous paragraph)
  subheading: a secondary heading, often following another heading (should never start in the middle of the previous paragraph)
  body: main paragraph content: may start or end in the middle of a word, use context to follow paragraphs that are split across textboxes (if a textbox contains both heading and body text, label it "body")
  math: textbox primarily containing math
  imageDescription: standalone callout that only describes an image, chart, table, or similar (typically begins with "Table...", "Figure...", "Fig. ...", "Above...", "Left...", etc.)
  authors: names of contributors to this document
  institutions: list of universities, business, and other institutions affiliated with this document
  publisher: info about the publisher, or provided by the publisher, including copyright info
  pagenum: the page number
  headerOrFooter: boilerplate text page number, or redundant headings at the top/bottom of the page
  toc: the table of contents
  references: bibliographic references and resources
  acknowledgements: thanks and credits to contributors
  appendix: index, appendix, or any other end matter not part of the main thread (including related headings)
  table: tabular data, label, title, or subheading for a grid or list of data
  datum: numeric data or data label from a chart or list
  advertisement: promotional ads and marketing
  layout: non-content related to the layout of the page, e.g. sidebar navigation
  callout: pull quote or long block text that stands apart from the main text
  footnote: footnotes and endnotes near the bottom of the page
  keywords: list of keywords or tags
  url: web address or email
  other: any other text that does not belong to the main thread

Each line of input is prefixed with a textbox ID in the format `id | Text content`. The output must be in the format `id | label`.

Note that body text may be split across multiple textboxes, and textboxes may start or end in the middle of a sentence or word. Because the text was extracted from a web page or document, paragraphs of body text may be interrupted by multiple textboxes of redundant headers, footnotes, page numbers, tables, images, etc. For example, a real heading will never interrupt the middle of a sentence. Use context clues like this to carefully label each textbox.

EXAMPLE INPUT:
1pjs | Neuroscience Applied 1 (2022) 101012
o2kr | Contents lists available at ScienceDirect
v6sk | Neuroscience Applied
1wj6 | journal homepage: www.journals.elsevier.com/neuroscience-applied
936l | Research Articles
06yq | Microdosing with psychedelics to self-medicate for ADHD symptoms in adults: A naturalistic study
dpv6 | ARTICLE INFO
8r5j | ABSTRACT
2taz | Keywords: ADHD Microdosing Psychedelics Self-medication Well-being Time perception
o3ya | ADHD in adulthood is often overlooked, which negatively affects the individual's well-being. First-line phar- macological interventions are effective in many ADHD patients, relieving symptoms rapidly. However, there seems to be a proportion of individuals who discontinue, or fail to respond to these treatments. For these in- dividuals, alternative treatment options should be explored.
r5il | 1. Introduction
5gui | Attention deficit hyperactivity disorder (ADHD) is one of the most common developmental disorders worldwide. Prevalence research indicates that 2.6% of the adult population has persistent ADHD. ADHD in adults is often overlooked because of the high comorbidity rate and lack of knowledge about how ADHD is expressed in adulthood (Kooij et al.,
9d4o | Fig. 1. Flowchart of included participants for each time point.
82qc | 2019). In addition, ADHD is associated with deficits in various domains of cogni- tive functioning. Twenty-five percent of ADHD cases suffered from ADHD symptoms purely because of de- ficiencies in temporal processing (Sonuga-Barke et al., 2010).
ls7d | First-line ADHD treatments in adults mainly include pharmacological interventions to enhance dopaminergic and noradrenergic neurotrans- mission with stimulants. Overall, they have been proven to work effectively in adults with ADHD, inducing fast symptom relief and thereby enhancing the person's quality of life. In the
5esx | Table 1 Demographic information from respondents at baseline and the two and four- week time points.
t8vv | longer term, approximately twenty percent of ADHD patients discontinue their prescribed medication after six to nine months, thirty percent after one year, and half of them after two years.
xrd1 | 2. Material and methods
5746 | 2.1. Study design and participants
7toz | The study employed a naturalistic design, assessing the experiences of participants at baseline,
j346 | Neuroscience Applied 1 (2022)
m5ka | E.C.H.M. Haijen et al.
kx2c | REVIEW
v8uq | before they start MD on their own initiative, and at two and four weeks after MD initiation. The target population included adults diagnosed with ADHD and individuals who experienced ADHD symptoms to the extent that these interfered with their daily lives and who had not been diag- nosed with ADHD before. To be included in the analyses, participants needed to score above a cut-off point on at least one of the subscales of the Conner's Adult ADHD Rating Scale (CAARS-S:SV).
2zmw | Fig. 2. Mean scores of the CAARS-S:SV DSM-IV total symptoms T-scores at baseline (0W) and two (2W) and four weeks (4W) after MD (A) of the whole sample, and (B) per conventional ADHD medication use. Error bars represent mean error.
sdk3 | <LATEX>\lim _ { x ightarrow \infty } rac { 6 x ^ { 2 } + 1 0 0 } { 7 x ^ { 2 } - 1 0 } =</LATEX>
x972 | 2.2. Study procedure
k221 | Mean performance measures of the CAARS-

EXAMPLE OUTPUT:
1pjs | headerOrFooter
o2kr | publisher
v6sk | publisher
1wj6 | publisher
936l | headerOrFooter
06yq | title
dpv6 | other
8r5j | heading
2taz | keywords
o3ya | body
r5il | heading
5gui | body
9d4o | imageDescription
82qc | body
ls7d | body
5esx | imageDescription
t8vv | body
xrd1 | heading
5746 | subheading
7toz | body
j346 | headerOrFooter
m5ka | authors
kx2c | headerOrFooter
v8uq | body
2zmw | imageDescription
sdk3 | math
x972 | heading
k221 | body




on9b | Second-order Methods. By incorporating Hessian infor- mation, second-order methods hold the promise to alleviate many of the well-known deficiencies of first-order alterna- tives, e.g., they are typically better suited to ill-conditioned problems (Xu et al., 2020b). For constrained problems, generic projected (quasi) Newton methods involve iterations of the form <LATEX>\mathrm { x } _ { k + 1 } = \mathrm { x } _ { k } + \alpha _ { k } \mathrm { p } _ { k }</LATEX> where
a8ge | <LATEX>\mathrm { p } _ { k } = \arg \min _ { \mathrm { x } \in \Omega } \langle \mathrm { g } _ { k } , \mathrm { p } \rangle + \langle \mathrm { p } , \mathrm { B } _ { k } \mathrm { p } \rangle / 2 ,</LATEX> (3)
h27a | where <LATEX>\alpha _ { k }</LATEX> is an appropriately chosen step-size, e.g., back- tracking line search, and <LATEX>\mathrm { B } _ { k }</LATEX> captures some curvature infor- mation of <LATEX>f</LATEX> at <LATEX>\mathrm { x } _ { k }</LATEX> (and also potentially the step-length as in the proximal arc search). For <LATEX>\mathrm { B } _ { k } = \mathrm { I }</LATEX> we recover a projected gradient variant, whereas for <LATEX>\mathrm { B } _ { k } = \mathrm { H } _ { k } ,</LATEX> or some approxi- mation, we obtain projected (or more generally proximal) Newton-type methods (Schmidt et al., 2009; 2011; Becker & Fadili, 2012; Lee et al., 2014; Shi & Liu, 2015). The main drawback of this framework is that the subproblem, (3), may no longer be a simple projection even when <LATEX>\Omega</LATEX> is a simple, and one has to resort to an optimisation subroutine to (approximately) solve (3).
pdjh | An alternative is the interior point framework (Nocedal & Wright, 2006), where the constraints are directly integrated into the objective as "barrier" functions. While the subprob- lems in this framework amount to solving linear systems, to produce accurate solutions the barrier function must ap-
xmgs | Inexact Newton-type Methods for Optimisation with Nonnegativity Constraints
bd9h | proach the constraint, which can lead to highly ill condi- tioned subproblems. Some recent works (Bian et al., 2014; Haeser et al., 2017; O'Neill & Wright, 2020) consider in- terior point methods for (1). In particular, in (O'Neill & Wright, 2020), capped Newton-CG with a preconditioned Hessian is used to optimise a log barrier augmented objec- tive. Due to issues arising from increasingly ill-conditioned subproblems, the practical efficacy of this method seems to be inferior when compared to projection-based methods, including those of first-order (Xie & Wright, 2023).
m047 | The issue with the general purpose second-order methods discussed so far is that, unlike projected gradient, they do not leverage the simplicity of the nonnegativity con- straints and the corresponding projection. In this light, a na√Øve adaptation of the projected gradient would imply di- rectly projecting the Newton step on the constraints, e.g., <LATEX>\mathrm { x } _ { k + 1 } = \mathcal{P} _ { \Omega } \left( \mathrm { x } _ { k } - \alpha _ { k } \mathrm { H } _ { k } ^ { - 1 } \mathrm { g } _ { k } \right) .</LATEX> Unfortunately, such a direct adaptation may lead to ascent directions for the objective function at the boundary. To that end, the two-metric projec- tion (TMP) framework (Bertsekas, 1982; Gafni & Bertsekas, 1984) offers an ingenious solution. Specifically, at each iteration, the component indices, <LATEX>\left\lceil d \right\rceil ,</LATEX> are divided into the approximately bound, <LATEX>\mathcal{J} _ { k } ^ { + } ,</LATEX> and free sets, <LATEX>\mathcal{J} _ { k } ^ { - }</LATEX> , given by
gy8c | (4)
y4mj | <LATEX>\mathcal{J} _ { k } ^ { + } = \left\{ i \in \left[ d \right] \mid \mathrm { x } _ { k } ^ { i } \leq \delta , \mathrm { g } _ { k } ^ { i } > 0 \right\} , \mathcal{J} _ { k } ^ { - } = \left[ d \right] \setminus \mathcal{J} _ { k } ^ { + } .</LATEX> where <LATEX>\delta > 0 .</LATEX> A matrix, <LATEX>\mathrm { D } _ { k } ,</LATEX> is then chosen to be "diagonal" with respect to set <LATEX>\mathcal{J} _ { k } ^ { + }</LATEX> , that is,
vpgq | <LATEX>\left( \mathrm { D } _ { k } \right) _ { i j } = 0 , i \in \mathcal{J} _ { k } ^ { + } , j \in \left[ d \right] \setminus \left\{ i \right\} ,</LATEX> and the update is simply given by
ousl | <LATEX>\mathrm { x } _ { k + 1 } = \mathcal{P} \left( \mathrm { x } _ { k } - \alpha _ { k } \mathrm { D } _ { k } \mathrm { g } _ { k } \right) .</LATEX> (5)
m4ju | It has been shown that TMP is asymptotically convergent under certain conditions and reasonable choices of <LATEX>\mathrm { D } _ { k } .</LATEX> For example, for strongly convex problems, the non-diagonal portion of <LATEX>\mathrm { D } _ { k }</LATEX> can consist of the inverse of the Hessian sub- matrix corresponding to the indices in <LATEX>\mathcal{J} _ { k } ^ { - }</LATEX> . In this case, (5) reduces to a scaled gradient in <LATEX>\mathcal{J} _ { k } ^ { + }</LATEX> and a Newton step in <LATEX>\mathcal{J} _ { k } ^ { - }</LATEX> . Bertsekas (1982) also shows that, under certain conditions, TMP can preserve fast "Newton like" local convergence. Practically, TMP type algorithms has been successfully ap- plied to a range of problems (Gafni & Bertsekas, 1984; Schmidt et al., 2007; Kim et al., 2010; Haber, 2014; Kuang et al., 2015; Cai et al., 2023). In large scale and nonconvex settings, employing the Newton step as part of (5) may be infeasible or even undesirable. Indeed, not only can Hessian storage and inversion costs be prohibitive, the existence of negative curvature can lead to ascent directions.
a0mb | With a view to eliminate the necessity of forming and invert- ing the Hessian, Kim et al. (2010) extend TMP to utilise a quasi-Newton update with asymptotic convergence guaran- tees in the convex setting. Also in this vein, Xie & Wright
ki8m | (2023) considered "projected Newton-CG", which entails a combination of the projected gradient and the inexact New- ton steps that preserve the simplicity of projection onto <LATEX>\mathbb{R} _ { + } ^ { d } .</LATEX> In particular, Newton-CG steps are based on the capped CG procedure of Royer et al. (2018). Unfortunately, the gradient and Newton-CG steps are not taken simultaneously. Instead, the algorithm employs projected gradient steps across all components until optimality is attained in the approximately active set. Only at that point is the Newton-CG step applied in the approximately inactive set. This implies that the algo- rithm may take projected gradient steps at most iterations, potentially impeding its practical performance.
km8w | Hessian-free Inexact Methods. In high-dimensional set- tings, storing the Hessian matrix may be impractical. More- over, an approximate direction can often be computed at a fraction of the cost of a full Newton step. In this con- text, Hessian-free inexact Newton-type algorithms leverage Krylov subspace methods (Saad, 2003), which are partic- ularly well-suited for these scenarios. Krylov subspace solvers can recover a reasonable approximate direction in just a few iterations and only require access to the Hessian- vector product mapping, <LATEX>\mathrm { v } \mapsto \mathrm { H } \left( \mathrm { x } \right) \mathrm { v } .</LATEX> The computational cost of a Hessian-vector product is comparable to that of a gradient evaluation and does not require the explicit forma- tion of H. Indeed, <LATEX>\mathrm { H } \left( \mathrm { x } \right) \mathrm { v }</LATEX> can be computed by obtaining the gradient of the map <LATEX>\mathrm { x } \mapsto \langle \mathrm { g } \left( \mathrm { x } \right) , \mathrm { v } \rangle</LATEX> using automatic differentiation, leading to one additional back propagation compared to computing
wzno | <LATEX>\mathrm { g } \left( \mathrm { x } \right) .</LATEX> Complexity in Optimisation. Recently, there has been a growing interest in obtaining global worst case iteration complexity guarantees for optimisation methods, namely a bound on the number of iterates required for the algo- rithm to compute an approximate solution. For instance, in unconstrained and nonconvex settings, gradient descent produces an approximate first-order optimal point satisfying <LATEX>\| \mathrm { g } \left( \mathrm { x } \right) \| \leq \epsilon _ { g }</LATEX> in at most <LATEX>\mathcal{O} \left( \epsilon _ { g } ^ { - 2 } \right)</LATEX> iterations for objectives with Lipschitz continuous gradients (Nesterov, 2004). This rate has been shown to be tight (Cartis et al., 2010). Without additional assumptions, similar rates have also been shown for second-order methods (Cartis et al., 2022). However, for objectives with both Lipschitz continuous gradient and Hes- sian, this rate can be improved to <LATEX>\mathcal{O} \left( \epsilon _ { g } ^ { - 3 / 2 } \right) ,</LATEX> which is also shown to be tight over a wide class of second-order algo- rithms (Cartis et al., 2011b). Second-order methods which achieve this rate include cubic regularised Newton's method and its adaptive variants (Nesterov & Polyak, 2006; Cartis et al., 2011c;a; Xu et al., 2020a), modified trust region based methods (Curtis et al., 2016; 2021; Curtis & Wang, 2023) and line search methods including Newton-CG (Royer et al., 2018) and Newton-MR (Liu & Roosta, 2022b) as well as their inexact variants (Yao et al., 2022; Lim & Roosta, 2023). Many of the above works also provide explicit bounds on the operational complexity, that is, a bound on the number
xagm | Inexact Newton-type Methods for Optimisation with Nonnegativity Constraints
iwn5 | of fundamental computational units (e.g. gradient evalua- tions, Hessian vector products) to obtain an approximate solution.
uooz | In the constrained setting, direct comparison between bounds is difficult due to differences in approximate op- timality conditions; see discussion in Xie & Wright (2023, Section 3) for the bound constraint case. However, the al- gorithms in Cartis et al. (2020); Birgin & Mart√≠nez (2018) achieve O(Eg 3/2) for a first-order optimal point with certain types of constraints, which is shown to be tight in Cartis et al. (2020). More specific to the bound constraint case, the Newton-CG log barrier method of O'Neill & Wright (2020) achieves a complexity of O(deg I-1/2 2 + ‚Ç¨g 3/2), while the projected Newton-CG algorithm of Xie & Wright (2023) obtains a rate of O(Eg ) under a set of approximate opti- mality conditions similar to this work.
pxbq | Optimality Conditions. Recall that x satisfies the first- order necessary conditions for (1) if
jfsr | x¬• ‚â• 0, and [Vf(x*)]i =0, if xi > 0, [f(x)] >0, if x4 = 0.
u15r | (6)
lq35 | We seek a point which satisfies these conditions to some "e" tolerance. There are a number of ways to adapt (6) into an approximate condition (Xie & Wright, 2023, Section 3). In this work we adopt Xie & Wright (2023, Definition 1).
3g9w | Definition 2.1 (‚Ç¨-Optimal Point). A point, x, is called ‚Ç¨- approximate first-order optimal (‚Ç¨-FO) if
d09o | gi ‚â• -VE, Vi E A(x, VE) (7a)
ylhz | |diag(xA)g4|| ‚â§ ‚Ç¨, (7b)
a3td | lgI| SE. (7c)
ex5k | We take (7a) and (7b) to be trivially satisfied if A(x, Ve) = √ò and similar for (7c) if I(x, VE) = √ò.
9d4o | This definition has been shown to be asymptotically exact. Lemma 2.2. (Xie & Wright, 2023, Lemma 1) Suppose that Ek 4 0 and we have a sequence {Xk }&-1 where each Xk satisfies the corresponding Ex-FO optimality condition. If Xk -> X* then x, satisfies (6).
cjq2 | 3. Newton-MR Two-Metric Projection
5esx | We now propose and theoretically study our extensions of the TMP framework, which involves simultaneously em- ploying gradient and inexact Newton steps, which are, re- spectively, restricted to the active and inactive sets.
m5ka | 3.1. MINRES and Its Properties
byu8 | The inexact Newton step is based on the recently proposed Newton-MR framework (Liu & Roosta, 2022b; Roosta et al.,