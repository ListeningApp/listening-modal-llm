{
  "id": "chatcmpl-A0YxUnjR6zXoQpFcwFVevYIUOcnRw",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "```\n0fpw | Perhaps the most obvious objection to the claim that LLMs deserve the attribution of rich predicates like meaning and representing is that such systems lack the kind of connections to the external world which seem necessary to ground semantic content. As Piantadosi and Hill note, there is a \"prominent view ... that models trained only on text cannot acquire realistic meanings because they lack reference, or connection to objects in the real world\". The assumption here is that words get their meaning from their connection to things outside the sphere of language, so that, e.g. \"dog\" means what it does because it refers to dogs or picks out the property of doghood. While sentences and utterances are meaningful because they make claims about the world, so that their meaning can be analysed in terms of their conditions of truth. As David Lewis famously put it, it seems that: \"semantics with no treatment of truth conditions is not semantics\". Yet, if all a system has available to it is intra-linguistic information, we might suspect that truth won't enter the picture, since the system is out of touch with the external states of affairs where (according to most theories) truth gets its footing.\ndqn8 | The worry that externally determined properties like truth don't get any traction within LLMs is reinforced by the well-known issue of LLM fabrication, or \"hallucination\":\ncop0 | A key limitation of LLMs is their tendency to generate errors without warning, including mathematical, programming, attribution, and higher-level conceptual errors. Such errors are often referred to as hallucinations per their tendency to appear as reasonable or aligned with truthful inferences. Hallucinations, such as erroneous references, content, and statements, may be intertwined with correct information, and presented in a persuasive and confident manner, making their identification difficult without close inspection and effortful fact-checking.\nlhkq | The existence of such fabrications undermines the idea that users can rest epistemic weight on LLM outputs, since falsehoods emerge unbidden from exactly the same background processes as give rise to statements of fact. As ChatGPT's developers, OpenAI, note, fixing this problem is far from straightforward:\n82ju | ChatGPT sometimes writes plausible-sounding but incorrect or nonsensical answers. Fixing this issue is challenging, as: (one) during RL training, there is currently no source of truth; (two) training the model to be more cautious causes it to decline questions that it can answer correctly; and (three) supervised training misleads the model because the ideal answer depends on what the model knows, rather than what the human demonstrator knows.\ny2qo | The Sceptic's argument is thus that a system needs robust externalist connections prior to any attribution of rich semantic predicates (like representation and meaning) and that since LLMs lack such connections they cannot be treated as semantic engines.\n1piq | In response, however, I think there are (at least) three kinds of response a Believer might make:\n7fqw | One. reject the claim that externalist connections are needed (or at least maintain that referential properties are only part of the story).\nrhaz | Two. accept the claim that externalist connections are needed but argue that future iterations of LLMs will have the required connections.\nwak1 | Three. accept the claim that externalist connections are needed, but argue that, as things stand, LLMs already realise appropriate word-world relations through their connection to humans.\nwtfz | I'll take these responses in order but focus on three, since I think it provides the most promising response to the Sceptic.\nqy1i | Two point one Distributional, Inferential, and Conceptual Role Semantics\n1k9e | A first move would be to reject the call for an externalist semantics in favour of some form of internalist approach. According to internalist semantic theories, words and sentences are made meaningful not through their connection to external objects and situations, but through their position in a linguistic or conceptual network. On this model, reference and truth emerge only later, at the point where already meaningful expressions are used to talk about the world. The properties internalists appeal to include distributional properties, entailment relations and\n5bjs | position within a conceptual network. The first idea - that the distributional properties of expressions (i.e. the statistical patterns of word occurrences) suffice for meaning - has a long history in philosophy and linguistics, with the linguist John Rupert Frith, writing in the nineteen fifties, capturing the approach in the slogan: \"You shall know a word by the company it keeps\". That is to say, 'distributional semantics' holds that the meaning of a word is fixed by the pattern of words it commonly co-occurs with, so that, e.g., \"dog\" means what it does because it commonly appears with words like \"barks\", \"bites\", \"bones\", etc, but less often (or not at all) with words like \"justice\", \"flies\", \"not\", etc. Distributional semantics is clearly a pre-cursor of the approach to natural language processing adopted by LLMs (and by corpus linguistics in general), thus an advocate of Distributional semantics is likely to simply deny the claim that LLMs do not directly target semantic properties. If meaning is given by distributional properties and LLMs model distribution, then LLMs just are modelling semantic content. One response to the Sceptic, then, would be to embrace Distributional semantics.\nuodj | LLMs may also qualify as modelling semantic content on alternative internalist approaches. For instance, Inferential Role semantics holds that meaning is grounded in the entailment relations that exist between sentences: to master \"cow\" is (to be disposed) to infer from \"x is a cow\" to \"x is an animal\", \"x has four legs\", \"x gives milk\", etc. As noted, it seems that LLMs are highly adept at capturing these kinds of inferential relations. Or again, Conceptual Role semantics couches the required internalist properties at the level of thought, focusing on the relationships between concepts which presumably underpin acceptable inferences at the level of language. Given LLMs success with inferential and conceptual network tasks, it could be argued that information about these relations is already tacitly contained within the system. Piantadosi and Hill seem to embrace this kind of view. They suggest that \"relationships between concepts are the essential, defining, aspects of meaning\" and they go on to argue that:\nk16v | The key question for LLMs is whether training to predict text could actually support discovery of conceptual roles. To us the question is empirical, and we believe has been answered in a promising, partial affirmative by studies showing success on tasks that require knowledge of relationships between concepts. Text provides such clues to conceptual role because human conceptual roles generated the text.\nb2lm | Deciding whether LLMs count as semantic engines given either an Inferential or a Conceptual Role approach requires taking a stance on what it is for information to be\n```",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1724697744,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_df84d6bd70",
  "usage": {
    "completion_tokens": 1460,
    "prompt_tokens": 3277,
    "total_tokens": 4737
  }
}