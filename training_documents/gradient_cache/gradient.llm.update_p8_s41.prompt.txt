You will receive raw text from an OCR scan of a document or web page. Each line of input represents one textbox from the document. Your task is to faithfully reproduce the text for our text-to-speech engine so that it is easily read aloud.

Note that the text might not start at the beginning and paragraphs may be split acrosss multiple textboxes. Textboxes may start or end in the middle of a word which is acceptable: do not combine text between textboxes and leave broken words at the start and end of a textbox unchanged.

You will receive one textbox per-line in the format `id | Textbox content`. The output must be in the format `id | Clean content`.

RULES:
 • Remove inline citations and references e.g. "(Author, 2021)" or "(https://wikipedia.org/article.html)" or "[ECMOS 35b, 47, 49]".
 • Spell out numbers, dates, and chemical formulas for the TTS engine e.g. "-2.5" -> "negative two point five" or "Jan. 2020" -> "January twenty twenty".
 • Spell out LaTeX formulas for the TTS engine e.g. "<LATEX>p = 1</LATEX>" -> "P equals one".
 • Rejoin hyphenated word fragments within the same textbox e.g. "synth -esize" -> "synthesize".
 • Fix simple typos and OCR errors e.g. "O nce upon a 1ime." -> "Once upon a time.".
 • Delete nonsequiter textboxes that interrupt the middle of a sentence if they obviously do not belong.

Otherwise, the output should exactly match the input text:
 • Do not combine text across lines or attempt to fix words broken across lines.
 • Ensure each ID from the input is included in the output.
 • Make as few changes as possible while respecting the above rules.

EXAMPLE INPUT:
ccpt | stigate the effectiveness of psychedelics for ADHD symp- toms in adults.
0fpw | 2. Materia1 and methods
dqn8 | 2.1. Study design and participants
cop0 | The study employed a naturalistic design, assessing the experiences of participants at bas-,
lhkq | eline before they start, and at 2 and 4 weeks after initiation. The target population included adults with ADHD symptoms who had not been diag- nosed with ADHD before. To be included, participants needed to score on Conner's Adult ADHD Rating Scale (CAARS-S:SV). This cut-off was indicative of clinically elevated symptoms (Conners et al., 1999) (see section 2.3).
82ju | 2.2. Study procedure
y2qo | The online advertisement was placed on a website providing information about psychedelics (www.microdo sing.nl). Interested participants were redirected to information explaining the study rationale and procedure. The baseline survey took 20 min to complete. If the survey had not been completed after 24 h, a reminder was sent. Each of the surveys at the 2- and 4-week time points took about 15 min to complete. Par- ticipants were able to pause the surveys. Data collection occurred between Nov. 2020 and Jul. 2021. The study was approved by the Ethics Review Committee of Psy- chology and Neuroscience at Maastricht University (ERCPN- 215_05_11_2019_A1).
1piq | 2.3. Measures
7fqw | 2.3.1. Demographic information and history of substance use
rhaz | At baseline, demographic information was collected. History of substance use assessed experience with psyche- delics (i.e., ayahuasca, DMT, 5-MeO-DMT, LSD, novel lysergamides (e.g., 1P-LSD, ALD-52), psilocybin, salvia divinorum, ibogaine, and mescaline) in both full (psychedelic) doses and microdoses.
wak1 | 2.3.2. Psychiatric and physiological diagnoses
wtfz | Participants were asked whether they had a current diagnosis of a disorder. These answer options were chosen because most of the listed diagnoses are often reported to co-occur with ADHD (Kooij et al., 2019), or because these diagnoses were reported to be common in people who microdose (Fadiman and Korb, 2019). We 
3j2l | REVIEW ARTICLE Frontiers in Psychiatry | www.frontiersin.org
03k3 | 2
1k9e | constructed a variable Comorbidity alongside ADHD, differenti- ating respondents with and without a comorbid diagnosis alongside ADHD (<LATEX>p < . 0 0 1 =</LATEX> only ADHD or no ADHD diagnosis; <LATEX>p > . 0 1 =</LATEX> ADHD and at least one other diagnosis). We constructed a variable Medication use alongside microdosing, differentiating respondents who were and were not using conven- tiona1 ADHD medication alongside microdosing (<LATEX>0 =</LATEX> only microdosing; <LATEX>1 =</LATEX> conven-

EXAMPLE OUTPUT:
ccpt | stigate the effectiveness of psychedelics for ADHD symptoms in adults.
0fpw | Two. Material and methods
dqn8 | Two point one. Study design and participants
cop0 | The study employed a naturalistic design, assessing the experiences of participants at bas-,
lhkq | eline before they start, and at two and four weeks after initiation. The target population included adults with ADHD symptoms who had not been diagnosed with ADHD before. To be included, participants needed to score on Conner's Adult ADHD Rating Scale. This cut-off was indicative of clinically elevated symptoms (see section two point three).
82ju | Two point two. Study procedure
y2qo | The online advertisement was placed on a website providing information about psychedelics. Interested participants were redirected to information explaining the study rationale and procedure. The baseline survey took twenty minutes to complete. If the survey had not been completed after twenty-four hours, a reminder was sent. Each of the surveys at the two and four-week time points took about fifteen minutes to complete. Participants were able to pause the surveys. Data collection occurred between November twenty twenty and July twenty twenty-one. The study was approved by the Ethics Review Committee of Psychology and Neuroscience at Maastricht University.
1piq | Two point three. Measures
7fqw | Two point three point one. Demographic information and history of substance use
rhaz | At baseline, demographic information was collected. History of substance use assessed experience with psychedelics (i.e., ayahuasca, DMT, five-MeO-DMT, LSD, novel lysergamides (e.g., one-P-LSD, ALD-fifty-two), psilocybin, salvia divinorum, ibogaine, and mescaline) in both full (psychedelic) doses and microdoses.
wak1 | Two point three point two. Psychiatric and physiological diagnoses
wtfz | Participants were asked whether they had a current diagnosis of a disorder. These answer options were chosen because most of the listed diagnoses are often reported to co-occur with ADHD, or because these diagnoses were reported to be common in people who microdose. We
3j2l | REVIEW ARTICLE Frontiers in Psychiatry: www.frontiersin.org
03k3 | Two
1k9e | constructed a variable Comorbidity alongside ADHD, differentiating respondents with and without a comorbid diagnosis alongside ADHD (P is less than point zero zero one equals only ADHD or no ADHD diagnosis; P is greater than point zero one equals ADHD and at least one other diagnosis). We constructed a variable Medication use alongside microdosing, differentiating respondents who were and were not using conventional ADHD medication alongside microdosing (zero equals only microdosing; one equals conven-




8sqd | Layer F6 contains 84 units (the reason for this number comes from the design of the output layer, explained below) and is fully connected to C5. It has 10 164 trainable parameters.
vlyz | As in classical NN's, units in layers up to F6 compute a dot product between their input vector and their weight vector, to which a bias is added. This weighted sum, denoted aj for unit i, is then passed through a sigmoid
5034 | squashing function to produce the state of unit i, denoted by xi
9lvu | Xi = f(ai). (5)
uy7a | The squashing function is a scaled hyperbolic tangent
lqs6 | f(a) = Atanh(Sa)
ri4s | (6)
2j8o | where A is the amplitude of the function and S determines its slope at the origin. The function f is odd, with horizontal asymptotes at +A and - A. The constant A is chosen to be 1.7159. The rationale for this choice of a squashing function is given in Appendix A.
qrl1 | Finally, the output layer is composed of Euclidean RBF units, one for each class, with 84 inputs each. The outputs of each RBF unit y; is computed as follows:
uvvk | yi = (P; - Wij) 2. (7)
mdnj | In other words, each output RBF unit computes the Eu- clidean distance between its input vector and its parameter vector. The further away the input is from the parameter vector, the larger the RBF output. The output of a particular RBF can be interpreted as a penalty term measuring the fit between the input pattern and a model of the class associated with the RBF. In probabilistic terms, the RBF output can be interpreted as the unnormalized negative log-likelihood of a Gaussian distribution in the space of configurations of layer F6. Given an input pattern, the loss function should be designed so as to get the configuration of F6 as close as possible to the parameter vector of the RBF that corresponds to the pattern's desired class. The parameter vectors of these units were chosen by hand and kept fixed (at least initially). The components of those parameters vectors were set to -1 or +1. While they could have been chosen at random with equal probabilities for -1 and +1, or even chosen to form an error correcting code as suggested by [47], they were instead designed to represent a stylized image of the corresponding character class drawn on a 7x12 bitmap (hence the number 84). Such a representation is not particularly useful for recognizing isolated digits, but it is quite useful for recognizing strings of characters taken from the fully printable ASCII set. The rationale is that characters that are similar, and therefore confusable, such as uppercase "O," lowercase "o," and zero, lowercase "I" digit one, and square brackets and uppercase "I," will have similar output codes. This is particularly useful if the system is combined with a linguistic post- processor that can correct such confusions. Because the codes for confusable classes are similar, the output of the corresponding RBF's for an ambiguous character will be similar, and the postprocessor will be able to pick the appropriate interpretation. Fig. 3 gives the output codes for the full ASCII set.
mwy0 | Another reason for using such distributed codes, rather than the more common "1 of N" code (also called place code or grandmother cell code) for the outputs is that nondistributed codes tend to behave badly when the number
hg8q | of classes is larger than a few dozen. The reason is that output units in a nondistributed code must be off most of the time. This is quite difficult to achieve with sigmoid units. Yet another reason is that the classifiers are often used not only to recognize characters, but also to reject noncharacters. RBF's with distributed codes are more appropriate for that purpose because unlike sigmoids, they are activated within a well-circumscribed region of their input space, outside of which nontypical patterns are more likely to fall.
rcyg | The parameter vectors of the RBF's play the role of target vectors for layer F6. It is worth pointing out that the components of those vectors are +1 or -1, which is well within the range of the sigmoid of F6, and therefore prevents those sigmoids from getting saturated. In fact, +1 and -1 are the points of maximum curvature of the sigmoids. This forces the F6 units to operate in their maximally nonlinear range. Saturation of the sigmoids must be avoided because it is known to lead to slow convergence and ill-conditioning of the loss function.
bj8l | C. Loss Function
cygk | The simplest output loss function that can be used with the above network is the maximum likelihood estimation criterion, which in our case is equivalent to the minimum mean squared error (MSE). The criterion for a set of training samples is simply
rtk7 | E(W)=>>UDP(ZP,W) p=1 P
1wc3 | (8)
333g | where yDp is the output of the Dpth RBF unit, i.e., the one that corresponds to the correct class of input pattern ZP. While this cost function is appropriate for most cases, it lacks three important properties. First, if we allow the parameters of the RBF to adapt, E(W) has a trivial, but totally unacceptable, solution. In this solution, all the RBF parameter vectors are equal and the state of F6 is constant and equal to that parameter vector. In this case the network happily ignores the input, and all the RBF outputs are equal
6kja | to zero. This collapsing phenomenon does not occur if the RBF weights are not allowed to adapt. The second problem is that there is no competition between the classes. Such a competition can be obtained by using a more discriminative training criterion, dubbed the maximum a posteriori (MAP) criterion, similar to maximum mutual information criterion sometimes used to train HMM's [48]-[50]. It corresponds to maximizing the posterior probability of the correct class Dp (or minimizing the logarithm of the probability of the correct class), given that the input image can come from one of the classes or from a background "rubbish" class label. In terms of penalties, it means that in addition to pushing down the penalty of the correct class like the MSE criterion, this criterion also pulls up the penalties of the incorrect classes
fmi5 | E(W)=> p=1 UDP(ZP,W)
bqn5 | - -
4lyq | + log (e-3 + )e-yi (Z",W)
jzqz | (9)
0uje | The negative of the second term plays a "competitive" role. It is necessarily smaller than (or equal to) the first term, therefore this loss function is positive. The constant j is positive and prevents the penalties of classes that are already very large from being pushed further up. The posterior probability of this rubbish class label would be the ratio of e-j and e- + Li e-yi (ZP,W). This discriminative criterion prevents the previously mentioned "collapsing effect" when the RBF parameters are learned because it keeps the RBF centers apart from each other. In Section VI, we present a generalization of this criterion for systems that learn to classify multiple objects in the input (e.g., characters in words or in documents).
apx3 | Computing the gradient of the loss function with respect to all the weights in all the layers of the convolutional network is done with back propagation. The standard al- gorithm must be slightly modified to take account of the
kcrm | weight sharing. An easy way to implement it is to first compute the partial derivatives of the loss function with respect to each connection, as if the network were a conventional multilayer network without weight sharing. Then the partial derivatives of all the connections that share a same parameter are added to form the derivative with respect to that parameter.