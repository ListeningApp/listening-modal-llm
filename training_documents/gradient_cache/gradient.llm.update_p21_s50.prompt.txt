You will receive raw text from an OCR scan of a document or web page. Each line of input represents one textbox from the document. Your task is to faithfully reproduce the text for our text-to-speech engine so that it is easily read aloud.

Note that the text might not start at the beginning and paragraphs may be split acrosss multiple textboxes. Textboxes may start or end in the middle of a word which is acceptable: do not combine text between textboxes and leave broken words at the start and end of a textbox unchanged.

You will receive one textbox per-line in the format `id | Textbox content`. The output must be in the format `id | Clean content`.

RULES:
 • Remove inline citations and references e.g. "(Author, 2021)" or "(https://wikipedia.org/article.html)" or "[ECMOS 35b, 47, 49]".
 • Spell out numbers, dates, and chemical formulas for the TTS engine e.g. "-2.5" -> "negative two point five" or "Jan. 2020" -> "January twenty twenty".
 • Spell out LaTeX formulas for the TTS engine e.g. "<LATEX>p = 1</LATEX>" -> "P equals one".
 • Rejoin hyphenated word fragments within the same textbox e.g. "synth -esize" -> "synthesize".
 • Fix simple typos and OCR errors e.g. "O nce upon a 1ime." -> "Once upon a time.".
 • Delete nonsequiter textboxes that interrupt the middle of a sentence if they obviously do not belong.

Otherwise, the output should exactly match the input text:
 • Do not combine text across lines or attempt to fix words broken across lines.
 • Ensure each ID from the input is included in the output.
 • Make as few changes as possible while respecting the above rules.

EXAMPLE INPUT:
ccpt | stigate the effectiveness of psychedelics for ADHD symp- toms in adults.
0fpw | 2. Materia1 and methods
dqn8 | 2.1. Study design and participants
cop0 | The study employed a naturalistic design, assessing the experiences of participants at bas-,
lhkq | eline before they start, and at 2 and 4 weeks after initiation. The target population included adults with ADHD symptoms who had not been diag- nosed with ADHD before. To be included, participants needed to score on Conner's Adult ADHD Rating Scale (CAARS-S:SV). This cut-off was indicative of clinically elevated symptoms (Conners et al., 1999) (see section 2.3).
82ju | 2.2. Study procedure
y2qo | The online advertisement was placed on a website providing information about psychedelics (www.microdo sing.nl). Interested participants were redirected to information explaining the study rationale and procedure. The baseline survey took 20 min to complete. If the survey had not been completed after 24 h, a reminder was sent. Each of the surveys at the 2- and 4-week time points took about 15 min to complete. Par- ticipants were able to pause the surveys. Data collection occurred between Nov. 2020 and Jul. 2021. The study was approved by the Ethics Review Committee of Psy- chology and Neuroscience at Maastricht University (ERCPN- 215_05_11_2019_A1).
1piq | 2.3. Measures
7fqw | 2.3.1. Demographic information and history of substance use
rhaz | At baseline, demographic information was collected. History of substance use assessed experience with psyche- delics (i.e., ayahuasca, DMT, 5-MeO-DMT, LSD, novel lysergamides (e.g., 1P-LSD, ALD-52), psilocybin, salvia divinorum, ibogaine, and mescaline) in both full (psychedelic) doses and microdoses.
wak1 | 2.3.2. Psychiatric and physiological diagnoses
wtfz | Participants were asked whether they had a current diagnosis of a disorder. These answer options were chosen because most of the listed diagnoses are often reported to co-occur with ADHD (Kooij et al., 2019), or because these diagnoses were reported to be common in people who microdose (Fadiman and Korb, 2019). We 
3j2l | REVIEW ARTICLE Frontiers in Psychiatry | www.frontiersin.org
03k3 | 2
1k9e | constructed a variable Comorbidity alongside ADHD, differenti- ating respondents with and without a comorbid diagnosis alongside ADHD (<LATEX>p < . 0 0 1 =</LATEX> only ADHD or no ADHD diagnosis; <LATEX>p > . 0 1 =</LATEX> ADHD and at least one other diagnosis). We constructed a variable Medication use alongside microdosing, differentiating respondents who were and were not using conven- tiona1 ADHD medication alongside microdosing (<LATEX>0 =</LATEX> only microdosing; <LATEX>1 =</LATEX> conven-

EXAMPLE OUTPUT:
ccpt | stigate the effectiveness of psychedelics for ADHD symptoms in adults.
0fpw | Two. Material and methods
dqn8 | Two point one. Study design and participants
cop0 | The study employed a naturalistic design, assessing the experiences of participants at bas-,
lhkq | eline before they start, and at two and four weeks after initiation. The target population included adults with ADHD symptoms who had not been diagnosed with ADHD before. To be included, participants needed to score on Conner's Adult ADHD Rating Scale. This cut-off was indicative of clinically elevated symptoms (see section two point three).
82ju | Two point two. Study procedure
y2qo | The online advertisement was placed on a website providing information about psychedelics. Interested participants were redirected to information explaining the study rationale and procedure. The baseline survey took twenty minutes to complete. If the survey had not been completed after twenty-four hours, a reminder was sent. Each of the surveys at the two and four-week time points took about fifteen minutes to complete. Participants were able to pause the surveys. Data collection occurred between November twenty twenty and July twenty twenty-one. The study was approved by the Ethics Review Committee of Psychology and Neuroscience at Maastricht University.
1piq | Two point three. Measures
7fqw | Two point three point one. Demographic information and history of substance use
rhaz | At baseline, demographic information was collected. History of substance use assessed experience with psychedelics (i.e., ayahuasca, DMT, five-MeO-DMT, LSD, novel lysergamides (e.g., one-P-LSD, ALD-fifty-two), psilocybin, salvia divinorum, ibogaine, and mescaline) in both full (psychedelic) doses and microdoses.
wak1 | Two point three point two. Psychiatric and physiological diagnoses
wtfz | Participants were asked whether they had a current diagnosis of a disorder. These answer options were chosen because most of the listed diagnoses are often reported to co-occur with ADHD, or because these diagnoses were reported to be common in people who microdose. We
3j2l | REVIEW ARTICLE Frontiers in Psychiatry: www.frontiersin.org
03k3 | Two
1k9e | constructed a variable Comorbidity alongside ADHD, differentiating respondents with and without a comorbid diagnosis alongside ADHD (P is less than point zero zero one equals only ADHD or no ADHD diagnosis; P is greater than point zero one equals ADHD and at least one other diagnosis). We constructed a variable Medication use alongside microdosing, differentiating respondents who were and were not using conventional ADHD medication alongside microdosing (zero equals only microdosing; one equals conven-




i22c | is associated with an image that contains all the ink between the cut associated with the source node and the cut associated with the destination node. An arc is created between two nodes if the segmentor decided that the ink between the corresponding cuts could form a candidate character. Typically, each individual piece of ink would be associated with an arc. Pairs of successive pieces of ink would also be included, unless they are separated by a wide gap, which is a clear indication that they belong to different characters. Each complete path through the graph contains each piece of ink once and only once. Each path corresponds to a different way of associating pieces of ink together so as to form characters.
ofle | B. Recognition Transformer and Viterbi Transformer
ezbr | A simple GTN to recognize character strings is shown in Fig. 17. It is composed of two GT's called the recognition transformer Trec and the Viterbi transformer Tvit. The goal of the recognition transformer is to generate a graph, called the interpretation graph or recognition graph Gint, that contains all the possible interpretations for all the possible segmentations of the input. Each path in Gint represents one possible interpretation of one particular segmentation
tpiw | of the input. The role of the Viterbi transformer is to extract the best interpretation from the interpretation graph.
dgkq | The recognition transformer Trec takes the segmentation graph Gseg as input, and applies the recognizer for single characters to the images associated with each of the arcs in the segmentation graph. The interpretation graph Gint has almost the same structure as the segmentation graph, except that each arc is replaced by a set of arcs from and to the same node. In this set of arcs, there is one arc for each pos- sible class for the image associated with the corresponding arc in Gseg. As shown in Fig. 18, to each arc is attached a class label, and the penalty that the image belongs to this class as produced by the recognizer. If the segmentor has computed penalties for the candidate segments, these penalties are combined with the penalties computed by the character recognizer to obtain the penalties on the arcs of the interpretation graph. Although combining penalties of different nature seems highly heuristic, the GTN training procedure will tune the penalties and take advantage of this combination anyway. Each path in the interpretation graph corresponds to a possible interpretation of the input word. The penalty of a particular interpretation for a particular segmentation is given by the sum of the arc penalties along the corresponding path in the interpretation graph. Computing the penalty of an interpretation independently of the segmentation requires to combine the penalties of all the paths with that interpretation. An appropriate rule for combining the penalties of parallel paths is given in Section VI-C.
hxjr | The Viterbi transformer produces a graph Gvit with a single path. This path is the path of least cumulated penalty in the Interpretation graph. The result of the recognition can be produced by reading off the labels of the arcs along the graph Gvit extracted by the Viterbi transformer. The Viterbi transformer owes its name to the famous Viterbi algorithm [70], an application of the principle of dynamic programming to find the shortest path in a graph efficiently. Let ci be the penalty associated to arc i, with source node Si and destination node di (note that there can be multiple arcs between two nodes). In the interpretation graph, arcs also have a label l ¿. The Viterbi algorithm proceeds as follows. Each node n is associated with a cumulated Viterbi penalty Un. Those cumulated penalties are computed in any order that satisfies the partial order defined by the interpretation graph (which is directed and acyclic). The start node is initialized with the cumulated penalty Ustart = 0. The other nodes cumulated penalties Un are computed recursively from the v values of their parent nodes, through the upstream arcs Un = {arc i with destination di = n}
2yv3 | Un = min (ci + Usi). iEUn (10)
opoe | Furthermore, the value of i for each node n which min- imizes the right-hand side is noted mn, the minimizing entering arc. When the end node is reached we obtain in Vend, the total penalty of the path with the smallest total penalty. We call this penalty the Viterbi penalty, and this sequence of arcs and nodes the Viterbi path. To obtain the Viterbi path with nodes n1 ... ny and arcs i1 ... ir-1, we trace back these nodes and arcs as follows, starting with ny = the end node, and recursively using the minimizing entering arc: it = mn+1, and nt = Sit until the start node is reached. The label sequence can then be read off the arcs of the Viterbi path.
vr2v | VI. GLOBAL TRAINING FOR GRAPH TRANSFORMER NETWORKS
m4jl | Section V described the process of recognizing a string using HOS, assuming that the recognizer is trained so as to give low penalties for the correct class label of correctly segmented characters, high penalties for erroneous categories of correctly segmented characters, and high penalties for all categories for poorly formed characters. This section explains how to train the system at the string level to do the above without requiring manual labeling of character segments. This training will be performed with a GTN whose architecture is slightly different from the recognition architecture described in Section V.
59hi | In many applications, there is enough a priori knowledge about what is expected from each of the modules in order to train them separately. For example, with HOS one could individually label single-character images and train a character recognizer on them, but it might be difficult to obtain an appropriate set of noncharacter images to train the model to reject wrongly segmented candidates. Although separate training is simple, it requires additional
ua3p | supervision information that is often lacking or incomplete (the correct segmentation and the labels of incorrect candi- date segments). Furthermore, it can be shown that separate training is suboptimal [67].
0a2t | The following section describes four different gradient- based methods for training GTN-based handwriting recog- nizers at the string level: Viterbi training, discriminative Viterbi training, forward training, and discriminative for- ward training. The last one is a generalization to graph- based systems of the maximum a posteriori criterion in- troduced in Section II-C. Discriminative forward training is somewhat similar to the so-called maximum mutual information criterion used to train HMM in speech recog- nition. However, our rationale differs from the classical one. We make no recourse to a probabilistic interpretation but show that, within the gradient-based learning approach, discriminative training is a simple instance of the pervasive principle of error correcting learning.
abow | Training methods for graph-based sequence recognition systems such as HMM's have been extensively studied in the context of speech recognition [28]. Those meth- ods require that the system be based on probabilistic generative models of the data, which provide normalized likelihoods over the space of possible input sequences. Popular HMM learning methods, such as the Baum-Welsh algorithm, rely on this normalization. The normalization cannot be preserved when nongenerative models such as NN's are integrated into the system. Other techniques, such as discriminative training methods, must be used in this case. Several authors have proposed such methods to train NN/HMM speech recognizers at the word or sentence level [29], [67], [71]-[78].
qx6n | Other globally trainable sequence recognition systems avoid the difficulties of statistical modeling by not resorting to graph-based techniques. The best example is recurrent NN's (RNN's). Unfortunately, despite early enthusiasm, the training of RNN's with gradient-based techniques has proven very difficult in practice [79].
p2bm | The GTN techniques presented below simplify and gen- eralize the global training methods developed for speech recognition.
cxyx | A. Viterbi Training