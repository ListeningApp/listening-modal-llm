{
  "id": "chatcmpl-A3UELipn6DKb9r78dg52zPVh7u6Ub",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "pg0s | Inexact Newton-type Methods for Optimisation with Nonnegativity Constraints\naiap | Abstract\n55v8 | We consider solving large scale nonconvex optimisation problems with nonnegativity constraints. Such problems arise frequently in machine learning, such as nonnegative least-squares, nonnegative matrix factorisation, as well as problems with sparsity-inducing regularisation. In such settings, first-order methods, despite their simplicity, can be prohibitively slow on ill-conditioned problems or become trapped near saddle regions, while most second-order alternatives involve nontrivially challenging subproblems. The two-metric projection framework, initially proposed by Bertsekas, alleviates these issues and achieves the best of both worlds by combining projected gradient steps at the boundary of the feasible region with Newton steps in the interior in such a way that feasibility can be maintained by simple projection onto the nonnegative orthant. We develop extensions of the two-metric projection framework, which by inexactly solving the subproblems as well as employing non-positive curvature directions, are suitable for large scale and nonconvex settings. We obtain state-of-the-art convergence rates for various classes of nonconvex problems and demonstrate competitive practical performance on a variety of problems.\n6sol | One. Introduction\ngtra | We consider high-dimensional problems of the form\nbhjq | x in R d min f of x, subject to x greater than or equal to zero,\n67jy | where d is much greater than one and f from R d to R is twice continuously differentiable and possibly nonconvex function. Despite the simplicity of its formulation, such problems arise in many\nu6kp | applications in science, engineering, and machine learning. Typical examples in machine learning include nonnegative formulations of least-squares and matrix factorisation. Additionally, problems involving sparsity inducing regularisation such as l one norm, which are typically non-smooth, can be reformulated into a differentiable objective with nonnegativity constraints.\nvpph | Many methods have been developed to solve x in R d min f of x, subject to x greater than or equal to zero. First-order methods, such as projected gradient descent, can be very simple to implement and as such are popular in machine learning. However, they come with well-known deficiencies, including relatively-slow convergence on ill-conditioned problems, sensitivity to hyper-parameter settings such as learning rate, and difficulty in escaping flat regions and saddle points. On the other hand, general purpose second-order algorithms, for example, projected Newton method and interior point methods, alleviate some of these issues such as susceptibility to ill-conditioning and or stagnation near flat regions. However, due to not leveraging the simplicity of the constraint, these advantages come at the cost of introducing highly non-trivial and challenging subproblems.\n03xu | By exploiting the structure of the constraint in x in R d min f of x, subject to x greater than or equal to zero, Bertsekas proposed the two-metric projection framework as a natural and simple adaptation of the classical Newton's method for unconstrained problems. By judicious modification of the Hessian matrix, this framework can be effectively seen as projecting Newton's step onto the nonnegative orthant. This allows for the best of both worlds, blending the efficiency of classical Newton's method with the simplicity of projected gradient descent. Indeed, similar to the classical Newton's method, the subproblem amounts to solving a linear system, while like projected gradient-descent, the projection step is straightforward.\n54ay | Contribution. In this paper, we design, theoretically analyse, and empirically evaluate novel two-metric projection type algorithms with desirable complexity guarantees for solving large scale and nonconvex optimisation problems with nonnegativity constraints. Both Algorithms One and Two are Hessian-free in that the subproblems are solved inexactly using the minimum residual method and only require Hessian-vector product evaluations. To achieve approximate first-order optimality, we leverage the theoretical properties of minimum residual, as recently established, for example, nonnegative curvature detection and monotonicity properties, and we show the following:\npb0r | One. Under minimal assumptions, Algorithm One achieves global iteration complexity that matches those of first-order alternatives.\n5j88 | Two. Under stronger assumptions, Algorithm Two enjoys a global iteration complexity guarantee with an improved rate that matches the state of the art for second-order methods.\nnfec | Three. Both variants obtain competitive oracle complexities, that is, the total number of gradient and Hessian-vector product evaluations.\nb5nq | Four. Our approach enjoys fast local convergence guarantees.\np2om | Five. Our approach exhibit highly competitive empirical performance on several machine learning problems.\nynxq | To our knowledge, the complexity guarantees outlined in this paper are the first to be established for two-metric projection type algorithms in nonconvex settings.",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725394553,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 985,
    "prompt_tokens": 2856,
    "total_tokens": 3841
  }
}