You will receive raw text from an OCR scan of a document or web page. Each line of input represents one textbox from the document.

Your task is to label each textbox with exactly one of the following labels:
  title: the primary document title, no more than one per document
  heading: the name of the following chapter or section (should never start in the middle of the previous paragraph)
  subheading: a secondary heading, often following another heading (should never start in the middle of the previous paragraph)
  body: main paragraph content: may start or end in the middle of a word, use context to follow paragraphs that are split across textboxes (if a textbox contains both heading and body text, label it "body")
  math: textbox primarily containing math
  imageDescription: standalone callout that only describes an image, chart, table, or similar (typically begins with "Table...", "Figure...", "Fig. ...", "Above...", "Left...", etc.)
  authors: names of contributors to this document
  institutions: list of universities, business, and other institutions affiliated with this document
  publisher: info about the publisher, or provided by the publisher, including copyright info
  pagenum: the page number
  headerOrFooter: boilerplate text page number, or redundant headings at the top/bottom of the page
  toc: the table of contents
  references: bibliographic references and resources
  acknowledgements: thanks and credits to contributors
  appendix: index, appendix, or any other end matter not part of the main thread (including related headings)
  table: tabular data, label, title, or subheading for a grid or list of data
  datum: numeric data or data label from a chart or list
  advertisement: promotional ads and marketing
  layout: non-content related to the layout of the page, e.g. sidebar navigation
  callout: pull quote or long block text that stands apart from the main text
  footnote: footnotes and endnotes near the bottom of the page
  keywords: list of keywords or tags
  url: web address or email
  other: any other text that does not belong to the main thread

Each line of input is prefixed with a textbox ID in the format `id | Text content`. The output must be in the format `id | label`.

Note that body text may be split across multiple textboxes, and textboxes may start or end in the middle of a sentence or word. Because the text was extracted from a web page or document, paragraphs of body text may be interrupted by multiple textboxes of redundant headers, footnotes, page numbers, tables, images, etc. For example, a real heading will never interrupt the middle of a sentence. Use context clues like this to carefully label each textbox.

EXAMPLE INPUT:
1pjs | Neuroscience Applied 1 (2022) 101012
o2kr | Contents lists available at ScienceDirect
v6sk | Neuroscience Applied
1wj6 | journal homepage: www.journals.elsevier.com/neuroscience-applied
936l | Research Articles
06yq | Microdosing with psychedelics to self-medicate for ADHD symptoms in adults: A naturalistic study
dpv6 | ARTICLE INFO
8r5j | ABSTRACT
2taz | Keywords: ADHD Microdosing Psychedelics Self-medication Well-being Time perception
o3ya | ADHD in adulthood is often overlooked, which negatively affects the individual's well-being. First-line phar- macological interventions are effective in many ADHD patients, relieving symptoms rapidly. However, there seems to be a proportion of individuals who discontinue, or fail to respond to these treatments. For these in- dividuals, alternative treatment options should be explored.
r5il | 1. Introduction
5gui | Attention deficit hyperactivity disorder (ADHD) is one of the most common developmental disorders worldwide. Prevalence research indicates that 2.6% of the adult population has persistent ADHD. ADHD in adults is often overlooked because of the high comorbidity rate and lack of knowledge about how ADHD is expressed in adulthood (Kooij et al.,
9d4o | Fig. 1. Flowchart of included participants for each time point.
82qc | 2019). In addition, ADHD is associated with deficits in various domains of cogni- tive functioning. Twenty-five percent of ADHD cases suffered from ADHD symptoms purely because of de- ficiencies in temporal processing (Sonuga-Barke et al., 2010).
ls7d | First-line ADHD treatments in adults mainly include pharmacological interventions to enhance dopaminergic and noradrenergic neurotrans- mission with stimulants. Overall, they have been proven to work effectively in adults with ADHD, inducing fast symptom relief and thereby enhancing the person's quality of life. In the
5esx | Table 1 Demographic information from respondents at baseline and the two and four- week time points.
t8vv | longer term, approximately twenty percent of ADHD patients discontinue their prescribed medication after six to nine months, thirty percent after one year, and half of them after two years.
xrd1 | 2. Material and methods
5746 | 2.1. Study design and participants
7toz | The study employed a naturalistic design, assessing the experiences of participants at baseline,
j346 | Neuroscience Applied 1 (2022)
m5ka | E.C.H.M. Haijen et al.
kx2c | REVIEW
v8uq | before they start MD on their own initiative, and at two and four weeks after MD initiation. The target population included adults diagnosed with ADHD and individuals who experienced ADHD symptoms to the extent that these interfered with their daily lives and who had not been diag- nosed with ADHD before. To be included in the analyses, participants needed to score above a cut-off point on at least one of the subscales of the Conner's Adult ADHD Rating Scale (CAARS-S:SV).
2zmw | Fig. 2. Mean scores of the CAARS-S:SV DSM-IV total symptoms T-scores at baseline (0W) and two (2W) and four weeks (4W) after MD (A) of the whole sample, and (B) per conventional ADHD medication use. Error bars represent mean error.
sdk3 | <LATEX>\lim _ { x ightarrow \infty } rac { 6 x ^ { 2 } + 1 0 0 } { 7 x ^ { 2 } - 1 0 } =</LATEX>
x972 | 2.2. Study procedure
k221 | Mean performance measures of the CAARS-

EXAMPLE OUTPUT:
1pjs | headerOrFooter
o2kr | publisher
v6sk | publisher
1wj6 | publisher
936l | headerOrFooter
06yq | title
dpv6 | other
8r5j | heading
2taz | keywords
o3ya | body
r5il | heading
5gui | body
9d4o | imageDescription
82qc | body
ls7d | body
5esx | imageDescription
t8vv | body
xrd1 | heading
5746 | subheading
7toz | body
j346 | headerOrFooter
m5ka | authors
kx2c | headerOrFooter
v8uq | body
2zmw | imageDescription
sdk3 | math
x972 | heading
k221 | body




mq4s | Translated in terms of penalties, the penalty of an inter- pretation should be the negative logarithm of the sum of the negative exponentials of the penalties of the individual paths. The overall penalty will be smaller than all the penalties of the individual paths.
95el | Given an interpretation, there is a well-known method, called the forward algorithm for computing the above quantity efficiently [28]. The penalty computed with this procedure for a particular interpretation is called the for- ward penalty. Consider again the concept of constrained
a0a5 | graph, the subgraph of the interpretation graph which contains only the paths that are consistent with a particular label sequence. There is one constrained graph for each possible label sequence (some may be empty graphs, which have infinite penalties). Given an interpretation, running the forward algorithm on the corresponding constrained graph gives the forward penalty for that interpretation. The forward algorithm proceeds in a way very similar to the Viterbi algorithm, except that the operation used at each node to combine the incoming cumulated penalties, instead of being the min function, is the so-called logadd operation, which can be seen as a "soft" version of the min function
gocl | fn = logaddieu, (ci + fsi)
ob1z | (13)
m4cb | where fstart = 0, Un is the set of upstream arcs of node n, Ci is the penalty on arc i, and
m61y | logadd(x1, x2, ... , In) = - log ¿= 1 > e (14)
u2ob | Note that because of numerical inaccuracies, it is better to factorize the largest e-ri (corresponding to the smallest penalty) out of the logarithm.
2gnm | An interesting analogy can be drawn if we consider that a graph on which we apply the forward algorithm is equivalent to an NN on which we run a forward propaga- tion, except that multiplications are replaced by additions, the additions are replaced by log-adds, and there are no sigmoids.
emgw | One way to understand the forward algorithm is to think about multiplicative scores (e.g., probabilities) instead of additive penalties on the arcs: score = exp(-penalty). In that case the Viterbi algorithm selects the path with the largest cumulative score (with scores multiplied along the path), whereas the forward score is the sum of the cumulative scores associated to each of the possible paths from the start to the end node. The forward penalty is always lower than the cumulated penalty on any of the paths, but if one path "dominates" (with a much lower penalty), its penalty is almost equal to the forward penalty. The forward algorithm gets its name from the forward pass of the well-known Baum-Welsh algorithm for training HMM's [28]. Section VIII-E gives more details on the relation between this work and HMM's.
xaqt | The advantage of the forward penalty with respect to the Viterbi penalty is that it takes into account all the different ways to produce an answer, not just the one with the lowest penalty. This is important if there is some ambiguity in the segmentation, since the combined forward penalty of two paths C1 and C2 associated with the same label sequence may be less than the penalty of a path C3 associated with another label sequence, even though the penalty of C3 might be less than any one of C1 or C2.
5rcb | The forward-training GTN is only a slight modification of the previously introduced Viterbi-training GTN. It suffices to turn the Viterbi transformers in Fig. 19 into forward scorers that take an interpretation graph as input an produce
524j | the forward penalty of that graph on output. Then the penalties of all the paths that contain the correct answer are lowered, instead of just that of the best one.
xd3m | Back propagating through the forward penalty computa- tion (the forward transformer) is quite different from back propagating through a Viterbi transformer. All the penalties of the input graph have an influence on the forward penalty, but penalties that belong to low-penalty paths have a stronger influence. Computing derivatives with respect to the forward penalties fn computed at each n node of a graph is done by back-propagation through the graph Gc
8qb6 | afdi JE cfa: - Ci (15)
wkbn | Ə fn = e-fn iEDn
ftfg | where Dn = {arc i with source si = n} is the set of downstream arcs from node n. From the above derivatives, the derivatives with respect to the arc penalties are obtained
7xi5 | JE
w8r5 | e-Ci-fsi+fdi. (16)
ybik | This can be seen as a "soft" version of the back propagation through a Viterbi scorer and transformer. All the arcs in Gc have an influence on the loss function. The arcs that belong to low penalty paths have a larger influence. Back propagation through the path selector is the same as before. The derivative with respect to Gint arcs that have an alter ego in Ge are simply copied from the corresponding arc in Gc. The derivatives with respect to the other arcs are zero.
gii5 | Several authors have applied the idea of back-propagating gradients through a forward scorer to train speech recogni- tion systems, including Bridle and his o-net model [73] and Haffner and his oß-TDNN model [81], but these authors recommended discriminative training as described in the next section.
q5eu | D. Discriminative Forward Training
a7ao | The information contained in the forward penalty can be used in another discriminative training criterion which we will call the discriminative forward criterion. This criterion corresponds to maximization of the posterior probability of choosing the paths associated with the correct interpreta- tion. This posterior probability is defined as the exponential of minus the constrained forward penalty, normalized by the exponential of minus the unconstrained forward penalty. Note that the forward penalty of the constrained graph is always larger or equal to the forward penalty of the unconstrained interpretation graph. Ideally, we would like the forward penalty of the constrained graph to be equal to the forward penalty of the complete interpretation graph. Equality between those two quantities is achieved when the combined penalties of the paths with the correct label sequence is negligibly small compared to the penalties of all the other paths, or that the posterior probability associated to the paths with the correct interpretation is almost one, which is precisely what we want. The corresponding GTN training architecture is shown in Fig. 21.
29a3 | Let the difference be denoted Edforw, and let us call Ccforw the forward penalty of the constrained graph and
sb1e | Fig. 21. Discriminative forward training GTN architecture for a character string recognizer based on HOS.
w2yn | Cforw the forward penalty of the complete interpretation graph
ap6l | Edforw = Ccforw -Cforw. (17)
iest | Edforw is always positive since the constrained graph is a subset of the paths in the interpretation graph, and the forward penalty of a graph is always larger than the forward penalty of a subgraph of this graph. In the ideal case, the penalties of incorrect paths are infinitely large, therefore the two penalties coincide and Edforw is zero. Readers familiar with the Boltzmann machine connectionist model might recognize the constrained and unconstrained graphs as analogous to the "clamped" (constrained by the observed values of the output variable) and "free" (unconstrained) phases of the Boltzmann machine algorithm [13].
cox8 | Back propagating derivatives through the discriminative forward GTN distributes gradients more evenly than in the Viterbi case. Derivatives are back propagated through the left half of the GTN in Fig. 21 down to the interpretation graph. Derivatives are negated and back propagated through the right-half, and the result for each arc is added to the contribution from the left half. Each arc in Gint now has a derivative. Arcs that are part of a correct path have a positive derivative. This derivative is very large if an incorrect path has a lower penalty than all the correct paths. Similarly, the derivatives with respect to arcs that are part of a low-penalty incorrect path have a large negative derivative. On the other hand, if the penalty of a path associated with the correct interpretation is much smaller than all other paths, the loss function is very close to zero and almost no gradient is back propagated. The training therefore concentrates on examples of images which yield a classification error, and furthermore, it concentrates on the pieces of the image which cause that error. Discriminative forward training is an elegant and efficient way of solving the infamous credit assignment problem for learning ma- chines that manipulate "dynamic" data structures such as graphs. More generally, the same idea can be used in all
ppqi | situations where a learning machine must choose between discrete alternative interpretations.
oxou | As previously, the derivatives on the interpretation graph penalties can then be back propagated into the character recognizer instances. Back propagation through the charac- ter recognizer gives derivatives on its parameters. All the gradient contributions for the different candidate segments are added up to obtain the total gradient associated to one pair (input image, correct label sequence), that is, one example in the training set. A step of stochastic gradient descent can then be applied to update the parameters.
495n | E. Remarks on Discriminative Training
awri | In the above discussion, the global training criterion was given a probabilistic interpretation, but the individual penalties on the arcs of the graphs were not. There are good reasons for that. For example, if some penalties are associated to the different class labels, they would: 1) have to sum to one (class posteriors) or 2) integrate to one over the input domain (likelihoods).
bl28 | Let us first discuss the first case (class posteriors nor- malization). This local normalization of penalties may eliminate information that is important for locally rejecting all the classes [82], e.g., when a piece of image does not correspond to a valid character class because some of the segmentation candidates may be wrong. Although an explicit "garbage class" can be introduced in a probabilistic framework to address that question, some problems remain because it is difficult to characterize such a class probabilis- tically and to train a system in this way (it would require a density model of unseen or unlabeled samples).
d5bz | The probabilistic interpretation of individual variables plays an important role in the Baum-Welsh algorithm in combination with the expectation-maximization (EM) procedure. Unfortunately, those methods cannot be applied to discriminative training criteria, and one is reduced to using gradient-based methods. Enforcing the normalization of the probabilistic quantities while performing gradient- based learning is complex, inefficient, time consuming, and creates ill-conditioning of the loss-function.
ap5o | Following [82], we therefore prefer to postpone normal- ization as far as possible (in fact, until the final decision stage of the system). Without normalization, the quantities manipulated in the system do not have a direct probabilistic interpretation.
7zvf | Let us now discuss the second case (using a generative model of the input). Generative models build the boundary indirectly by first building an independent density model for each class and then performing classification decisions on the basis of these models. This is not a discriminative approach in that it does not focus on the ultimate goal of learning, which in this case is to learn the classification decision surface. Theoretical arguments [6], [7] suggest that estimating input densities when the real goal is to obtain a discriminant function for classification is a suboptimal strategy. In theory, the problem of estimating densities in high-dimensional spaces is much more ill posed than finding decision boundaries.
mov9 | Fig. 22. Explicit segmentation can be avoided by sweeping a recognizer at every possible location in the input field.
ks7m | Even though the internal variables of the system do not have a direct probabilistic interpretation, the overall system can still be viewed as producing posterior probabilities for the classes. In fact, assuming that a particular label sequence is given as the "desired sequence" to the GTN in Fig. 21, the exponential of minus Edforw can be interpreted as an estimate of the posterior probability of that label sequence given the input. The sum of those posteriors for all the possible label sequences is one. Another approach would consists of directly minimizing an approximation of the number of misclassifications [83], [76]. We prefer to use the discriminative forward loss function because it causes less numerical problems during the optimization. We will see in Section X-C that this is a good way to obtain scores on which to base a rejection strategy. The important point being made here is that one is free to choose any parameterization deemed appropriate for a classification model. The fact that a particular parameterization uses internal variables with no clear probabilistic interpretation does not make the model any less legitimate than models that manipulate normalized quantities.
w1w1 | An important advantage of global and discriminative training is that learning focuses on the most important errors, and the system learns to integrate the ambiguities from the segmentation algorithm with the ambiguities of the character recognizer. In Section IX we present ex- perimental results with an online handwriting recognition system that confirm the advantages of using global training versus separate training. Experiments in speech recognition with hybrids of NN's and HMM's also showed marked improvements brought by global training [29], [67], [77], [84].
h8gv | VII. MULTIPLE OBJECT RECOGNITION: SPACE DISPLACEMENT NEURAL NETWORK
kwkp | There is a simple alternative to explicitly segmenting images of character strings using heuristics. The idea is to sweep a recognizer at all possible locations across a normalized image of the entire word or string as shown in Fig. 22. With this technique, no segmentation heuristics are required since the system essentially examines all the possible segmentations of the input. However, there are problems with this approach. First, the method is in general
yj0m | quite expensive. The recognizer must be applied at every possible location on the input, or at least at a large enough subset of locations so that misalignments of characters in the field of view of the recognizers are small enough to have no effect on the error rate. Second, when the recognizer is centered on a character to be recognized, the neighbors of the center character will be present in the field of view of the recognizer, possibly touching the center character. Therefore the recognizer must be able to correctly recognize the character in the center of its input field, even if neighboring characters are very close to or touching the central character. Third, a word or character string cannot be perfectly size-normalized. Individual characters within a string may have widely varying sizes and baseline positions. Therefore the recognizer must be very robust to shifts and size variations.
ewfp | These three problems are elegantly circumvented if a convolutional network is replicated over the input field. First of all, as shown in Section III, convolutional NN's are very robust to shifts and scale variations of the input image, as well as to noise and extraneous marks in the input. These properties take care of the latter two problems mentioned in the previous paragraph. Second, convolutional networks provide a drastic saving in computational requirement when replicated over large input fields. A replicated convolutional network, also called an SDNN [27], is shown in Fig. 23. While scanning a recognizer can be prohibitively expen- sive in general, convolutional networks can be scanned or replicated very efficiently over large, variable-size input fields. Consider one instance of a convolutional net and its