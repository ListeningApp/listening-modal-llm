{
  "id": "chatcmpl-A0ZgY31hOvyQpbxgFOozByfhRMk6o",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "```\n8rz1 | Suppose a system has some measure space X of states with energy E : X maps to R. In thermal equilibrium the probability distribution on states, p: X maps to R, maximizes the Gibbs entropy S equals negative k times the integral of p of x times the natural log of p of x dx\nxhb4 | subject to a constraint on the expected value of energy: the expected value of E equals the integral of p of x times E of x dx\nkext | Typically when this happens p is the Boltzmann distribution\ntshv | e to the power of negative E of x divided by kT dx\narau | where T is the temperature and k is Boltzmann's constant.\ni36b | Then as we vary the expected value of E we have\nx6yc | d of the expected value of E equals T dS\nxcfx | We can now generalize a lot of our work from a finite set of states to a general measure space. I won't redo all the arguments, just state the results and point out a couple of caveats.\nybu5 | For any measure space X we say a function p: X maps to [zero, infinity) is a probability distribution if it's measurable and the integral of p of x dx equals one.\nwxhx | We can define a version of Shannon entropy for p by\nf4i3 | H equals negative the integral of p of x times the log of p of x dx,\navlk | but physicists mainly use the Gibbs entropy, defined by\nvi9g | S equals negative k times the integral of p of x times the natural log of p of x dx.\ntyl7 | As I warned you last time, this can take values in [negative infinity, infinity], though we are mainly interested in cases when it's finite. If we think of X as the space of states of some system, we can pick any measurable function E : X maps to R and call it the 'energy'. Its expected value is then\n2hpu | the expected value of E equals the integral of E of x times p of x dx\na10k | at least when this integral converges.\n2pp5 | We say the probability distribution p describes thermal equilibrium if it maximizes S subject to a constraint of the expected value of E equals c. Typically when this happens p is a Boltzmann distribution\nl20h | p of x equals one divided by Z times e to the power of negative beta times E of x dx\nkfwj | where beta is called the coolness. I say 'typically' because even when X is a finite set, we saw in Puzzle twenty-four that there can be thermal equilibria that are not Boltzmann distributions, but only limits of Boltzmann distributions as beta approaches positive infinity or negative infinity. This can also happen for other measure spaces X. I will not delve into this, because my goal now is to get to some physics.\n7hi1 | As before, we can write beta equals one over kT, at least if beta is not zero, and then write the Boltzmann distribution as\nvpa2 | X times e to the power of negative E of x divided by kT dx\nmtlg | p of x equals\n6rz3 | Also as before, the Boltzmann distributions obey the crucial relation\noxdb | dH equals beta times the differential of the expected value of E.\nv978 | Rewriting this in terms of Gibbs entropy S equals kH and temperature T equals one divided by k beta, it becomes this famous relation between temperature, entropy and the expected energy: TdS equals the differential of the expected value of E.\nuqzw | Notice that the units match here. The Shannon entropy H is dimensionless, but since k has units of energy per temperature, the Gibbs entropy S equals kH has units of energy per temperature. Thus TdS has units of energy, as does the differential of the expected value of E.\n56yl | THE CHANGE IN ENTROPY\nr00c | As we change the temperature of a system from T one to T two while keeping it in thermal equilibrium, the change in its entropy is\nbhhp | S of T one minus S of T zero equals the integral from T zero to T one of the differential of the expected value of E divided by T\nbyke | where the expected value of E is its expected energy at temperature T.\nz2w8 | Last time we saw that as we change the expected energy of E of a system while keeping it in thermal equilibrium, this fundamental relation holds:\n1l0m | TdS equals the differential of the expected value of E.\n9ctx | We can rewrite this as\nyipb | dS equals the differential of the expected value of E divided by T\na9dp | and then integrate this from one temperature to another-remember, as the expected energy changes, so does the temperature. We get\nzso4 | the integral from T zero to T one of the differential of the expected value of E divided by T equals S of T one minus S of T zero.\n4g5b | This is the main way people do experiments to 'measure entropy'. Slowly heat something up, keeping track of how much energy it takes to increase its temperature each little bit. Using this data you can approximately calculate the integral at left and that gives the change in entropy!\nahtn | But so far we're just measuring changes in entropy. How can you figure out the actual value of the entropy? One way is to assume the Third Law of Thermodynamics, which says that in thermal equilibrium the entropy approaches zero as the temperature approaches zero from above. This gives\n5jda | the integral from T zero to T one of the differential of the expected value of E divided by T equals S of T\nq1jc | This is how people often 'measure the entropy' of a system in thermal equilibrium. They heat it up starting from absolute zero, very slowly so they hope it is close to thermal equilibrium at every moment-and they take data on how much energy is used, and approximately calculate the integral at left!\nke6a | But this relies on the Third Law of Thermodynamics. So where does that come from?\nc4gy | THE THIRD LAW OF THERMODYNAMICS\nlr9x | If a system has countably many states, with just one of lowest energy, and thermal equilibrium is possible for this system for some temperature T greater than zero, then its entropy in thermal equilibrium approaches zero as T approaches zero from above:\njux5 | the limit as T approaches zero from above of S of T equals zero\nbe8m | Some people say the Third Law of Thermodynamics this way: \"entropy is zero at absolute zero\". But it's not really that simple indeed, other people say it's impossible to reach absolute zero. Above I've stated a version of the Third Law that's actually a theorem. Let's prove it!\n1lxd | Actually, let's prove it now for systems with only finitely many states. It'll be easier to handle systems with a countably infinite number of states later, when we've developed more tools. And by the way, we'll see the Third Law isn't always true for systems with a continuum of states. It will fail for all three of the problems on our big to-do list: the classical harmonic oscillator, the classical particle in a box and the classical ideal gas. This is often taken as a failure of classical mechanics, since switching to quantum mechanics makes the Third Law hold for these systems.\nz3ff | Let's show that for a system with finitely many states i equals one, to n with energies Ei, as the temperature T approaches zero from above, the entropy of the system in thermal equilibrium approaches k natural log of N where N is the number of lowest-energy states. In thermal equilibrium\nrv0j | Pi equals e to the power of negative Ei divided by kT.\nkv4p | Thus, all states with the lowest energy have the same probability, while as the temperature approaches zero from above, any higher-energy states have pi approaches zero. So, as the temperature approaches zero from above, the probability of the system being in any one of its N lowest-energy states approaches one divided by N, and we get\ng1v3 | the limit as T approaches zero from above of S of T equals the limit as T approaches zero from above of negative k times the sum from i equals one to n of pi times the natural log of pi equals negative k times the sum from i equals one to n of one divided by N times the natural log of one divided by N.\n7d2w | natural log of N equals k natural log of N.\n3b38 | In particular, if the system has just one lowest-energy state, we get the Third Law of Thermodynamics:\nil0a | the limit as T approaches zero from above of S of T equals zero.\nmato | Here T approaches zero from above means that T is approaching zero from above.\n```",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1724700538,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_3aa7262c27",
  "usage": {
    "completion_tokens": 1858,
    "prompt_tokens": 3360,
    "total_tokens": 5218
  }
}