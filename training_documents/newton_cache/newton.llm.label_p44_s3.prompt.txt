You will receive raw text from an OCR scan of a document or web page. Each line of input represents one textbox from the document.

Your task is to label each textbox with exactly one of the following labels:
  title: the primary document title, no more than one per document
  heading: the name of the following chapter or section (should never start in the middle of the previous paragraph)
  subheading: a secondary heading, often following another heading (should never start in the middle of the previous paragraph)
  body: main paragraph content: may start or end in the middle of a word, use context to follow paragraphs that are split across textboxes (if a textbox contains both heading and body text, label it "body")
  math: textbox primarily containing math
  imageDescription: standalone callout that only describes an image, chart, table, or similar (typically begins with "Table...", "Figure...", "Fig. ...", "Above...", "Left...", etc.)
  authors: names of contributors to this document
  institutions: list of universities, business, and other institutions affiliated with this document
  publisher: info about the publisher, or provided by the publisher, including copyright info
  pagenum: the page number
  headerOrFooter: boilerplate text page number, or redundant headings at the top/bottom of the page
  toc: the table of contents
  references: bibliographic references and resources
  acknowledgements: thanks and credits to contributors
  appendix: index, appendix, or any other end matter not part of the main thread (including related headings)
  table: tabular data, label, title, or subheading for a grid or list of data
  datum: numeric data or data label from a chart or list
  advertisement: promotional ads and marketing
  layout: non-content related to the layout of the page, e.g. sidebar navigation
  callout: pull quote or long block text that stands apart from the main text
  footnote: footnotes and endnotes near the bottom of the page
  keywords: list of keywords or tags
  url: web address or email
  other: any other text that does not belong to the main thread

Each line of input is prefixed with a textbox ID in the format `id | Text content`. The output must be in the format `id | label`.

Note that body text may be split across multiple textboxes, and textboxes may start or end in the middle of a sentence or word. Because the text was extracted from a web page or document, paragraphs of body text may be interrupted by multiple textboxes of redundant headers, footnotes, page numbers, tables, images, etc. For example, a real heading will never interrupt the middle of a sentence. Use context clues like this to carefully label each textbox.

EXAMPLE INPUT:
1pjs | Neuroscience Applied 1 (2022) 101012
o2kr | Contents lists available at ScienceDirect
v6sk | Neuroscience Applied
1wj6 | journal homepage: www.journals.elsevier.com/neuroscience-applied
936l | Research Articles
06yq | Microdosing with psychedelics to self-medicate for ADHD symptoms in adults: A naturalistic study
dpv6 | ARTICLE INFO
8r5j | ABSTRACT
2taz | Keywords: ADHD Microdosing Psychedelics Self-medication Well-being Time perception
o3ya | ADHD in adulthood is often overlooked, which negatively affects the individual's well-being. First-line phar- macological interventions are effective in many ADHD patients, relieving symptoms rapidly. However, there seems to be a proportion of individuals who discontinue, or fail to respond to these treatments. For these in- dividuals, alternative treatment options should be explored.
r5il | 1. Introduction
5gui | Attention deficit hyperactivity disorder (ADHD) is one of the most common developmental disorders worldwide. Prevalence research indicates that 2.6% of the adult population has persistent ADHD. ADHD in adults is often overlooked because of the high comorbidity rate and lack of knowledge about how ADHD is expressed in adulthood (Kooij et al.,
9d4o | Fig. 1. Flowchart of included participants for each time point.
82qc | 2019). In addition, ADHD is associated with deficits in various domains of cogni- tive functioning. Twenty-five percent of ADHD cases suffered from ADHD symptoms purely because of de- ficiencies in temporal processing (Sonuga-Barke et al., 2010).
ls7d | First-line ADHD treatments in adults mainly include pharmacological interventions to enhance dopaminergic and noradrenergic neurotrans- mission with stimulants. Overall, they have been proven to work effectively in adults with ADHD, inducing fast symptom relief and thereby enhancing the person's quality of life. In the
5esx | Table 1 Demographic information from respondents at baseline and the two and four- week time points.
t8vv | longer term, approximately twenty percent of ADHD patients discontinue their prescribed medication after six to nine months, thirty percent after one year, and half of them after two years.
xrd1 | 2. Material and methods
5746 | 2.1. Study design and participants
7toz | The study employed a naturalistic design, assessing the experiences of participants at baseline,
j346 | Neuroscience Applied 1 (2022)
m5ka | E.C.H.M. Haijen et al.
kx2c | REVIEW
v8uq | before they start MD on their own initiative, and at two and four weeks after MD initiation. The target population included adults diagnosed with ADHD and individuals who experienced ADHD symptoms to the extent that these interfered with their daily lives and who had not been diag- nosed with ADHD before. To be included in the analyses, participants needed to score above a cut-off point on at least one of the subscales of the Conner's Adult ADHD Rating Scale (CAARS-S:SV).
2zmw | Fig. 2. Mean scores of the CAARS-S:SV DSM-IV total symptoms T-scores at baseline (0W) and two (2W) and four weeks (4W) after MD (A) of the whole sample, and (B) per conventional ADHD medication use. Error bars represent mean error.
sdk3 | <LATEX>\lim _ { x ightarrow \infty } rac { 6 x ^ { 2 } + 1 0 0 } { 7 x ^ { 2 } - 1 0 } =</LATEX>
x972 | 2.2. Study procedure
k221 | Mean performance measures of the CAARS-

EXAMPLE OUTPUT:
1pjs | headerOrFooter
o2kr | publisher
v6sk | publisher
1wj6 | publisher
936l | headerOrFooter
06yq | title
dpv6 | other
8r5j | heading
2taz | keywords
o3ya | body
r5il | heading
5gui | body
9d4o | imageDescription
82qc | body
ls7d | body
5esx | imageDescription
t8vv | body
xrd1 | heading
5746 | subheading
7toz | body
j346 | headerOrFooter
m5ka | authors
kx2c | headerOrFooter
v8uq | body
2zmw | imageDescription
sdk3 | math
x972 | heading
k221 | body




09z2 | For projected Newton-CG, we use the parameter settings from the experiments in Xie & Wright (2023). Specifically, in the notation of Xie & Wright (2023), we set the accuracy parameter and back tracking parameter to <LATEX>\zeta = \theta = 0 . 5</LATEX> and the step acceptance parameter to <LATEX>\eta = 0 . 2 .</LATEX> Furthermore, following the algorithmic to have equivalent termination conditions, we modify the gradient negativity check <LATEX>\begin{array}{} \text { lescription of Xie } 8 \text { Wright } \left( 2 0 2 3 \right) , \text { and } \\ \text { from } g _ { L } ^ { i } < - \epsilon _ { L } ^ { 3 / 2 } \text { to g } _ { k } ^ { i } < - \epsilon _ { k } \text { for this } \end{array}</LATEX> <LATEX>\mathrm { g } _ { k } ^ { i } < - \epsilon _ { k } ^ { 3 / 2 }</LATEX> <LATEX>\mathrm { g } _ { k } ^ { i } < - \epsilon _ { k }</LATEX> method.
13fv | For projected gradient and Newton-MR TMP, we set the scaling parameter in Algorithms 5 and 6 to <LATEX>\zeta = 0 . 5</LATEX> and the sufficient decrease parameter to <LATEX>\rho = 1 0 ^ { - 4 } .</LATEX> All line searches are initialised from <LATEX>\alpha _ { 0 } = 1 .</LATEX> We note that, for both FISTA and PGM, we terminate the iterations when <LATEX>| \left( f \left( \mathrm { x } _ { k } \right) + \lambda \| \mathrm { x } _ { k } \| _ { 1 } - \left( f \left( \mathrm { x } _ { k - 1 } \right) + \lambda \| \mathrm { x } _ { k - 1 } \| _ { 1 } \right) | < 1 0 ^ { - 8 } \right.</LATEX> on the <LATEX>\ell _ { 1 }</LATEX> problem and <LATEX>| f \left( \mathrm { x } _ { k } \right) - f \left( \mathrm { x } _ { k - 1 } \right) | < 1 0 ^ { - 8 }</LATEX> otherwise. We set the momentum term in PGM to <LATEX>\beta = 0 . 9</LATEX> and select the fixed step size by starting from <LATEX>\alpha = 1</LATEX> and successively shrinking the step size by a factor of 10 until the iterates are stable for the duration of the experiment, i.e., no divergence or large scale oscillations. This procedure resulted in a step size of <LATEX>\alpha = 1 0 ^ { - 3 }</LATEX> for the <LATEX>\ell _ { 1 }</LATEX> MLP (Figure 3) and <LATEX>\alpha = 1</LATEX> for the NNMF problems (Figures 4 and 5).
v5pj | We now give a more complete description of each of the objective functions.
trab | Multinomial Regression We first consider is the problems of multinomial regression on <LATEX>C</LATEX> classes. Specifically, consider a set of data items <LATEX>\left\{ \mathrm { a } _ { i } , b _ { i } \right\} _ { i = 1 } ^ { n } \subset \mathbb{R} ^ { d } \times \left\{ 1 , \ldots C \right\} .</LATEX> Denote the weights of each class as <LATEX>\mathrm { x } _ { 1 } , \ldots , \mathrm { x } _ { C }</LATEX> and define <LATEX>x =</LATEX> <LATEX>\left[ \mathrm { x } _ { 1 } , \ldots , \mathrm { x } _ { C - 1 } \right] .</LATEX> We are free to take <LATEX>\mathrm { x } _ { C } = 0</LATEX> as class <LATEX>C</LATEX> is identifiable from the weights of the other classes. The objective, <LATEX>f ,</LATEX> is given by
grxs | <LATEX>f \left( \mathrm { x } \right) = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } \sum _ { c = 1 } ^ { C - 1 } - 1 \left( b _ { i } = c \right) \log \left( \mathrm { s o f t m a x } \left( \mathrm { x } _ { c } , \mathrm { a } _ { i } \right) \right) ,</LATEX> (62)
u9dn | where <LATEX>1 \left( \cdot \right)</LATEX> is the indicator function and
t25w | <LATEX>\mathrm { s o f t m a x } \left( \mathrm { x } _ { c } , \mathrm { a } _ { i } \right) = \frac { \exp \left( \langle \mathrm { x } _ { c } , \mathrm { a } _ { i } \rangle \right) } { \sum _ { c = 1 } ^ { C } \exp \left( \langle \mathrm { x } _ { c } , \mathrm { a } _ { i } \rangle \right) } .</LATEX> In this case, the objective is convex. We allow for a constant term in each set of weights, <LATEX>\mathrm { x } _ { c } ,</LATEX> which we do not apply the <LATEX>\ell _ { 1 }</LATEX> penalisation to.
x62u | All methods for this example are initialised from
aoc8 | <LATEX>\mathrm { x } _ { 0 } = 0 .</LATEX> Neural Network Again, suppose we have a set of data items <LATEX>\left\{ \mathrm { a } _ { i } , b _ { i } \right\} _ { i = 1 } ^ { n } \subset \mathbb{R} ^ { d } \times \left\{ 1 , \ldots C \right\} .</LATEX> We consider a small two layer network with a smooth activation function. Specifically, we consider the sigmoid weighted linear unit (SiLU) activation (Elfwing et al., 2017) defined by
3mdu | <LATEX>\sigma \left( x \right) = \frac { x } { 1 + e ^ { - x } } .</LATEX> We note that the SiLU activation is similar to ReLU and is the product of a linear activation with a standard sigmoid activation. We define a network, <LATEX>\mathrm { h } \left( \cdot ; \mathrm { x } \right)</LATEX> parameterised by the weights, x, with the following architecture
m04v | <LATEX>\mathrm { I n p u t } \left( \mathrm { d } \right) \rightarrow \mathrm { L i n e a r } \left( 1 0 0 \right) \rightarrow \mathrm { S i L U } \rightarrow \mathrm { L i n e a r } \left( 1 0 0 \right) \rightarrow \mathrm { S i L U } \rightarrow \mathrm { L i n e a r } \left( 1 0 \right) ,</LATEX> where the number in brackets denotes the size of the output from the layer. Note that we allow for a bias term in each linear layer which we do not apply the <LATEX>\ell _ { 1 }</LATEX> penalty to. The objective function, <LATEX>f ,</LATEX> is given by cross entropy loss incurred by the network over the entire dataset
3aip | <LATEX>f \left( \mathrm { x } \right) = - \frac { 1 } { n } \sum _ { i = 1 } ^ { n } \log \left( \frac { \exp \left( \left[ \mathrm { h } \left( \mathrm { a } _ { i } ; \mathrm { x } \right) \right] ^ { b _ { i } } \right) } { \sum _ { c = 1 } ^ { C } \exp \left( \left[ \mathrm { h } \left( \mathrm { a } _ { i } ; \mathrm { x } \right) \right] ^ { c } \right) } \right) .</LATEX> (63)
q3gu | The weights for layer <LATEX>i ,</LATEX> denoted <LATEX>X ,</LATEX> are initialised with the default PyTorch initialisation, that is, via independent uniform draw
txab | <LATEX>\mathrm { x } _ { i } \sim U \left( - \sqrt { k } , \sqrt { k } \right) ,</LATEX> where <LATEX>k = 1 / \left( \# \mathrm { I n p u t s } \right)</LATEX> with (#Inputs) the number of input features into the layer.
dov0 | 44
xd5o | Inexact Newton-type Methods for Optimisation with Nonnegativity Constraints
3gny | NNMF Problem A common choice for (13) is a standard Euclidean distance function
ws73 | WH) = 1 | - v 2 F ,
vyos | (64)
w4hq | where | . || F is the Frobenius matrix norm. In this case, (13) is nonconvex in both W and H, when considered simultaneously, but convex so long as one of the variables is held fixed. This motivates the standard approach to solving (13) based on alternating updates to W and H (Gillis, 2014) where one variable is fixed while optimise over the other (e.g., alternating nonnegative least squares).
ghbe | By contrast, to test our algorithm, we specifically consider solving (13) as a nonconvex problem in W and H simultaneously3. For our first experiment (Figure 4), we consider a text data application. When comparing text documents, we aim to have a similarity measure that is independent of document length. Indeed, we consider documents similar if they have similar word frequency ratios. This notion of similarity is naturally captured by measuring alignment between vectors, which motivates the use of a loss function based on cosine similarity as
1lkf | D(Y,WH) == > i=1 1 - cos (0(yi, (WH);)),
uobn | (65)
3194 | where 0(yi, (WH);) is the angle between the ith predicted and true document. This loss function only considers the alignment between documents. Indeed, we can write
r22o | cos (0(yi, (WH)¿)) = (Yi, (WH)¿)
nupw | However, using this representation it is clear that, due to the nonnegativity of Y and WH, (65) ranges between 0 and 1. It is also clear that (65) is equivalent to a Euclidean distance with normalisation
37xs | | (WH)¿ || (WH)¿ 2
hoi0 | i=1 Yi |Y ill
tffb | In our second example (Figure 5), we consider (13) with a standard Euclidean distance function (64) and a nonconvex regularisation term R1. Specifically, we consider a version of the smooth clipped absolute deviation regularisation (SCAD) first proposed in Fan & Li (2001). SCAD uses a quadratic function to smoothly interpolate between a regular {1 penalty and a constant penalty
ewwt | |x| < 1,
2tj1 | SCADA,a(x) =
nbhq | a-1 , 1 5 |x| < a>, |x | > al .
w251 | The SCAD penalty reduces the downward bias on large parameters typical of the {1 penalty while still allowing for sparsification of small parameters. We consider a twice smooth clipped absolute deviation, which we call TSCAD. TSCAD replaces the quadratic interpolant with a quartic, Qx,a (x), which allows for a twice continuously differentiable penalty
1vgx | |x| < >, 1 5 |x| < a>, - (a+1)\2 2 ,
yjnk | TSCADA, a (x) = {Qx, a (x),
9zmh | |x| > al .
mi3g | The regularisation term is simply given by
ntjg | RX (W, H) = i,j TSCADA, a (Wij) +> i,j TSCADA,a (Hij).
86at | 45
bpd2 | Inexact Newton-type Methods for Optimisation with Nonnegativity Constraints
hkd8 | Due to the inherent nonconvexity of the NNMF problem, initialisation is key to obtaining good results. We utilised a simple half normal initialisation. Indeed, because the data matrix for each NNMF example (Figures 4 and 5) satisfies <LATEX>0 \leq \mathrm { Y } \leq 1 ,</LATEX> we produced the initialisation by drawing <LATEX>\left( \mathrm { W } _ { 0 } ^ { \prime } \right) _ { i j } \sim \mathcal{N} \left( 0 , 1 \right)</LATEX> and <LATEX>\left( \mathrm { H } _ { 0 } ^ { \prime } \right) _ { i j } \sim \mathcal{N} \left( 0 , 1 \right)</LATEX> and normalising in the following manner
sptg | <LATEX>\mathrm { W } _ { 0 } \leftarrow \frac { | \mathrm { W } _ { 0 } ^ { \prime } | } { \sqrt { \max \left( | \mathrm { W } _ { 0 } ^ { \prime } | | \mathrm { H } _ { 0 } ^ { \prime } | \right) } } , \quad \mathrm { H } _ { 0 } \leftarrow \frac { | \mathrm { H } _ { 0 } ^ { \prime } | } { \sqrt { \max \left( | \mathrm { W } _ { 0 } ^ { \prime } | | \mathrm { H } _ { 0 } ^ { \prime } | \right) } } ,</LATEX> where <LATEX>| \cdot |</LATEX> is taken elementwise. This initialisation was found to result in nontrivial solutions (i.e., visually reasonable low rank representations H) to (13).
2k40 | F.4. Simulations For Fast Local Convergence