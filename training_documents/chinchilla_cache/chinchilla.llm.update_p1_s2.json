{
  "id": "chatcmpl-A3U6IdDenKTXSkgjOvDeqvoTW8b2j",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "``` \n5bq3 | Training Compute-Optimal Large Language Models\nezla | We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over four hundred language models ranging from seventy million to over sixteen billion parameters on five to five hundred billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with seventy billion parameters and four times more data. Chinchilla uniformly and significantly outperforms Gopher, GPT-three, Jurassic-one, and Megatron-Turing NLG on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of sixty-seven point five percent on the MMLU benchmark, greater than a seven percent improvement over Gopher.\nrsbu | One. Introduction\nmagd | Recently a series of Large Language Models have been introduced, with the largest dense language models now having over five hundred billion parameters. These large autoregressive transformers have demonstrated impressive performance on many tasks using a variety of evaluation protocols such as zero-shot, few-shot, and fine-tuning.\ntb74 | The compute and energy cost for training large language models is substantial and rises with increasing model size. In practice, the allocated training compute budget is often known in advance: how many accelerators are available and for how long we want to use them. Since it is typically only feasible to train these large models once, accurately estimating the best model hyperparameters for a given compute budget is critical.\n8l69 | Kaplan et al. showed that there is a power law relationship between the number of parameters in an autoregressive language model and its performance. As a result, the field has been training larger and larger models, expecting performance improvements. One notable conclusion in Kaplan et al. is that large models should not be trained to their lowest possible loss to be compute optimal. Whilst we reach the same conclusion, we estimate that large models should be trained for many more training tokens than recommended by the authors. Specifically, given a ten times increase computational budget, they suggest that the size of the model should increase five point five times while the number of training tokens should only increase one point eight times. Instead, we find that model size and the number of training tokens should be scaled in equal proportions.\n1jyv | Following Kaplan et al. and the training setup of GPT-three, many of the recently trained large models have been trained for approximately three hundred billion tokens, in line with the approach of predominantly increasing model size when increasing compute.\nolce | In this work, we revisit the question: Given a fixed FLOPs budget, how should one trade-off model size and the number of training tokens? To answer this question, we model the final pre-training loss as a function of the number of model parameters, and the number of training tokens. Since the computational budget is a deterministic function of the number of seen training tokens and model parameters, we are interested in minimizing the loss under the constraint:\nerx1 | optimal Number of parameters, optimal Number of tokens equals argmin Loss. \nb3bt | The functions for optimal Number of parameters and optimal Number of tokens describe the optimal allocation of a computational budget. We empirically estimate these functions based on the losses of over four hundred models, ranging from under seventy million to over sixteen billion parameters, and trained on five billion to over four hundred billion tokens - with each model configuration trained for several different training horizons. Our approach leads to considerably different results than that of Kaplan et al. We highlight our results in Figure one and how our approaches differ in Section two.\n0w8m | Based on our estimated compute-optimal frontier, we predict that for the compute budget used to train Gopher, an optimal model should be four times smaller, while being trained on four times more tokens. We verify this by training a more compute-optimal seventy billion model, called Chinchilla, on one point four trillion tokens. Not only does Chinchilla outperform its much larger counterpart, Gopher, but its reduced model size reduces inference cost considerably and greatly facilitates downstream uses on smaller hardware. The energy cost of a large language model is amortized through its usage for inference and fine-tuning. The benefits of a more optimally trained smaller model, therefore, extend beyond the immediate benefits of its improved performance.\nvcu3 | Two. Related Work\nhjzj | Large language models. A variety of large language models have been introduced in the last few years. These include both dense transformer models and mixture-of-expert models. The largest dense transformers have passed five hundred billion parameters. The drive to train larger and larger models is clear-so far increasing the size of language models has been responsible for improving the state-of-the-art in many language modeling tasks. Nonetheless, large language models face several challenges, including their overwhelming computational requirements and the need for acquiring more high-quality training data. In fact, in this work we find that larger, high quality datasets will play a key role in any further scaling of language models.\n```",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725394054,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 1126,
    "prompt_tokens": 3098,
    "total_tokens": 4224
  }
}