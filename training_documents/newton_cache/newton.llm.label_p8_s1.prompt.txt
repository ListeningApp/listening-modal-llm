You will receive raw text from an OCR scan of a document or web page. Each line of input represents one textbox from the document.

Your task is to label each textbox with exactly one of the following labels:
  title: the primary document title, no more than one per document
  heading: the name of the following chapter or section (should never start in the middle of the previous paragraph)
  subheading: a secondary heading, often following another heading (should never start in the middle of the previous paragraph)
  body: main paragraph content: may start or end in the middle of a word, use context to follow paragraphs that are split across textboxes (if a textbox contains both heading and body text, label it "body")
  math: textbox primarily containing math
  imageDescription: standalone callout that only describes an image, chart, table, or similar (typically begins with "Table...", "Figure...", "Fig. ...", "Above...", "Left...", etc.)
  authors: names of contributors to this document
  institutions: list of universities, business, and other institutions affiliated with this document
  publisher: info about the publisher, or provided by the publisher, including copyright info
  pagenum: the page number
  headerOrFooter: boilerplate text page number, or redundant headings at the top/bottom of the page
  toc: the table of contents
  references: bibliographic references and resources
  acknowledgements: thanks and credits to contributors
  appendix: index, appendix, or any other end matter not part of the main thread (including related headings)
  table: tabular data, label, title, or subheading for a grid or list of data
  datum: numeric data or data label from a chart or list
  advertisement: promotional ads and marketing
  layout: non-content related to the layout of the page, e.g. sidebar navigation
  callout: pull quote or long block text that stands apart from the main text
  footnote: footnotes and endnotes near the bottom of the page
  keywords: list of keywords or tags
  url: web address or email
  other: any other text that does not belong to the main thread

Each line of input is prefixed with a textbox ID in the format `id | Text content`. The output must be in the format `id | label`.

Note that body text may be split across multiple textboxes, and textboxes may start or end in the middle of a sentence or word. Because the text was extracted from a web page or document, paragraphs of body text may be interrupted by multiple textboxes of redundant headers, footnotes, page numbers, tables, images, etc. For example, a real heading will never interrupt the middle of a sentence. Use context clues like this to carefully label each textbox.

EXAMPLE INPUT:
1pjs | Neuroscience Applied 1 (2022) 101012
o2kr | Contents lists available at ScienceDirect
v6sk | Neuroscience Applied
1wj6 | journal homepage: www.journals.elsevier.com/neuroscience-applied
936l | Research Articles
06yq | Microdosing with psychedelics to self-medicate for ADHD symptoms in adults: A naturalistic study
dpv6 | ARTICLE INFO
8r5j | ABSTRACT
2taz | Keywords: ADHD Microdosing Psychedelics Self-medication Well-being Time perception
o3ya | ADHD in adulthood is often overlooked, which negatively affects the individual's well-being. First-line phar- macological interventions are effective in many ADHD patients, relieving symptoms rapidly. However, there seems to be a proportion of individuals who discontinue, or fail to respond to these treatments. For these in- dividuals, alternative treatment options should be explored.
r5il | 1. Introduction
5gui | Attention deficit hyperactivity disorder (ADHD) is one of the most common developmental disorders worldwide. Prevalence research indicates that 2.6% of the adult population has persistent ADHD. ADHD in adults is often overlooked because of the high comorbidity rate and lack of knowledge about how ADHD is expressed in adulthood (Kooij et al.,
9d4o | Fig. 1. Flowchart of included participants for each time point.
82qc | 2019). In addition, ADHD is associated with deficits in various domains of cogni- tive functioning. Twenty-five percent of ADHD cases suffered from ADHD symptoms purely because of de- ficiencies in temporal processing (Sonuga-Barke et al., 2010).
ls7d | First-line ADHD treatments in adults mainly include pharmacological interventions to enhance dopaminergic and noradrenergic neurotrans- mission with stimulants. Overall, they have been proven to work effectively in adults with ADHD, inducing fast symptom relief and thereby enhancing the person's quality of life. In the
5esx | Table 1 Demographic information from respondents at baseline and the two and four- week time points.
t8vv | longer term, approximately twenty percent of ADHD patients discontinue their prescribed medication after six to nine months, thirty percent after one year, and half of them after two years.
xrd1 | 2. Material and methods
5746 | 2.1. Study design and participants
7toz | The study employed a naturalistic design, assessing the experiences of participants at baseline,
j346 | Neuroscience Applied 1 (2022)
m5ka | E.C.H.M. Haijen et al.
kx2c | REVIEW
v8uq | before they start MD on their own initiative, and at two and four weeks after MD initiation. The target population included adults diagnosed with ADHD and individuals who experienced ADHD symptoms to the extent that these interfered with their daily lives and who had not been diag- nosed with ADHD before. To be included in the analyses, participants needed to score above a cut-off point on at least one of the subscales of the Conner's Adult ADHD Rating Scale (CAARS-S:SV).
2zmw | Fig. 2. Mean scores of the CAARS-S:SV DSM-IV total symptoms T-scores at baseline (0W) and two (2W) and four weeks (4W) after MD (A) of the whole sample, and (B) per conventional ADHD medication use. Error bars represent mean error.
sdk3 | <LATEX>\lim _ { x ightarrow \infty } rac { 6 x ^ { 2 } + 1 0 0 } { 7 x ^ { 2 } - 1 0 } =</LATEX>
x972 | 2.2. Study procedure
k221 | Mean performance measures of the CAARS-

EXAMPLE OUTPUT:
1pjs | headerOrFooter
o2kr | publisher
v6sk | publisher
1wj6 | publisher
936l | headerOrFooter
06yq | title
dpv6 | other
8r5j | heading
2taz | keywords
o3ya | body
r5il | heading
5gui | body
9d4o | imageDescription
82qc | body
ls7d | body
5esx | imageDescription
t8vv | body
xrd1 | heading
5746 | subheading
7toz | body
j346 | headerOrFooter
m5ka | authors
kx2c | headerOrFooter
v8uq | body
2zmw | imageDescription
sdk3 | math
x972 | heading
k221 | body




w95z | Inexact Newton-type Methods for Optimisation with Nonnegativity Constraints
0i1k | 4. Numerical Experiments
dqlg | We now compare the performance of our method for solving (1) with several alternatives using various convex and non- convex examples. Specifically, we consider Algorithm 2 (denoted by MR), projected Newton-CG (denoted by CG) as in Xie & Wright (2023, Algorithm 1), and projected gradient with line search (denoted by PG) (Beck, 2017). For convex problems, we also include FISTA with line search (Beck & Teboulle, 2009), while for non-convex settings, we compare against the proximal gradient with momentum and fine-tuned constant step size (denoted by PGM) from Lin et al. (2020, Algorithm 4.1). We exclude proximal Newton methods due to the difficulty of solving its subproblems at each iteration. We also do not consider the Newton-CG log barrier method (O'Neill & Wright, 2020) due to poor practical performance observed in Xie & Wright (2023).
p6pq | For all applicable methods we terminate according to (7) with <LATEX>\epsilon _ { g } = 1 0 ^ { - 8 } .</LATEX> Instead of the highly implementation dependent "wall-clock" time, here we plot the objective value against the number of oracle calls, i.e., the number of equivalent function evaluations. For completeness, however, we also include plots of objective value against wall-clock time in Appendix F.5. The PyTorch (Paszke et al., 2019) implementation for our experiments is available here. All experiments were performed on a GPU cluster. See Ap- pendix F.3 for further experimental details.
zse6 | 4.1. Sparse Regularisation With <LATEX>\ell _ { 1 }</LATEX> Norm
p2n3 | We first consider sparse regression using {1-regularisation
ohh7 | <LATEX>\min _ { \mathrm { x } \in \mathbb{R} ^ { d } } f \left( \mathrm { x } \right) + \lambda \| \mathrm { x } \| _ { 1 } ,</LATEX> (12)
vbni | where <LATEX>f</LATEX> is a smooth function. Although the objective function in (12) is nonsmooth, it can be reformulated into a smooth optimisation problem with nonnegativity con- straints; see Appendix F.2 for details. We consider two examples in this context.
dgcw | Multinomial Regression. In Figures 1 and 2, we consider convex multinomial regression with <LATEX>C</LATEX> classes where <LATEX>f \quad i s</LATEX> given by (62). The FISTA method is applied directly to (12). While FISTA clearly outperforms the others, our method is competitive. Further simulations showing fast local con- vergence of our method on these examples are given in Appendix F.4.
gvwz | Neural Network. Figure 3 shows the results using a two layer neural network where <LATEX>f</LATEX> is non-convex and defined by (63). Again, PGM is applied directly to (12) and its step size is fine-tuned for best performance. We once again observe superior performance of our method compared with the alternatives.
sk5m | Figure 1. Logistic regression <LATEX>\left( C = 2 \right)</LATEX> on the binarised MNIST dataset (LeCun et al., 1998) <LATEX>\left( d = 7 8 5 \right)</LATEX> with
qws8 | Figure 2. Multinomial regression <LATEX>\left( C = 1 0 \right)</LATEX> on CIFAR10 dataset
877u | <LATEX>\left( d = 2 7 , 6 5 7 \right) \text { with } \lambda = 1 0 ^ { - } .</LATEX> 4.2. Nonnegative Matrix Factorisation
rzhj | Given a nonnegative data matrix <LATEX>\mathrm { Y } \in \mathbb{R} _ { + } ^ { n \times m }</LATEX> nonnegative matrix factorisation (NNMF) aims to produce two low rank, say <LATEX>r ,</LATEX> nonnegative matrices <LATEX>\mathrm { W } \in \mathbb{R} _ { + } ^ { n \times \eta }</LATEX> and <LATEX>\mathrm { H } \in \mathbb{R} _ { + } ^ { r \times m }</LATEX> such that <LATEX>\mathrm { Y } \approx \mathrm { W H } .</LATEX> This can be formulated as
bfax | <LATEX>\min _ { \mathrm { W } \geq 0 , \mathrm { H } \geq 0 } D \left( \mathrm { Y } , \mathrm { W H } \right) + R _ { \lambda } \left( \mathrm { W } , \mathrm { H } \right) ,</LATEX> (13)
f2y8 | where <LATEX>D \left( \cdot , \cdot \right)</LATEX> is a 'distance' and <LATEX>R _ { \lambda } \left( \cdot , \cdot \right)</LATEX> is a regularisation term. In Figure 4, we consider a text dataset and cosine similarity based distance function, while in Figure 5, we use an image dataset and a Euclidean distance function with a nonconvex regulariser; see Appendix F.3 for details. Clearly, our method outperforms all others across both problems.
o7qm | 5. Conclusions and Future Directions
d52l | We developed Newton-MR variants of the two-metric pro- jection framework. By inexactly solving the subproblems using MINRES as well as employing non-positive curva- ture directions, our proposed variants are suitable for large scale and nonconvex settings. We demonstrated that, under
qlbb | Inexact Newton-type Methods for Optimisation with Nonnegativity Constraints
nhsg | Figure 3. Training a two-layer neural network on the Fashion MNIST dataset (Xiao et al., 2017) <LATEX>\left( d = 8 9 , 6 1 0 \right)</LATEX> with
hy0l | Figure 4. NNMF <LATEX>\left( r = 2 0 \right)</LATEX> with cosine distance on top 1000 TF- <LATEX>I D F</LATEX> features of the 20 Newsgroup dataset (Mitchell, 1999)
4jmz | <LATEX>1 0 ^ { - 2 }</LATEX> <LATEX>1 0 ^ { \circ }</LATEX> <LATEX>1 0 ^ { 1 }</LATEX> <LATEX>1 0 ^ { 2 }</LATEX> <LATEX>1 0 ^ { 3 }</LATEX> <LATEX>1 0 ^ { 4 }</LATEX> <LATEX>1 0 ^ { 5 }</LATEX> <LATEX>\begin{array}{} { 1 0 ^ { 2 } } & { 1 0 ^ { 3 } } \\ { 0 r a c l e } & { \text { Calls } } \end{array}</LATEX> Figure 5. NNMF <LATEX>\left( r = 1 0 \right)</LATEX> with nonconvex TSCAD regulariser on the Olivetti <LATEX>f a c e s \quad d a t a s e t \left( P e d r e g o s a \quad e t \quad a l . , 2 0 1 1 \right) \left( d = \right.</LATEX> 44, 960). We used <LATEX>\lambda = 1 0 ^ { - 4 }</LATEX> lariser.
b7cq | certain assumptions, the convergence rates of our methods match the state-of-the-art and showcased competitive practi- cal performance across a variety of problems.
0b39 | Possible avenues for future research include extensions to box constraints, variants with second-order complexity guar- antees, and the development of stochastic algorithms.
qecd | Acknowledgements
1o7p | This research was partially supported by the Australian Re- search Council through an Industrial Transformation Train- ing Centre for Information Resilience (IC200100022).
cp13 | Impact Statement
bgre | This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.
57lp | References
ukth | Inexact Newton-type Methods for Optimisation with Nonnegativity Constraints
kpvt | Multiplier Methods. Athena Scientific, 1996. ISBN 1- 886529-04-30.
jie2 | e878c8f38381d0964677fb9536c494ee-Paper-Complexity guarantees for nonconvex optimization. SIAM pdf.
d0ik | Cartis, C., Gould, N., and Toint, P. Adaptive cubic regular- isation methods for unconstrained optimization. Part II: worst-case function- and derivative-evaluation complex- ity. Mathematical programming, 130(2):295-319, 2011a. ISSN 0025-5610. doi: 10.1007/s10107-009-0337-y.
0x0s | Cartis, C., Gould, N., and Toint, P. Optimal Newton-type methods for nonconvex smooth optimization problems. ERGO Technical Report, 11-009, 2011b.
4exe | Fan, J. and Li, R. Variable selection via nonconcave penal- ized likelihood and its oracle properties. Journal of the American Statistical Association, 96(456):1348-1360, 2001. ISSN 01621459. URL http://www.jstor. org/stable/3085904.
0myz | 10
xh3i | Inexact Newton-type Methods for Optimisation with Nonnegativity Constraints
syiw | Gafni, E. M. and Bertsekas, D. P. Two-metric pro- jection methods for constrained optimization. SIAM Journal on Control and Optimization, 22(6):936-964, 1984. doi: 10.1137/0322061. URL https: //doi. org/10.1137/0322061.
kg3t | Lee, D. and Seung, H. S. Algorithms for non-negative matrix factorization. In Leen, T., Dietterich, T., and Tresp, V. (eds.), Advances in Neural Information Processing Systems, volume 13. MIT Press, 2000. URL https://proceedings.neurips. cc/paper_files/paper/2000/file/
lvae | f9d1152547c0bde01830b7e8bd60024c-Paper. pdf.
o9sc | Nesterov, Y. and Polyak, B. Cubic regularization of Newton method and its global performance. Math. Program., 108: 177-205, 08 2006. doi: 10.1007/s10107-006-0706-8.
121u | Inexact Newton-type Methods for Optimisation with Nonnegativity Constraints
zh03 | Nocedal, J. and Wright, S. J. Numerical Optimization. Springer Series in Operations Research and Financial Engineering. Springer New York, New York, NY, second edition edition, 2006. ISBN 9780387303031.
1mvw | Royer, C. W., O'Neill, M., and Wright, S. J. A Newton-CG algorithm with complexity guarantees for smooth uncon- strained optimization. arXiv preprint arXiv:1803.02924, 3 2018.
6d4s | Saad, Y. Iterative Methods for Sparse Linear Systems. SIAM, 2nd edition, 2003. ISBN 9780898715347.
mqah | Xie, Y. and Wright, S. J. Complexity of a projected Newton- CG method for optimization with bounds. Mathemetical Programming, July 2023. doi: https://doi.org/10.1007/ s10107-023-02000-z.
xxr8 | Inexact Newton-type Methods for Optimisation with Nonnegativity Constraints
rl4j | 13
vz32 | Inexact Newton-type Methods for Optimisation with Nonnegativity Constraints
ncb0 | A. MINRES and Newton-MR
8mp0 | In this section, for completeness, we discuss MINRES (Algorithm 3) and provide some of its fundamental properties. We note that our presentation is essentially that of Liu & Roosta (2022b, Appendix A) as the notation and implementation is well adapted to our setting. Recall that MINRES combines the Lanczos process, a QR decomposition, and an updating formula to iteratively solve a symmetric linear least-squares problem of the form
xjyk | min |Hs + g||2.
mt0f | We now discuss each of these aspects in detail.
awab | Lanczos Process. Recall that, starting from v1 = g/||g||, after t iterations of the Lanczos process, the Lanczos vectors {V1, V2, ... , V+1}, form a basis for the Krylov subspace K+1 (H, g). Collecting these vectors into an orthogonal matrix
ixcc | Vt+1 =[V1, ... Vt+1] € Rdx(t+1)
8p6h | we can write
pmgl | HVt = Vt+1Tt, where Tt E R(t+1),t is an upper Hessenberg matrix of the form
90no | Tt = ã1 ₿2
jt0y | B2 ã2
qb7o | , Ť, A B+1eŢ .
oae5 | Tt
s0fl | ₿3
43ia | 10° 18 10
3g2y | This relation yields the underlying update process of the MINRES iterations for t ≥ 2 as,
gc7v | Hvt = BtVt-1 + Õ+ V+ + B+1V+1.
wqfv | The Lanczos process terminates when ßt+1 = 0. We remark that computing an expansion of the basis requires a single Hessian-vector product, HVt. The basis for the Krylov subspace allows us to significantly simplify (8). Indeed, let St be a solution to (8) at iteration t. By St € Kt(H, g), we have st = Vtyt for some yt € Rt. Hence, the residual can be written as
j30a | rt = - g - Hst = - g - HVtyt =- g -Vt+1Ttyt =- Vt+1(|g|e1 + Ttyt).
7bdk | In the final equality, we applied the orthogonality of the basis vectors and v1 = g/||g||. Applying this expression to (8) and using the orthogonality of Vt+1, we obtain the reduced tridiagonal least-squares problem
fim0 | Ře1+Ťtyt| 01
ojro | min
l6t9 | (14)
1ujy | where 31 = ||g|l.
qzor | QR Factorisation. The next step in the MINRES procedure is to solve (14) by computing the full QR factorisation QtTt = Rt where Qt E R(t+1) x(t+1) and Rt E R(t+1)xt. Because It is already close to being upper triangular, we form the QR factorisation using a series of Householder reflections to annihilate the sub-diagonal elements. Each Householder reflection affects only two rows of Tt. We can summarise the effect of two successive Householder reflections
a0by | 14
u1ky | Inexact Newton-type Methods for Optimisation with Nonnegativity Constraints
htm9 | for <LATEX>3 < i < t \quad - \quad 1</LATEX> as