{
  "id": "chatcmpl-A3U9lcMI06CyPjC6L6otGYGb9vheV",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "``` \nko3c | LLMs, Turing Tests and Chinese Rooms: the prospects for meaning in Large Language Models\nxek3 | Discussions of artificial intelligence and computational systems in philosophy of mind (and elsewhere) have been shaped to a large extent by two brilliant and highly influential thought-experiments: Alan Turing's Imitation Test for thinking systems and John Searle's Chinese Room Argument. As is extremely well-known, Turing argued for a behavioural test for intelligence. Searle, on the other hand, argued that passing a purely behavioural test, focused at the level of inputs and outputs, could never guarantee that meaning, understanding, or thinking was occurring in a system, since a purely formal, syntactic capacity could underpin the behaviour. In many ways, recent debates about large language models are struggling to move beyond the divide marked by these original, opposing thought-experiments. Thanks to the open access status of some large language models, we all now have, available on any suitable laptop or phone, an extraordinarily powerful example of a system which (at least within certain parameters) is capable of passing a Turing Test. Yet those who feel swayed by Searle's Chinese Room are likely to remain unconvinced that the incredible behavioural prowess displayed by these systems is sufficient to show that LLMs deserve the attribution of any richer predicates.\nbbpr | One way to move this debate forward, stressed in several places already, is to 'look under the hood' of AI systems, asking exactly what (if anything) the elements they manipulate represent and what features are relevant to the computations they perform. While this is certainly an important strategy, and I'll touch on it below, in this paper I want to pursue a different route, exploring the features those sceptical about LLMs might take to ground meaning. In section one I set out some common ground between Believers and Sceptics, concerning how LLMs work and the kind of features a Sceptic is likely to demand from an LLM before she would be willing to treat it as a semantic engine. In sections two and three, I explore the first potential ground of meaning, which requires a robust relation between linguistic signs and external objects or states of affairs. In section three I argue that concerns about worldly connections can be met, so that the outputs of LLMs should be viewed as genuinely meaningful. Then in section four I turn to the idea that LLMs require original intentionality before we admit them to the space of meaning. I suggest that this demand is not a prerequisite for meaning per se, but rather for agency or conscious understanding. I'll suggest\n5zyf | that the demands for original intentionality are not met by LLMs, but that nor should we want them to be.\n9own | Turing Tests, Chinese Rooms and LLMs\nbqfb | Ideas generated in philosophy of mind usually remain within the confines of the ivory tower where they were born, but this is certainly not true of Turing Tests and possibly not true of Chinese Rooms. Still, just to ensure everything is on the table, in this section I'll recap very briefly on the two thought-experiments which have done so much to shape understanding in this field.\nwwdt | Alan Turing proposed what he called 'the imitation game' in his nineteen fifty paper 'Computing machinery and intelligence' as an operationalized method to replace the question \"Can machines think?\". Turing's starting point was the recognition that terms like \"thinking\" and \"intelligence\" are difficult to define and are prone to yield divergent interpretations in different contexts. Instead, then, he proposed replacing questions framed in these vague terms with an investigation into a much more tractable question: \"Can a machine pass the imitation test?\". In the imitation test, a human subject poses questions to an unknown target, via text, and receives answers in the same way. If, given a reasonable length of conversation, the human subject is unable to tell whether they are interacting with another human or with a machine, then, if they are in fact interacting with a machine, that machine is judged to pass the imitation test. While Turing doesn't in fact say very much about the repercussions of passing such a test, the suggestion seems to be that, for all intents and purposes, a successful machine would qualify as a thinking thing. The only other option, Turing seems to think, would be adopting some kind of solipsistic condition on thinking, requiring first-personal conscious awareness, so that we judge thinking to be going on only when we 'know it from the inside'. However, as Turing points out, this solipsistic perspective is not the one we adopt in everyday life, instead he notes that \"it is usual to have the polite convention that everyone thinks\". A similar courtesy, it seems, should be extended to behaviourally adept machines.\nmn1r | In Searle's famous thought experiment from nineteen eighty, however, behavioural success of the kind envisaged by Turing is held to be insufficient for attribution of rich predicates like thinking or meaning to a system. For Searle objects that success at the level of outputs could come from a system which did nothing more than manipulate symbols in terms of their formal, syntactic properties, with no consideration of semantic content, or meaning, occurring within the system.\n5vqw | To see this, he asks us to consider a sealed room with a human subject in it. The subject receives what appear to her to be meaningless marks via an input slot. She then uses a giant look-up table to pair the incoming marks with a further set of marks and she outputs this further set of apparently meaningless marks via an output slot. Unbeknownst to the subject, however, the incoming symbols are questions in Chinese and the series of marks which she outputs constitute answers to these questions (again, in Chinese). Searle asks us to consider whether the individual in the room knows or understands Chinese and the intuitive answer, despite her impressive performance with Chinese questions, is that she does not. Yet, Searle contends, as for the individual in the Chinese Room, so for the computer. Both systems are purely syntactic engines with no semantic properties in play.\nmeyw | So, what are we to say of LLMs? All parties agree that Large Language Models are trained for the task of minimizing prediction errors when deciding on the most probable next word in a sequence. Crudely, faced with the sentence \"Emily paid in her cheques at the _\", the task of an LLM is to fill in the blank with the most probable next word, e.g. completing this sentence with the expression \"bank\". In order to carry out this task, LLMs utilize what is known as transformer architecture, which allows expressions of a natural language to be mapped into vectors within a multidimensional vector space. During pre-training, the system is provided with an almost unimaginably huge dataset (hundreds of billions to trillions of token words) which it uses to learn the statistical relationships between linguistic items in the database. The system does this by repeatedly masking words in sentences within its database and 'making guesses' (initially entirely randomly) about what the masked word is. Once the correct word is revealed the system then recalibrates the vectors it assigns to words in light of this information. The vector assigned to each word is thus determined by the statistical correlations between expressions across the entire training data. Following this unsupervised learning, there is then a period of human feedback which helps the system fine-tune its guesses for new input strings and then recalibrate resulting vector weights. Finally, vectors are also moderated by the correlations in the input strings themselves, so that, e.g., the vector assigned to a token of \"dog\" will differ dependent on whether the input string to the LLM is \"The big dog ... \" or \"The barking dog ... \", etc.\n```",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725394269,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 1609,
    "prompt_tokens": 3356,
    "total_tokens": 4965
  }
}