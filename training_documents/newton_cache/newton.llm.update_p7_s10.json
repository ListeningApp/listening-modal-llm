{
  "id": "chatcmpl-A3UEJ65l4kfEp9dVzKpznZEuPVCTw",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "q0se | An advantage of the original TMP method of Bertsekas is that we get fast local convergence, a property that is shared by many Newton-type methods. We now show that our algorithm, in a slightly modified form, also exhibits this property. The basis for the local convergence is the fact that, under certain conditions, projected gradient algorithms are capable of identifying the true set of active constraints in a finite number of iterations. This result was first established for projected gradient with bound constraints in Bertsekas but has been extended to a variety of constraints. In the case of two-metric projection, once the active set is identified, the combined step reduces to an\ntz7u | unconstrained Newton step in the inactive set.\njszk | For the analysis, we consider a \"local phase\" variant of Algorithm one. Specifically, we maintain flexibility in defining the outer and inner termination conditions and tolerances, eliminate the strongly positive curvature validation, and only perform backtracking line search from alpha sub zero equals one to ensure the step length remains bounded. The pseudo-code for this local phase version is given in Algorithm four for completeness. To show that the active set is identified in a finite number of iterations, we need non-degeneracy and second-order sufficiency assumptions, which are standard in this context.\negkh | Assumption three point eleven. A local minima, X sub star, is non-degenerate if\nkbpy | left bracket g of X sub star right bracket super i is greater than zero, for all i in script A of X sub star comma zero. Assumption three point twelve. A local minima, X sub star, satisfies the second-order sufficiency condition if zero is less than inner product of Z and H of X sub star and Z for all Z not equal to zero such that Z super i equals zero if\nafdp | i in script A of X sub star comma zero. Theorem three point thirteen (Active Set Identification). Let f satisfy Assumption three point one and x plus be a local minima satisfying Assumptions three point eleven and three point twelve. Let the sequence of iterates generated by Algorithm four with delta chosen according to equation forty-four. There exists delta sub acrv greater than zero such that if X sub bar k in B of X sub star comma delta sub acrv, then script A of X sub k comma delta equals script A of X sub k comma zero equals script A of X sub star comma zero all\n5jom | k is greater than or equal to bar k plus one. We defer the proof to Appendix E. Once the active set is identified, our method reduces to unconstrained Newton-MR on the inactive set. Local convergence is therefore a simple corollary of Theorem three point thirteen.\nfdhb | Corollary three point fourteen (Local Convergence). For k is greater than or equal to bar k plus one (cf. Theorem three point thirteen), the convergence of Algorithm four is driven by the local properties of the Newton-MR portion of the step.\no1k9 | Remark three point fifteen. The local convergence of Newton-MR is similar to that of other inexact Newton methods. Suppose that we use a relative residual tolerance, norm r super script I norm less than or equal to eta norm g super script I norm, as the criteria for the MINRES termination. Under Assumption three point twelve, we know that H of X sub star is positive definite on the inactive indices. Therefore, by applying Nocedal and Wright, we obtain a superlinear convergence if we choose eta equals script O of one and let X sub k be close enough to X sub star. If we choose eta equals script O of norm g sub k norm and the Hessian is Lipschitz then we can improve the rate to quadratic.\ns1uu | Remark three point sixteen. A central ingredient in the projected Newton-CG of Xie and Wright is the damping of the Hessian in the form of diagonal perturbation (i.e., H plus epsilon I for all Newton-CG steps in the inactive set. While this facilitates an optimal global complexity, an unfortunate consequence, at least in theory, is that the algorithm no longer enjoys a guaranteed fast Newton-type local convergence rate. In other words, one can at best show linear rates in local regimes.\n22hf | Inexact Newton-type Methods for Optimization with Nonnegativity Constraints\ngici | Four. Numerical Experiments\nowaw | We now compare the performance of our method for solving one with several alternatives using various convex and non-convex examples. Specifically, we consider Algorithm two (denoted by MR), projected Newton-CG (denoted by CG) as in Xie and Wright, and projected gradient with line search (denoted by PG). For convex problems, we also include FISTA with line search, while for non-convex settings, we compare against the proximal gradient with momentum and fine-tuned constant step size (denoted by PGM) from Lin et al. We exclude proximal Newton methods due to the difficulty of solving its subproblems at each iteration. We also do not consider the Newton-CG log barrier method due to poor practical performance observed in Xie and Wright.",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725394551,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 1059,
    "prompt_tokens": 3343,
    "total_tokens": 4402
  }
}