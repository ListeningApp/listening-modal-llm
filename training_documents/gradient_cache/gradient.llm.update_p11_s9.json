{
  "id": "chatcmpl-A3U7PDMnra4UQSwix1wMtV8PB7GM1",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "vvqn | The influence of the training set size was measured by training the network with fifteen thousand, thirty thousand, and sixty thousand examples. The resulting training error and test error are shown in the figure. It is clear that, even with specialized architectures such as LeNet-five, more training data would improve the accuracy.\np6xg | To verify this hypothesis, we artificially generated more training examples by randomly distorting the original training images. The increased training set was composed of the sixty thousand original patterns plus five hundred forty thousand instances of distorted patterns with randomly picked distortion parameters.\nbppc | The distortions were combinations of the following planar affine transformations: horizontal and vertical translations; scaling; squeezing (simultaneous horizontal compression and vertical elongation, or the reverse); and horizontal shearing. The figure shows examples of distorted patterns used for training. When distorted data were used for training, the test error rate dropped to zero point eight percent (from zero point ninety-five percent without deformation). The same training parameters were used as without deformations. The total length of the training session was left unchanged (twenty passes of sixty thousand patterns each). It is interesting to note that the network effectively sees each individual sample only twice over the course of these twenty passes.\nilnw | C. Comparison with Other Classifiers\nf3rc | For the sake of comparison, a variety of other trainable classifiers was trained and tested on the same database. An early subset of these results was presented. The error rates on the test set for the various methods are shown in the figure.\nd7bx | One) Linear Classifier and Pairwise Linear Classifier: Possibly the simplest classifier that one might consider is a linear classifier. Each input pixel value contributes to a weighted sum for each output unit. The output unit with the highest sum (including the contribution of a bias constant) indicates the class of the input character. On the regular data, the error rate is twelve percent. The network has seven thousand eight hundred fifty free parameters. On the deslanted images, the test error rate is eight point four percent. The network has four thousand ten free parameters. The deficiencies of the linear classifier are well documented, and it is included here simply to form a basis of comparison for more sophisticated classifiers. Various combinations of sigmoid units, linear units, gradient descent learning, and learning by directly solving linear systems gave similar results.\ngx97 | A simple improvement of the basic linear classifier was tested. The idea is to train each unit of a single-layer network to separate each class from each other class. In our case this layer comprises forty-five units labeled zero/one, zero/two, ... , zero/nine, one/two, ... , eight/nine. Unit i/j is trained to produce plus one on patterns of class i, negative one on patterns of class j, and it is not trained on other patterns. The final score for class i is the sum of the outputs all the units labeled i/x minus the sum of the output of all the units labeled y/i, for all x and y. The error rate on the regular test set was seven point six percent.\nppkp | Two) Baseline Nearest Neighbor Classifier: Another simple classifier is a K-N-N classifier with a Euclidean distance measure between input images. This classifier has the advantage that no training time, and no thought on the part of the designer, are required. However the memory requirement and recognition time are large: the complete sixty thousand twenty by twenty pixel training images (about twenty-four megabytes at one byte per pixel) must be available at run time. Much\n8br6 | more compact representations could be devised with modest increase in error rate. On the regular test set the error rate was five point zero percent. On the deslanted data, the error rate was two point four percent, with k equals three. Naturally, a realistic Euclidean distance nearest-neighbor system would operate on feature vectors\nde3k | rather than directly on the pixels, but since all of the other systems presented in this study operate directly on the pixels, this result is useful for a baseline comparison.\n2kb3 | Three) P-C-A and Polynomial Classifier: Following, a preprocessing stage was constructed which computes the projection of the input pattern on the forty principal components of the set of training vectors. To compute the principal components, the mean of each input component was first computed and subtracted from the training vectors. The covariance matrix of the resulting vectors was then computed and diagonalized using singular value decomposition. The forty-dimensional feature vector was used as the input of a second degree polynomial classifier. This classifier can be seen as a linear classifier with eight hundred twenty-one inputs, preceded by a module that computes all products of pairs of input variables. The error on the regular test set was three point three percent.\nlqcx | Four) RBF Network: Following, an RBF network was constructed. The first layer was composed of one thousand Gaussian RBF units with twenty-eight by twenty-eight inputs, and the second layer was a simple one thousand inputs/ten outputs linear classifier. The RBF units were divided into ten groups of one hundred. Each group of units was trained on all the training examples of one of the ten classes using the adaptive K-means algorithm. The second-layer weights were computed using a regularized pseudoinverse method. The error rate on the regular test set was three point six percent.\n3h7w | Five) One-Hidden-Layer Fully Connected Multilayer NN: Another classifier that we tested was a fully connected multilayer NN with two layers of weights (one hidden layer) trained with the version of back-propagation described in Appendix C. Error on the regular test set was four point seven percent for a network with three hundred hidden units and four point five percent for a network with one thousand hidden units. Using artificial distortions to generate more training data brought only marginal improvement: three point six percent for three hundred hidden units and three point eight percent for one thousand hidden units. When deslanted images were used, the test error jumped down to one point six percent for a network with three hundred hidden units.\nth5p | It remains somewhat of a mystery that networks with such a large number of free parameters manage to achieve reasonably low testing errors. We conjecture that the dynamics of gradient descent learning in multilayer nets has a \"self-regularization\" effect. Because the origin of weight space is a saddle point that is attractive in almost every direction, the weights invariably shrink during the first few epochs (recent theoretical analysis seem to confirm this). Small weights cause the sigmoids to operate in the quasi-linear region, making the network essentially equivalent to a low-capacity, single-layer network. As the learning proceeds the weights grow, which\nb7nk | progressively increases the effective capacity of the network. This seems to be an almost perfect, if fortuitous, implementation of Vapnik's \"structural risk minimization\" principle. A better theoretical understanding of these phenomena, and more empirical evidence, are definitely needed.",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725394123,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 1433,
    "prompt_tokens": 3188,
    "total_tokens": 4621
  }
}