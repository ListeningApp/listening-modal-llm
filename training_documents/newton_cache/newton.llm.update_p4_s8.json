{
  "id": "chatcmpl-A3UEKF2x6f5CDI2U79cne9KW9209A",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "rwty | Definition two point one Optimal Point. A point, X, is called approximate first-order optimal if\nki97 | G i is greater than or equal to negative V E for all i in A(x, V E) equation seven a\n099j | absolute value of the diagonal of x A times G four is less than or equal to E, equation seven b\nm7zf | absolute value of G i is less than or equal to E. equation seven c\n2ghw | We take equations seven a and seven b to be trivially satisfied if A(x, V E) is the empty set and similar for equation seven c if I(x, V E) is the empty set.\nhnfy | This definition has been shown to be asymptotically exact. Lemma two point two. Suppose that E K is greater than zero and we have a sequence {X K} where each XK satisfies the corresponding E x-FO optimality condition. If X K converges to X*, then XK satisfies equation six.\nw6d9 | Three. Newton-M R Two-Metric Projection\nlbm8 | We now propose and theoretically study our extensions of the T M P framework, which involves simultaneously employing gradient and inexact Newton steps, which are respectively restricted to the active and inactive sets.\nkrjy | Three point one. M I N R E S and Its Properties\nn7dw | The inexact Newton step is based on the recently proposed Newton-M R framework, where instead of C G, subproblems are approximately solved using the minimum residual M I N R E S method. Recall that the t-th iteration of M I N R E S is formulated as\nfr45 | s of t equals arg min absolute value of H s plus G squared. S in K t of H, g\ns2ty | equation eight\n3cdb | where K t of H, g equals Span of g, H g, and so on to H t minus one g, is the Krylov subspace of degree t generated from H and g. On each iteration M I N R E S minimizes the squared norm of the residual of the Newton system over the corresponding Krylov subspace. Note that, from an optimization perspective, the residual itself can be viewed as the gradient of the second-order Taylor approximation typically considered by second-order methods, that is, r approximately equals negative H s minus g equals negative gradient of s in g, s plus s, H s. This highlights an advantage of M I N R E S over C G. Indeed, unlike C G, which aims to minimize the second-order Taylor approximation, minimization of the residual norm remains well defined even if H is indefinite. For more theoretical and empirical comparisons between C G and M I N R E S, see Lim et al. twenty twenty-four.\n6hsm | Recently, Liu and Roosta established several properties of M I N R E S that make it particularly well-suited for nonconvex settings. For example, to assess the availability of a non-positive curvature direction in M I N R E S, one merely needs to monitor the condition\nom5d | r of t minus one, H r of t minus one is less than or equal to zero, equation nine\n0svq | This condition is shown to be both necessary and sufficient for the existence of non-positive curvature directions in K t of H, g. In addition, M I N R E S enjoys a natural termination condition in non-convex settings. More specifically, for any user specified tolerance n greater than zero, the termination condition\n1too | H r of t minus one is less than absolute value H s of t minus one, equation ten\npti9 | is satisfied at some iteration. Note that the left hand side, H r of t minus one, is simply the residual of the normal equation H two s equals negative H g. Condition ten is particularly appealing in non-convex settings where we might have g not in the range of H and therefore r is not zero for all s in R d. In this case, a more typical termination condition r of t minus one is less than n may never be satisfied for a given n greater than zero. By contrast, condition ten is applicable in all situations since H r of t minus one is guaranteed to monotonically decrease to zero, while absolute value H s of t minus one is monotonically increasing. Remarkably, both conditions nine and ten can be computed with a scalar update directly from the M I N R E S iterates without any additional Hessian-vector products; see Lemma A point one.\n4mtx | A Newton-M R step is computed by running M I N R E S until condition nine is detected, in which case r of t minus one is returned. Since r of t minus one is a non-positive curvature direction, we label this\nrgxy | Inexact Newton-type Methods for Optimization with Non-negativity Constraints\nvsq9 | case as a \"non-positive curvature\" step. Otherwise, when the termination condition ten is satisfied, S of t minus one is returned. This step serves as an approximate solution to equation eight and so we label this case as a \"solution\" step. Let p denote the direction returned by negative curvature detecting M I N R E S. Liu and Roosta shows that p serves as a direction of first and second-order descent for the function f, namely p, g is less than zero and p, g plus p, H p divided by two is less than zero, as well as a direction of non-ascent for the norm of its gradient, that is, p, H g is less than zero for a solution step and p, H g equals zero for a non-positive curvature step.\nd5a1 | We include the full M I N R E S algorithm as well as some additional properties in Appendix A.\nqtb8 | Three point two. Global Convergence: Minimal Assumptions",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725394552,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 1220,
    "prompt_tokens": 3136,
    "total_tokens": 4356
  }
}