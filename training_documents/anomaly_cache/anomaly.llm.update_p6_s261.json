{
  "id": "chatcmpl-A3UPXLZ1dXmCBXAZV4mkIwvsdFem9",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "ccmb | Even though MVT two D has established itself as a standard benchmark in the past, this dataset (especially the textures) is easily solvable for recent methods, and differences are mainly in the sub-percent range, which is only a minor difference in terms of the comparatively small size of the dataset. In the following, we focus on the newer, more challenging MVT three D dataset where the normal data shows\nma93 | more variance and anomalies only partly occur in one of the two data modalities, R G B and three D.\nc6hh | The results for individual classes of MVT three D grouped by data modality are given in Table three. We are able to outperform all previous methods for all data modalities regarding the average of all classes by a large margin of five point one percent for three D, five percent for R G B and seven point two percent for the combination. Facing the individual classes and data domains, we set a new state-of-the-art in twenty-one of thirty cases. Note that this data set is much more challenging when comparing the best results from previous work (ninety-nine point one percent for MVT two D vs. eighty-six point five percent AUROC for MVT three D). Nevertheless, we detect defects in seven out of ten cases for R G B plus three D at an AUROC of at least ninety-three percent, which demonstrates the robustness of our method. In contrast, the nearest-neighbor approach PatchCore, which provides comparable performance to us on MVT two D, struggles with the increased demands of the dataset and is outperformed by eleven percent on R G B. The same applies for the three D extension using FPFH despite using a foreground mask as well. Figure one shows qualitative results for the R G B plus three D case given both inputs and ground truth annotations. More ex-\ny7ea | amples can be found in the supplemental material. Despite the low resolution, the regions of the anomaly can still be localized well for practical purposes. Table four reports the pixel-AUROC of our method and previous work.\nnav4 | For the class peach in the R G B plus three D setting, the top of Figure five compares the distribution of student-teacher distances for anomalous and normal regions. The distribution of anomalous samples shows a clear shift towards larger distances. At the bottom of Figure five, the outputs of student and teacher as well as our the distance of corresponding pairs representing our anomaly score are visualized by a random orthographic two D projection. Note that visualizations made by techniques such as t-SNE or P C A are not meaningful here, since the teacher outputs (and therefore most of the student outputs) follow an isotropic standard normal distribution. Therefore, different random projections barely differ qualitatively.\ngem8 | Four point four point two Ablation Studies\ne8nr | We demonstrate the effectiveness of our contributions and design decisions with several ablation studies. Table five compares the performance of variants of students with the teacher, which can be used as a density estimator itself for anomaly detection by using its likelihoods, given by Equation two, as anomaly score. In comparison, a symmetric student-teacher pair worsens the results by one to two percent, excepting the R G B case. However, the performance is already improved for R G B and three D plus R G B by creating the asymmetry with a deeper version of the student than the teacher by doubling the number of coupling blocks to eight. This effect is further\nk78w | enhanced if the architecture of the N F-teacher is replaced by a conventional feedforward network as we suggest. We also vary the depth of our student network and analyzed its relation to performance, model size and inference time in Table six. With an increasing number of residual blocks N S T underscore blocks, we observe an increasing performance which is almost saturated after four blocks. Since the remaining potential in detection performance is not in relation to the linearly increasing additional computational effort per block, we suggest to choose four blocks to have a good trade-off.\ng9du | In Table seven we investigate the impact of the positional encoding and the foreground mask. For MVT three D, positional encoding improves the detection by one point four percent of our AST-pair when trained with three D data as the only input. Even though the effect is not present when combining both data modalities, we consider it generally reasonable to use the positional encoding, considering that the integration with just thirty-two additional channels does not significantly increase the computational effort.\nf682 | Foreground extraction in order to mask the loss for training and anomaly score for testing is also highly effective. Since the majority of the image area often consists of background, the teacher has to spend a large part of the distribution on the background. Masking allows the teacher and student to focus on the essential structures. Moreover, noisy\nli1k | background scores are eliminated.\npbea | Five. Conclusion\nccpt | We discovered the generalization problem of previous student teacher pairs for A D and introduced an alternative student-teacher method that prevents this issue by using a highly different architecture for student and teacher. We were able to compensate for skewed likelihoods of a normalizing flow-based teacher, which was used directly for detection in previous work, by the additional use of a student. Future work could extend the approach to more data domains and improve the localization resolution.",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725395247,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 1091,
    "prompt_tokens": 2837,
    "total_tokens": 3928
  }
}