You will receive raw text from an OCR scan of a document or web page. Each line of input represents one textbox from the document.

Your task is to label each textbox with exactly one of the following labels:
  title: the primary document title, no more than one per document
  heading: the name of the following chapter or section (should never start in the middle of the previous paragraph)
  subheading: a secondary heading, often following another heading (should never start in the middle of the previous paragraph)
  body: main paragraph content: may start or end in the middle of a word, use context to follow paragraphs that are split across textboxes (if a textbox contains both heading and body text, label it "body")
  math: textbox primarily containing math
  imageDescription: standalone callout that only describes an image, chart, table, or similar (typically begins with "Table...", "Figure...", "Fig. ...", "Above...", "Left...", etc.)
  authors: names of contributors to this document
  institutions: list of universities, business, and other institutions affiliated with this document
  publisher: info about the publisher, or provided by the publisher, including copyright info
  pagenum: the page number
  headerOrFooter: boilerplate text page number, or redundant headings at the top/bottom of the page
  toc: the table of contents
  references: bibliographic references and resources
  acknowledgements: thanks and credits to contributors
  appendix: index, appendix, or any other end matter not part of the main thread (including related headings)
  table: tabular data, label, title, or subheading for a grid or list of data
  datum: numeric data or data label from a chart or list
  advertisement: promotional ads and marketing
  layout: non-content related to the layout of the page, e.g. sidebar navigation
  callout: pull quote or long block text that stands apart from the main text
  footnote: footnotes and endnotes near the bottom of the page
  keywords: list of keywords or tags
  url: web address or email
  other: any other text that does not belong to the main thread

Each line of input is prefixed with a textbox ID in the format `id | Text content`. The output must be in the format `id | label`.

Note that body text may be split across multiple textboxes, and textboxes may start or end in the middle of a sentence or word. Because the text was extracted from a web page or document, paragraphs of body text may be interrupted by multiple textboxes of redundant headers, footnotes, page numbers, tables, images, etc. For example, a real heading will never interrupt the middle of a sentence. Use context clues like this to carefully label each textbox.

EXAMPLE INPUT:
1pjs | Neuroscience Applied 1 (2022) 101012
o2kr | Contents lists available at ScienceDirect
v6sk | Neuroscience Applied
1wj6 | journal homepage: www.journals.elsevier.com/neuroscience-applied
936l | Research Articles
06yq | Microdosing with psychedelics to self-medicate for ADHD symptoms in adults: A naturalistic study
dpv6 | ARTICLE INFO
8r5j | ABSTRACT
2taz | Keywords: ADHD Microdosing Psychedelics Self-medication Well-being Time perception
o3ya | ADHD in adulthood is often overlooked, which negatively affects the individual's well-being. First-line phar- macological interventions are effective in many ADHD patients, relieving symptoms rapidly. However, there seems to be a proportion of individuals who discontinue, or fail to respond to these treatments. For these in- dividuals, alternative treatment options should be explored.
r5il | 1. Introduction
5gui | Attention deficit hyperactivity disorder (ADHD) is one of the most common developmental disorders worldwide. Prevalence research indicates that 2.6% of the adult population has persistent ADHD. ADHD in adults is often overlooked because of the high comorbidity rate and lack of knowledge about how ADHD is expressed in adulthood (Kooij et al.,
9d4o | Fig. 1. Flowchart of included participants for each time point.
82qc | 2019). In addition, ADHD is associated with deficits in various domains of cogni- tive functioning. Twenty-five percent of ADHD cases suffered from ADHD symptoms purely because of de- ficiencies in temporal processing (Sonuga-Barke et al., 2010).
ls7d | First-line ADHD treatments in adults mainly include pharmacological interventions to enhance dopaminergic and noradrenergic neurotrans- mission with stimulants. Overall, they have been proven to work effectively in adults with ADHD, inducing fast symptom relief and thereby enhancing the person's quality of life. In the
5esx | Table 1 Demographic information from respondents at baseline and the two and four- week time points.
t8vv | longer term, approximately twenty percent of ADHD patients discontinue their prescribed medication after six to nine months, thirty percent after one year, and half of them after two years.
xrd1 | 2. Material and methods
5746 | 2.1. Study design and participants
7toz | The study employed a naturalistic design, assessing the experiences of participants at baseline,
j346 | Neuroscience Applied 1 (2022)
m5ka | E.C.H.M. Haijen et al.
kx2c | REVIEW
v8uq | before they start MD on their own initiative, and at two and four weeks after MD initiation. The target population included adults diagnosed with ADHD and individuals who experienced ADHD symptoms to the extent that these interfered with their daily lives and who had not been diag- nosed with ADHD before. To be included in the analyses, participants needed to score above a cut-off point on at least one of the subscales of the Conner's Adult ADHD Rating Scale (CAARS-S:SV).
2zmw | Fig. 2. Mean scores of the CAARS-S:SV DSM-IV total symptoms T-scores at baseline (0W) and two (2W) and four weeks (4W) after MD (A) of the whole sample, and (B) per conventional ADHD medication use. Error bars represent mean error.
sdk3 | <LATEX>\lim _ { x ightarrow \infty } rac { 6 x ^ { 2 } + 1 0 0 } { 7 x ^ { 2 } - 1 0 } =</LATEX>
x972 | 2.2. Study procedure
k221 | Mean performance measures of the CAARS-

EXAMPLE OUTPUT:
1pjs | headerOrFooter
o2kr | publisher
v6sk | publisher
1wj6 | publisher
936l | headerOrFooter
06yq | title
dpv6 | other
8r5j | heading
2taz | keywords
o3ya | body
r5il | heading
5gui | body
9d4o | imageDescription
82qc | body
ls7d | body
5esx | imageDescription
t8vv | body
xrd1 | heading
5746 | subheading
7toz | body
j346 | headerOrFooter
m5ka | authors
kx2c | headerOrFooter
v8uq | body
2zmw | imageDescription
sdk3 | math
x972 | heading
k221 | body




06ht | Layer F6 contains 84 units (the reason for this number comes from the design of the output layer, explained below) and is fully connected to C5. It has 10 164 trainable parameters.
v3r2 | As in classical NN's, units in layers up to F6 compute a dot product between their input vector and their weight vector, to which a bias is added. This weighted sum, denoted aj for unit i, is then passed through a sigmoid
nv42 | squashing function to produce the state of unit i, denoted by xi
xags | Xi = f(ai). (5)
jc47 | The squashing function is a scaled hyperbolic tangent
ko3c | f(a) = Atanh(Sa)
xek3 | (6)
bbpr | where A is the amplitude of the function and S determines its slope at the origin. The function f is odd, with horizontal asymptotes at +A and - A. The constant A is chosen to be 1.7159. The rationale for this choice of a squashing function is given in Appendix A.
5zyf | Finally, the output layer is composed of Euclidean RBF units, one for each class, with 84 inputs each. The outputs of each RBF unit y; is computed as follows:
9own | yi = (P; - Wij) 2. (7)
bqfb | In other words, each output RBF unit computes the Eu- clidean distance between its input vector and its parameter vector. The further away the input is from the parameter vector, the larger the RBF output. The output of a particular RBF can be interpreted as a penalty term measuring the fit between the input pattern and a model of the class associated with the RBF. In probabilistic terms, the RBF output can be interpreted as the unnormalized negative log-likelihood of a Gaussian distribution in the space of configurations of layer F6. Given an input pattern, the loss function should be designed so as to get the configuration of F6 as close as possible to the parameter vector of the RBF that corresponds to the pattern's desired class. The parameter vectors of these units were chosen by hand and kept fixed (at least initially). The components of those parameters vectors were set to -1 or +1. While they could have been chosen at random with equal probabilities for -1 and +1, or even chosen to form an error correcting code as suggested by [47], they were instead designed to represent a stylized image of the corresponding character class drawn on a 7x12 bitmap (hence the number 84). Such a representation is not particularly useful for recognizing isolated digits, but it is quite useful for recognizing strings of characters taken from the fully printable ASCII set. The rationale is that characters that are similar, and therefore confusable, such as uppercase "O," lowercase "o," and zero, lowercase "I" digit one, and square brackets and uppercase "I," will have similar output codes. This is particularly useful if the system is combined with a linguistic post- processor that can correct such confusions. Because the codes for confusable classes are similar, the output of the corresponding RBF's for an ambiguous character will be similar, and the postprocessor will be able to pick the appropriate interpretation. Fig. 3 gives the output codes for the full ASCII set.
wwdt | Another reason for using such distributed codes, rather than the more common "1 of N" code (also called place code or grandmother cell code) for the outputs is that nondistributed codes tend to behave badly when the number
mn1r | Fig. 3. Initial parameters of the output RBF's for recognizing the full ASCII set.
5vqw | of classes is larger than a few dozen. The reason is that output units in a nondistributed code must be off most of the time. This is quite difficult to achieve with sigmoid units. Yet another reason is that the classifiers are often used not only to recognize characters, but also to reject noncharacters. RBF's with distributed codes are more appropriate for that purpose because unlike sigmoids, they are activated within a well-circumscribed region of their input space, outside of which nontypical patterns are more likely to fall.
meyw | The parameter vectors of the RBF's play the role of target vectors for layer F6. It is worth pointing out that the components of those vectors are +1 or -1, which is well within the range of the sigmoid of F6, and therefore prevents those sigmoids from getting saturated. In fact, +1 and -1 are the points of maximum curvature of the sigmoids. This forces the F6 units to operate in their maximally nonlinear range. Saturation of the sigmoids must be avoided because it is known to lead to slow convergence and ill-conditioning of the loss function.
2e62 | C. Loss Function
75y3 | The simplest output loss function that can be used with the above network is the maximum likelihood estimation criterion, which in our case is equivalent to the minimum mean squared error (MSE). The criterion for a set of training samples is simply
s2g2 | E(W)=>>UDP(ZP,W) p=1 P
q1q2 | (8)
cb7r | where yDp is the output of the Dpth RBF unit, i.e., the one that corresponds to the correct class of input pattern ZP. While this cost function is appropriate for most cases, it lacks three important properties. First, if we allow the parameters of the RBF to adapt, E(W) has a trivial, but totally unacceptable, solution. In this solution, all the RBF parameter vectors are equal and the state of F6 is constant and equal to that parameter vector. In this case the network happily ignores the input, and all the RBF outputs are equal
68xu | to zero. This collapsing phenomenon does not occur if the RBF weights are not allowed to adapt. The second problem is that there is no competition between the classes. Such a competition can be obtained by using a more discriminative training criterion, dubbed the maximum a posteriori (MAP) criterion, similar to maximum mutual information criterion sometimes used to train HMM's [48]-[50]. It corresponds to maximizing the posterior probability of the correct class Dp (or minimizing the logarithm of the probability of the correct class), given that the input image can come from one of the classes or from a background "rubbish" class label. In terms of penalties, it means that in addition to pushing down the penalty of the correct class like the MSE criterion, this criterion also pulls up the penalties of the incorrect classes
hd35 | E(W)=> p=1 UDP(ZP,W)
2zpk | - -
epu7 | + log (e-3 + )e-yi (Z",W)
7od1 | (9)
0bqy | The negative of the second term plays a "competitive" role. It is necessarily smaller than (or equal to) the first term, therefore this loss function is positive. The constant j is positive and prevents the penalties of classes that are already very large from being pushed further up. The posterior probability of this rubbish class label would be the ratio of e-j and e- + Li e-yi (ZP,W). This discriminative criterion prevents the previously mentioned "collapsing effect" when the RBF parameters are learned because it keeps the RBF centers apart from each other. In Section VI, we present a generalization of this criterion for systems that learn to classify multiple objects in the input (e.g., characters in words or in documents).
b780 | Computing the gradient of the loss function with respect to all the weights in all the layers of the convolutional network is done with back propagation. The standard al- gorithm must be slightly modified to take account of the
ccmb | weight sharing. An easy way to implement it is to first compute the partial derivatives of the loss function with respect to each connection, as if the network were a conventional multilayer network without weight sharing. Then the partial derivatives of all the connections that share a same parameter are added to form the derivative with respect to that parameter.
ma93 | Such a large architecture can be trained very efficiently, but doing so requires the use of a few techniques that are described in the appendixes. Appendix A describes details such as the particular sigmoid used and the weight ini- tialization. Appendixes B and C describe the minimization procedure used, which is a stochastic version of a diagonal approximation to the Levenberg-Marquardt procedure.
c6hh | III. RESULTS AND COMPARISON WITH OTHER METHODS
y7ea | While recognizing individual digits is only one of many problems involved in designing a practical recognition system, it is an excellent benchmark for comparing shape recognition methods. Though many existing methods com- bine a hand-crafted feature extractor and a trainable clas- sifier, this study concentrates on adaptive methods that operate directly on size-normalized images.
nav4 | A. Database: The Modified NIST Set
gem8 | The database used to train and test the systems described in this paper was constructed from the NIST's Special Database 3 and Special Database 1 containing binary im- ages of handwritten digits. NIST originally designated SD-3 as their training set and SD-1 as their test set. However, SD-3 is much cleaner and easier to recognize than SD-1. The reason for this can be found on the fact that SD- 3 was collected among Census Bureau employees, while SD-1 was collected among high-school students. Drawing sensible conclusions from learning experiments requires that the result be independent of the choice of training set and test among the complete set of samples. Therefore it was necessary to build a new database by mixing NIST's datasets.
e8nr | SD-1 contains 58 527 digit images written by 500 dif- ferent writers. In contrast to SD-3, where blocks of data from each writer appeared in sequence, the data in SD-1 is scrambled. Writer identities for SD-1 are available and we used this information to unscramble the writers. We then split SD-1 in two: characters written by the first 250 writers went into our new training set. The remaining 250 writers were placed in our test set. Thus we had two sets with nearly 30 000 examples each. The new training set was completed with enough examples from SD-3, starting at pattern #0, to make a full set of 60 000 training patterns. Similarly, the new test set was completed with SD-3 examples starting at pattern #35 000 to make a full set with 60 000 test patterns. In the experiments described here, we only used a subset of 10 000 test images (5,000 from SD-1 and 5,000 from SD-3), but we used the full 60 000 training samples. The resulting database was called the modified NIST, or MNIST, dataset.
k78w | The original black and white (bilevel) images were size normalized to fit in a 20×20 pixel box while preserving their aspect ratio. The resulting images contain grey levels as result of the antialiasing (image interpolation) technique used by the normalization algorithm. Three versions of the database were used. In the first version, the images were centered in a 28×28 image by computing the center of mass of the pixels and translating the image so as to position this point at the center of the 28x28 field. In some instances, this 28x28 field was extended to 32x32 with background pixels. This version of the database will be referred to as the regular database. In the second version of the database, the character images were deslanted and cropped down to 20×20 pixels images. The deslanting computes the second moments of inertia of the pixels (counting a foreground pixel as one and a background pixel as zero) and shears the image by horizontally shifting the lines so that the principal axis is vertical. This version of the database will be referred to as the deslanted database. In the third version of the database, used in some early experiments, the images were reduced to 16×16 pixels.1 Fig. 4 shows examples randomly picked from the test set.
g9du | B. Results
f682 | Several versions of LeNet-5 were trained on the regu- lar MNIST database. Twenty iterations through the entire training data were performed for each session. The values of the global learning rate n [see (21) in Appendix C for a definition] was decreased using the following schedule: 0.0005 for the first two passes; 0.0002 for the next three; 0.0001 for the next three; 0.000 05 for the next 4; and 0.000 01 thereafter. Before each iteration, the diagonal
li1k | Fig. 5. Training and test error of LeNet-5 as a function of the number of passes through the 60 000 pattern training set (without distortions). The average training error is measured on-the-fly as training proceeds. This explains why the training error appears to be larger than the test error initially. Convergence is attained after 10-12 passes through the training set.
pbea | Hessian approximation was reevaluated on 500 samples, as described in Appendix C, and was kept fixed during the entire iteration. The parameter į was set to 0.02. The resulting effective learning rates during the first pass varied between approximately 7x10-5 and 0.016 over the set of parameters. The test error rate stabilizes after around ten passes through the training set at 0.95%. The error rate on the training set reaches 0.35% after 19 passes. Many authors have reported observing the common phenomenon of overtraining when training NN's or other adaptive algorithms on various tasks. When overtraining occurs, the training error keeps decreasing over time but the test error goes through a minimum and starts increasing after a certain number of iterations. While this phenomenon is very common, it was not observed in our case as the learning curves in Fig. 5 show. A possible reason is that the learning rate was kept relatively large. The effect of this is that the weights never settle down in the local minimum but keep oscillating randomly. Because of those fluctuations, the average cost will be lower in a broader minimum. Therefore, stochastic gradient will have a similar effect as a regularization term that favors broader minima. Broader minima correspond to solutions with large entropy of the parameter distribution, which is beneficial to the generalization error.
ccpt | The influence of the training set size was measured by training the network with 15 000, 30 000, and 60 000 examples. The resulting training error and test error are shown in Fig. 6. It is clear that, even with specialized architectures such as LeNet-5, more training data would improve the accuracy.
0fpw | To verify this hypothesis, we artificially generated more training examples by randomly distorting the original train- ing images. The increased training set was composed of the 60 000 original patterns plus 540 000 instances of dis- torted patterns with randomly picked distortion parameters.
dqn8 | The distortions were combinations of the following planar affine transformations: horizontal and vertical translations; scaling; squeezing (simultaneous horizontal compression and vertical elongation, or the reverse); and horizontal shearing. Fig. 7 shows examples of distorted patterns used for training. When distorted data were used for training, the test error rate dropped to 0.8% (from 0.95% without deformation). The same training parameters were used as without deformations. The total length of the training session was left unchanged (20 passes of 60 000 patterns each). It is interesting to note that the network effectively sees each individual sample only twice over the course of these 20 passes.
cop0 | Fig. 8 shows all 82 misclassified test examples. some of those examples are genuinely ambiguous, but several are perfectly identifiable by humans, although they are written in an under-represented style. This shows that further improvements are to be expected with more training data.
lhkq | C. Comparison with Other Classifiers