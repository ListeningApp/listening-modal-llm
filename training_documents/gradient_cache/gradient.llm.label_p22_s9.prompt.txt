You will receive raw text from an OCR scan of a document or web page. Each line of input represents one textbox from the document.

Your task is to label each textbox with exactly one of the following labels:
  title: the primary document title, no more than one per document
  heading: the name of the following chapter or section (should never start in the middle of the previous paragraph)
  subheading: a secondary heading, often following another heading (should never start in the middle of the previous paragraph)
  body: main paragraph content: may start or end in the middle of a word, use context to follow paragraphs that are split across textboxes (if a textbox contains both heading and body text, label it "body")
  math: textbox primarily containing math
  imageDescription: standalone callout that only describes an image, chart, table, or similar (typically begins with "Table...", "Figure...", "Fig. ...", "Above...", "Left...", etc.)
  authors: names of contributors to this document
  institutions: list of universities, business, and other institutions affiliated with this document
  publisher: info about the publisher, or provided by the publisher, including copyright info
  pagenum: the page number
  headerOrFooter: boilerplate text page number, or redundant headings at the top/bottom of the page
  toc: the table of contents
  references: bibliographic references and resources
  acknowledgements: thanks and credits to contributors
  appendix: index, appendix, or any other end matter not part of the main thread (including related headings)
  table: tabular data, label, title, or subheading for a grid or list of data
  datum: numeric data or data label from a chart or list
  advertisement: promotional ads and marketing
  layout: non-content related to the layout of the page, e.g. sidebar navigation
  callout: pull quote or long block text that stands apart from the main text
  footnote: footnotes and endnotes near the bottom of the page
  keywords: list of keywords or tags
  url: web address or email
  other: any other text that does not belong to the main thread

Each line of input is prefixed with a textbox ID in the format `id | Text content`. The output must be in the format `id | label`.

Note that body text may be split across multiple textboxes, and textboxes may start or end in the middle of a sentence or word. Because the text was extracted from a web page or document, paragraphs of body text may be interrupted by multiple textboxes of redundant headers, footnotes, page numbers, tables, images, etc. For example, a real heading will never interrupt the middle of a sentence. Use context clues like this to carefully label each textbox.

EXAMPLE INPUT:
1pjs | Neuroscience Applied 1 (2022) 101012
o2kr | Contents lists available at ScienceDirect
v6sk | Neuroscience Applied
1wj6 | journal homepage: www.journals.elsevier.com/neuroscience-applied
936l | Research Articles
06yq | Microdosing with psychedelics to self-medicate for ADHD symptoms in adults: A naturalistic study
dpv6 | ARTICLE INFO
8r5j | ABSTRACT
2taz | Keywords: ADHD Microdosing Psychedelics Self-medication Well-being Time perception
o3ya | ADHD in adulthood is often overlooked, which negatively affects the individual's well-being. First-line phar- macological interventions are effective in many ADHD patients, relieving symptoms rapidly. However, there seems to be a proportion of individuals who discontinue, or fail to respond to these treatments. For these in- dividuals, alternative treatment options should be explored.
r5il | 1. Introduction
5gui | Attention deficit hyperactivity disorder (ADHD) is one of the most common developmental disorders worldwide. Prevalence research indicates that 2.6% of the adult population has persistent ADHD. ADHD in adults is often overlooked because of the high comorbidity rate and lack of knowledge about how ADHD is expressed in adulthood (Kooij et al.,
9d4o | Fig. 1. Flowchart of included participants for each time point.
82qc | 2019). In addition, ADHD is associated with deficits in various domains of cogni- tive functioning. Twenty-five percent of ADHD cases suffered from ADHD symptoms purely because of de- ficiencies in temporal processing (Sonuga-Barke et al., 2010).
ls7d | First-line ADHD treatments in adults mainly include pharmacological interventions to enhance dopaminergic and noradrenergic neurotrans- mission with stimulants. Overall, they have been proven to work effectively in adults with ADHD, inducing fast symptom relief and thereby enhancing the person's quality of life. In the
5esx | Table 1 Demographic information from respondents at baseline and the two and four- week time points.
t8vv | longer term, approximately twenty percent of ADHD patients discontinue their prescribed medication after six to nine months, thirty percent after one year, and half of them after two years.
xrd1 | 2. Material and methods
5746 | 2.1. Study design and participants
7toz | The study employed a naturalistic design, assessing the experiences of participants at baseline,
j346 | Neuroscience Applied 1 (2022)
m5ka | E.C.H.M. Haijen et al.
kx2c | REVIEW
v8uq | before they start MD on their own initiative, and at two and four weeks after MD initiation. The target population included adults diagnosed with ADHD and individuals who experienced ADHD symptoms to the extent that these interfered with their daily lives and who had not been diag- nosed with ADHD before. To be included in the analyses, participants needed to score above a cut-off point on at least one of the subscales of the Conner's Adult ADHD Rating Scale (CAARS-S:SV).
2zmw | Fig. 2. Mean scores of the CAARS-S:SV DSM-IV total symptoms T-scores at baseline (0W) and two (2W) and four weeks (4W) after MD (A) of the whole sample, and (B) per conventional ADHD medication use. Error bars represent mean error.
sdk3 | <LATEX>\lim _ { x ightarrow \infty } rac { 6 x ^ { 2 } + 1 0 0 } { 7 x ^ { 2 } - 1 0 } =</LATEX>
x972 | 2.2. Study procedure
k221 | Mean performance measures of the CAARS-

EXAMPLE OUTPUT:
1pjs | headerOrFooter
o2kr | publisher
v6sk | publisher
1wj6 | publisher
936l | headerOrFooter
06yq | title
dpv6 | other
8r5j | heading
2taz | keywords
o3ya | body
r5il | heading
5gui | body
9d4o | imageDescription
82qc | body
ls7d | body
5esx | imageDescription
t8vv | body
xrd1 | heading
5746 | subheading
7toz | body
j346 | headerOrFooter
m5ka | authors
kx2c | headerOrFooter
v8uq | body
2zmw | imageDescription
sdk3 | math
x972 | heading
k221 | body




z8tt | Training methods for graph-based sequence recognition systems such as HMM's have been extensively studied in the context of speech recognition [28]. Those meth- ods require that the system be based on probabilistic generative models of the data, which provide normalized likelihoods over the space of possible input sequences. Popular HMM learning methods, such as the Baum-Welsh algorithm, rely on this normalization. The normalization cannot be preserved when nongenerative models such as NN's are integrated into the system. Other techniques, such as discriminative training methods, must be used in this case. Several authors have proposed such methods to train NN/HMM speech recognizers at the word or sentence level [29], [67], [71]-[78].
9g22 | Other globally trainable sequence recognition systems avoid the difficulties of statistical modeling by not resorting to graph-based techniques. The best example is recurrent NN's (RNN's). Unfortunately, despite early enthusiasm, the training of RNN's with gradient-based techniques has proven very difficult in practice [79].
41fx | The GTN techniques presented below simplify and gen- eralize the global training methods developed for speech recognition.
7um8 | A. Viterbi Training
uow2 | During recognition, we select the path in the interpre- tation graph that has the lowest penalty with the Viterbi algorithm. Ideally, we would like this path of lowest penalty to be associated with the correct label sequence as often as possible. An obvious loss function to minimize is therefore the average over the training set of the penalty of the path associated with the correct label sequence that has the lowest penalty. The goal of training will be to find the set of recognizer parameters (the weights, if the recognizer is an NN) that minimize the average penalty of this "correct" lowest penalty path. The gradient of this loss function can be computed by back propagation through the GTN architecture shown in Fig. 19. This training architecture is almost identical to the recognition architecture described in the previous section, except that an extra GT called a
iat7 | Fig. 19. Viterbi training GTN architecture for a character string recognizer based on HOS.
ywwr | path selector is inserted between the interpretation graph and the Viterbi transformer. This transformer takes the interpretation graph and the desired label sequence as input. It extracts from the interpretation graph those paths that contain the correct (desired) label sequence. Its output graph Gc is called the constrained interpretation graph (also known as forced alignment in the HMM literature) and contains all the paths that correspond to the correct label sequence. The constrained interpretation graph is then sent to the Viterbi transformer which produces a graph Gevit with a single path. This path is the "correct" path with the lowest penalty. Finally, a path scorer transformer takes Gevit and simply computes its cumulated penalty Ccvit by adding up the penalties along the path. The output of this GTN is the loss function for the current pattern
0i6j | Evit = Cevit. (11)
87cq | The only label information that is required by the above system is the sequence of desired character labels. No knowledge of the correct segmentation is required on the part of the supervisor, since it chooses among the segmentations in the interpretation graph the one that yields the lowest penalty.
0qs3 | The process of back propagating gradients through the Viterbi training GTN is now described. As explained in Section IV, the gradients must be propagated backward through all modules of the GTN in order to compute gradients in preceding modules and thereafter tune their parameters. Back propagating gradients through the path scorer is quite straightforward. The partial derivatives of the loss function with respect to the individual penalties on the constrained Viterbi path Gevit are equal to one, since the loss function is simply the sum of those penalties. Back propagating through the Viterbi Transformer is equally simple. The partial derivatives of Evit with respect to the penalties on the arcs of the constrained graph Gc are one for those arcs that appear in the constrained Viterbi path Gcvit and zero for those that do not. Why is it legitimate to back
z8f7 | propagate through an essentially discrete function such as the Viterbi transformer? The answer is that the Viterbi trans- former is nothing more than a collection of min functions and adders put together. It was shown in Section IV that gradients can be back propagated through min functions without adverse effects. Back propagation through the path selector transformer is similar to back propagation through the Viterbi transformer. Arcs in Gint that appear in Gc have the same gradient as the corresponding arc in Gc, i.e., one or zero, depending on whether the arc appear in Gevit. The other arcs, i.e., those that do not have an alter ego in Gc because they do not contain the right label have a gradient of zero. During the forward propagation through the recognition transformer, one instance of the recognizer for single character was created for each arc in the segmentation graph. The state of recognizer instances was stored. Since each arc penalty in Gint is produced by an individual output of a recognizer instance, we now have a gradient (one or zero) for each output of each instance of the recognizer. Recognizer outputs that have a nonzero gradient are part of the correct answer and will therefore have their value pushed down. The gradients present on the recognizer outputs can be back propagated through each recognizer instance. For each recognizer instance, we obtain a vector of partial derivatives of the loss function with respect to the recognizer instance parameters. All the recognizer instances share the same parameter vector, since they are merely clones of each other, therefore the full gradient of the loss function with respect to the recognizer's parameter vector is simply the sum of the gradient vectors produced by each recognizer instance. Viterbi training, though formulated differently, is often use in HMM-based speech recognition systems [28]. Similar algorithms have been applied to speech recognition systems that integrate NN's with time alignment [71], [72], [76] or hybrid neural- network/HMM systems [29], [74], [75].
7w7j | While it seems simple and satisfying, this training archi- tecture has a flaw that can potentially be fatal. The problem was already mentioned in Section II-C. If the recognizer is a simple NN with sigmoid output units, the minimum of the loss function is attained, not when the recognizer always gives the right answer, but when it ignores the input and sets its output to a constant vector with small values for all the components. This is known as the collapse problem. The collapse only occurs if the recognizer outputs can simultaneously take their minimum value. If, on the other hand, the recognizer's output layer contains RBF units with fixed parameters, then there is no such trivial solution. This is due to the fact that a set of RBF with fixed distinct parameter vectors cannot simultaneously take their minimum value. In this case, the complete collapse described above does not occur. However, this does not totally prevent the occurrence of a milder collapse because the loss function still has a "flat spot" for a trivial solution with constant recognizer output. This flat spot is a saddle point, but it is attractive in almost all directions and is very difficult to get out of using gradient-based minimization procedures. If the parameters of the RBF's are allowed
51vd | to adapt, then the collapse problems reappear because the RBF centers can all converge to a single vector, and the underlying NN can learn to produce that vector and ignore the input. A different kind of collapse occurs if the width of the RBF's are also allowed to adapt. The collapse only occurs if a trainable module such as an NN feeds the RBF's. The collapse does not occur in HMM-based speech recognition systems because they are generative systems that produce normalized likelihoods for the input data (more on this later). Another way to avoid the collapse is to train the whole system with respect to a discriminative training criterion, such as maximizing the conditional probability of the correct interpretations (correct sequence of class labels) given the input image.
baj1 | Another problem with Viterbi training is that the penalty of the answer cannot be used reliably as a measure of confidence because it does not take low-penalty (or high- scoring) competing answers into account.
n1wo | B. Discriminative Viterbi Training
5bq3 | A modification of the training criterion can circumvent the collapse problem described above and at the same time produce more reliable confidence values. The idea is to not only minimize the cumulated penalty of the lowest penalty path with the correct interpretation, but also to somehow increase the penalty of competing and possibly incorrect paths that have a dangerously low penalty. This type of criterion is called discriminative because it plays the good answers against the bad ones. Discriminative training procedures can be seen as attempting to build appropriate separating surfaces between classes rather than to model in- dividual classes independently of each other. For example, modeling the conditional distribution of the classes given the input image is more discriminative (focusing more on the classification surface) than having a separate generative model of the input data associated to each class (which, with class priors, yields the whole joint distribution of classes and inputs). This is because the conditional approach does not need to assume a particular form for the distribution of the input data.
ezla | One example of discriminative criterion is the difference between the penalty of the Viterbi path in the constrained graph, and the penalty of the Viterbi path in the (uncon- strained) interpretation graph, i.e., the difference between the penalty of the best correct path and the penalty of the best path (correct or incorrect). The corresponding GTN training architecture is shown in Fig. 20. The left side of the diagram is identical to the GTN used for nondiscriminative Viterbi training. This loss function re- duces the risk of collapse because it forces the recognizer to increases the penalty of wrongly recognized objects. Discriminative training can also be seen as another example of error correction procedure, which tends to minimize the difference between the desired output computed in the left half of the GTN in Fig. 20 and the actual output computed in the right half of Fig. 20.
rsbu | Let the discriminative Viterbi loss function be denoted Edvit, and let us call Cevit the penalty of the Viterbi path
magd | in the constrained graph and Cvit the penalty of the Viterbi path in the unconstrained interpretation graph
tb74 | Edvit = Cevit - Cvit. (12)
8l69 | Edvit is always positive since the constrained graph is a subset of the paths in the interpretation graph, and the Viterbi algorithm selects the path with the lowest total penalty. In the ideal case, the two paths Cevit and Cvit coincide, and Edvit is zero.
1jyv | Back-propagating gradients through the discriminative Viterbi GTN adds some "negative" training to the previ- ously described nondiscriminative training. Fig. 20 shows how the gradients are back propagated. The left half is identical to the nondiscriminative Viterbi training GTN, therefore the back propagation is identical. The gradients back propagated through the right half of the GTN are multiplied by -1, since Cvit contributes to the loss with a negative sign. Otherwise the process is similar to the left half. The gradients on arcs of Gint get positive contributions from the left half and negative contributions from the right half. The two contributions must be added since the penalties on Gint arcs are sent to the two halves through a "Y" connection in the forward pass. Arcs in Gint that appear neither in Gvit nor in Gevit have a gradient of zero. They do not contribute to the cost. Arcs that appear in both Gvit and Gevit also have zero gradient. The -1 contribution from the right half cancels the +1 contribution from the left half. In other words, when an arc is rightfully part of the answer there is no gradient. If an arc appears in Gevit but not in Gvit, the gradient is +1. The arc should have had a lower penalty to make it to Gvit. If an arc is in Gvit but not in Gevit, the gradient is -1. The arc had a low penalty, but it should have had a higher penalty since it is not part of the desired answer.
olce | Variations of this technique have been used for the speech recognition. Driancourt and Bottou [76] used a version of it where the loss function is saturated to a fixed value. This can be seen as a generalization of the Learning Vector Quantization 2 (LVQ-2) loss function [80]. Other variations of this method use not only the Viterbi path but the K- best paths. The discriminative Viterbi algorithm does not have the flaws of the nondiscriminative version, but there are problems nonetheless. The main problem is that the criterion does not build a margin between the classes. The gradient is zero as soon as the penalty of the constrained Viterbi path is equal to that of the Viterbi path. It would be desirable to push up the penalties of the wrong paths when they are dangerously close to the good one. The following section presents a solution to this problem.
erx1 | C. Forward Scoring and Forward Training
b3bt | While the penalty of the Viterbi path is perfectly appro- priate for the purpose of recognition it gives only a partial picture of the situation. Imagine the lowest penalty paths corresponding to several different segmentations produced the same answer (the same label sequence). Then it could be argued that the overall penalty for the interpretation should
0w8m | Fig. 20. Discriminative Viterbi training GTN architecture for a character string recognizer based on HOS. Quantities in square brackets are penalties computed during the forward propagation. Quantities in parentheses are partial derivatives computed during the backward propagation.
vcu3 | be smaller than the penalty obtained when only one path produced that interpretation, because multiple paths with identical label sequences are more evidence that the label sequence is correct. Several rules can be used compute the penalty associated to a graph that contains several parallel paths. We use a combination rule borrowed from a probabilistic interpretation of the penalties as negative log posteriors. In a probabilistic framework, the posterior probability for the interpretation should be the sum of the posteriors for all the paths that produce that interpretation.
hjzj | Translated in terms of penalties, the penalty of an inter- pretation should be the negative logarithm of the sum of the negative exponentials of the penalties of the individual paths. The overall penalty will be smaller than all the penalties of the individual paths.
j79f | Given an interpretation, there is a well-known method, called the forward algorithm for computing the above quantity efficiently [28]. The penalty computed with this procedure for a particular interpretation is called the for- ward penalty. Consider again the concept of constrained
6tbv | graph, the subgraph of the interpretation graph which contains only the paths that are consistent with a particular label sequence. There is one constrained graph for each possible label sequence (some may be empty graphs, which have infinite penalties). Given an interpretation, running the forward algorithm on the corresponding constrained graph gives the forward penalty for that interpretation. The forward algorithm proceeds in a way very similar to the Viterbi algorithm, except that the operation used at each node to combine the incoming cumulated penalties, instead of being the min function, is the so-called logadd operation, which can be seen as a "soft" version of the min function
xb13 | fn = logaddieu, (ci + fsi)
cxqi | (13)
qfmx | where fstart = 0, Un is the set of upstream arcs of node n, Ci is the penalty on arc i, and
rut3 | logadd(x1, x2, ... , In) = - log ¿= 1 > e (14)
m3cp | Note that because of numerical inaccuracies, it is better to factorize the largest e-ri (corresponding to the smallest penalty) out of the logarithm.