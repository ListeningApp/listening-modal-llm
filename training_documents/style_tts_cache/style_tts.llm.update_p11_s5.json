{
  "id": "chatcmpl-A0ZlYRS9e9MtuyDW1STDZXtJXew9q",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "```\nxjyk | There are three Transferable Monotonic Aligner related innovations in this work: the decoder is trained with hard monotonic alignment and soft attention in a fifty percent to fifty percent manner and two TMA objectives functions. The fifty percent to fifty percent training is motivated by the fact that the monotonic alignment search proposed is not differentiable, and the soft attention alignment does not necessarily provide correct alignments for duration prediction in the second stage of training. This fifty percent to fifty percent split is arbitrary and can be changed to anything from ten percent to ninety percent to ninety percent to ten percent, depending on the dataset and the application. When the ratio is one hundred percent to zero percent, it becomes the case where the external aligners are not fine-tuned like in most parallel TTS systems such as FastSpeech, while when the ratio is zero percent to one hundred percent, it becomes the case we fine-tune the aligner with only soft attention such as in Cotatron for voice conversion applications. We find that training with external aligners hundred percent hard, no fine-tuning decreases the naturalness of the synthesized speech because bad alignments can happen due to covariate shifts between the training dataset LibriSpeech and testing dataset LJSpeech as in the case of Montreal Forced Aligner. One example is given in the leftmost figure in Figure five. On the other hand, if we only fine-tune the decoder with soft alignment, the decoder will overfit on the soft alignment and be unable to synthesize audible speech from hard alignment because the soft alignments are not either zero or one and the precise numerical values of alignments are used by the decoder to generate speech.\nmt0f | Another notable case is when we do not use a pre-trained text aligner such as in the case of VITS. This case makes MOS even lower than the case of no fine-tuning, suggesting that overfitting on a smaller dataset can be more detrimental than failure in generalization on the TTS dataset for some samples. The figure in the middle in Figure five shows an alignment with gaps and no background noises. This indicates overfitting of the text aligner to the smaller dataset for the mel-spectrogram reconstruction objective. However, since our goal is to synthesize the speech from predicted alignment, overfitting to speech reconstruction can be harmful to natural speech synthesis during inference.\nawab | In addition to the fifty percent to fifty percent training, we also introduced two TMA objectives Ls2s and Lmono. This is motivated by the fact that Ls2s learns correct alignments for S2S-ASR but not necessarily monotonic while non-differentiable monotonic alignments obtained through dynamic programming algorithms proposed do not necessarily produce correct alignments. By combining Ls2s and Lmono, we can learn an aligner that produces both correct and monotonic alignments.\nixcc | B. AdaIN, AdaLN, and Concatenation\n8p6h | As shown in Table seven and Table eight, AdaIN outperforms AdaLN and simple concatenation for both naturalness and style\npmgl | reflection. Here we describe our intuitions behind these results.\n90no | Concatenation vs. AdaIN. When we concatenate the style vector to each frame of the encoded phonetic representations, we create a representation hstyle equals Thtext s . When the hstyle is passed to the next convolution layer whose parameter is W, we get —\njt0y | hstyle equals W equals\nqb7o | —\noae5 | equals\ns0fl | equals htext equals Wtext plus S equals Wstyle\n43ia | equals htext equals Wtext plus Concat htext, s\n3g2y | where Wtext and Wstyle are block matrix notation of the corresponding weights for hstyle and s and Concat htext, S equals s equals Wstyle denotes the concatenation operation as a function of input htext and style vector s. This Concat x, s function is almost like AdaIN in equation one where Lu s equals Wstyle except we do not have the temporal modulation term La s. The modulation term is very important in style transfer, and some works argue that modulation alone is enough for diverse style representations. In contrast, concatenation only provides the addition term Lu but no modulation term La. Intuitively, the modulation term can determine the variance of the pitch and energy, for example, and therefore without such a term, correlations for pitch and energy standard deviation are much lower than AdaIN and AdaLN as shown in Table eight.\ngc7v | AdaLN vs. AdaIN. Generative models for speech synthesis learn to generate mel-spectrograms, which is essentially a one-dimensional feature map with eighty channels. Each channel in the mel-spectrogram represents a single frequency range. When we apply AdaIN, we learn a distribution with a style-specific mean and variance for each channel, compared to AdaLN, where a single mean and variance are learned for the entire feature map. This inherent difference between feature distributions makes AdaIN more expressive in terms of style reflection than AdaLN.\n```",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1724700848,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_a2ff031fb5",
  "usage": {
    "completion_tokens": 1051,
    "prompt_tokens": 2777,
    "total_tokens": 3828
  }
}