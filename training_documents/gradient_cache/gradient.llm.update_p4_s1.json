{
  "id": "chatcmpl-A3U7PP4vG09gvAFQovmwieEil2ZdM",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "tey0 | of several layers of processing, that is, the back-propagation algorithm. The third event was the demonstration that the back-propagation procedure applied to multilayer neural networks with sigmoidal units can solve complicated learning tasks. The basic idea of back propagation is that gradients can be computed efficiently by propagation from the output to the input. This idea was described in the control theory literature of the early nineteen sixties, but its application to machine learning was not generally realized then. Interestingly, the early derivations of back propagation in the context of neural network learning did not use gradients but \"virtual targets\" for units in intermediate layers, or minimal disturbance arguments. The Lagrange formalism used in the control theory literature provides perhaps the best rigorous method for deriving back propagation and for deriving generalizations of back propagation to recurrent networks and networks of heterogeneous modules. A simple derivation for generic multilayer systems is given in Section One point E.\nghiv | The fact that local minima do not seem to be a problem for multilayer neural networks is somewhat of a theoretical mystery. It is conjectured that if the network is oversized for the task (as is usually the case in practice), the presence of \"extra dimensions\" in parameter space reduces the risk of unattainable regions. Back propagation is by far the most widely used neural-network learning algorithm, and probably the most widely used learning algorithm of any form.\no5ts | D. Learning in Real Handwriting Recognition Systems\nhd7d | Isolated handwritten character recognition has been extensively studied in the literature, and it was one of the early successful applications of neural networks. Comparative experiments on recognition of individual handwritten digits are reported in Section Three. They show that neural networks trained with gradient-based learning perform better than all other methods tested here on the same data. The best neural networks, called convolutional networks, are designed to learn to extract relevant features directly from pixel images (see Section Two).\nehux | One of the most difficult problems in handwriting recognition, however, is not only to recognize individual characters, but also to separate out characters from their neighbors within the word or sentence, a process known as segmentation. The technique for doing this that has become the \"standard\" is called HOS. It consists of generating a large number of potential cuts between characters using heuristic image processing techniques, and subsequently selecting the best combination of cuts based on scores given for each candidate character by the recognizer. In such a model, the accuracy of the system depends upon the quality of the cuts generated by the heuristics, and on the ability of the recognizer to distinguish correctly segmented characters from pieces of characters, multiple characters, or otherwise incorrectly segmented characters. Training a recognizer to perform this task poses a major challenge because of the difficulty in creating a labeled database of incorrectly segmented characters. The simplest solution\npoxj | consists of running the images of character strings through the segmenter and then manually labeling all the character hypotheses. Unfortunately, not only is this an extremely tedious and costly task, it is also difficult to do the labeling consistently. For example, should the right half of a cut-up four be labeled as a one or as a noncharacter? Should the right half of a cut-up eight be labeled as a three?\n2xvb | The first solution, described in Section Five, consists of training the system at the level of whole strings of characters rather than at the character level. The notion of gradient-based learning can be used for this purpose. The system is trained to minimize an overall loss function which measures the probability of an erroneous answer. Section Five explores various ways to ensure that the loss function is differentiable and therefore lends itself to the use of gradient-based learning methods. Section Five introduces the use of directed acyclic graphs whose arcs carry numerical information as a way to represent the alternative hypotheses and introduces the idea of GTN.\nm1hx | The second solution, described in Section Seven, is to eliminate segmentation altogether. The idea is to sweep the recognizer over every possible location on the input image, and to rely on the \"character spotting\" property of the recognizer, that is, its ability to correctly recognize a well-centered character in its input field, even in the presence of other characters besides it, while rejecting images containing no centered characters. The sequence of recognizer outputs obtained by sweeping the recognizer over the input is then fed to a GTN that takes linguistic constraints into account and finally extracts the most likely interpretation. This GTN is somewhat similar to hidden Markov models, which makes the approach reminiscent of the classical speech recognition. While this technique would be quite expensive in the general case, the use of convolutional neural networks makes it particularly attractive because it allows significant savings in computational cost.\n9psh | E. Globally Trainable Systems\naiqv | As stated earlier, most practical pattern recognition systems are composed of multiple modules. For example, a document recognition system is composed of a field locator (which extracts regions of interest), a field segmenter (which cuts the input image into images of candidate characters), a recognizer (which classifies and scores each candidate character), and a contextual postprocessor, generally based on a stochastic grammar (which selects the best grammatically correct answer from the hypotheses generated by the recognizer). In most cases, the information carried from module to module is best represented as graphs with numerical information attached to the arcs. For example, the output of the recognizer module can be represented as an acyclic graph where each arc contains the label and the score of a candidate character, and where each path represents an alternative interpretation of the input string. Typically, each module is manually optimized, or sometimes trained, outside of its context. For example, the character recognizer would be trained on labeled images of presegmented characters. Then the complete system is\n4klz | assembled, and a subset of the parameters of the modules is manually adjusted to maximize the overall performance. This last step is extremely tedious, time consuming, and almost certainly suboptimal.\nxz5d | A better alternative would be to somehow train the entire system so as to minimize a global error measure such as the probability of character misclassifications at the document level. Ideally, we would want to find a good minimum of this global loss function with respect to all the parameters in the system. If the loss function E measuring the performance can be made differentiable with respect to the system's tunable parameters W, we can find a local minimum of E using gradient-based learning. However, at first glance, it appears that the sheer size and complexity of the system would make this intractable.\nerwx | To ensure that the global loss function EP of ZP and W is differentiable, the overall system is built as a feedforward network of differentiable modules. The function implemented by each module must be continuous and differentiable almost everywhere with respect to the internal parameters of the module (e.g., the weights of a neural network character recognizer in the case of a character recognition module), and with respect to the module's inputs. If this is the case, a simple generalization of the well-known back-propagation procedure can be used to efficiently compute the gradients of the loss function with respect to all the parameters in the system. For example, let us consider a system built as a cascade of modules, each of which implements a function Xn equals Fn of Wn and Xn-minus one, where Xn is a vector representing the output of the module, Wn is the vector of tunable parameters in the module (a subset of W), and Xn-minus one is the module's input vector (as well as the previous module's output vector). The input Xo to the first module is the input pattern ZP. If the partial derivative of EP with respect to Xn is known, then the partial derivatives of EP with respect to Wn and Xn-minus one can be computed using the backward recurrence",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725394123,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 1612,
    "prompt_tokens": 3357,
    "total_tokens": 4969
  }
}