{
  "id": "chatcmpl-A3UD9ZPu8KuVk2dY865URVhnbhYNX",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "1o7p | Note that Chimera can be generalized to combine more than two pipelines, which further reduces the bubbles and balances the activations memory consumption, but at a cost of higher communication overhead and weights memory consumption.\ncp13 | Three point two. Communication Scheme\nbgre | Chimera uses point-to-point communication to transfer the intermediate activations and gradients with respect to the inputs between pipeline stages in the forward pass and the backward pass, respectively. Since Chimera combines bidirectional pipelines together, collective communication, i.e., allreduce, is used to synchronize the weight gradients across stage replicas before the next training iteration. Figure four a presents a simple way for gradient synchronization, namely synchronizing the gradients for each stage maintained by the workers after all the local computation of the current iteration is finished. Note that the gradient synchronization for the middle stages is partially overlapped by the computation on the beginning and the end stages.\n57lp | For a deeper communication overlap, we launch allreduce eagerly by utilizing the bubbles in the pipeline. Taking P zero and P three in Figure four b as an example, after these two workers finish the backward passes on micro-batch three and micro-batch one, respectively, the calculation for the weight gradients of stage three has been finished; therefore, P zero and P three can launch an asynchronous allreduce using nonblocking collectives to synchronize the gradients of stage three as soon as they are finished, and a wait operation is called after all the local computation to make sure the allreduce is finished. In this way, the gradient synchronization for stage three is overlapped and the following computation. However, unlike P zero and P three, we choose not to conduct eager gradient synchronization for stage two, a middle stage, on P one and P two, since there is no bubble from the completion of stage two's gradients to the end of local computation. Although the asynchronous communication may proceed while the computation happens, it may cause additional overheads such as initialization, threading, etc. which could extend the critical path of the pipeline and jeopardize the overall performance. Performance modelling of the communication scheme will be presented in Section three point four.\nukth | Three point three. Hybrid of Pipeline and Data Parallelism\nkpvt | Chimera supports a hybrid of pipeline and data parallelism. The bidirectional pipelines of Chimera are replicated W times to scale to W times D workers. Since we consider the large models which can be easily partitioned into balanced stages, all D stages are equally replicated W times in hybrid parallelism. When scaling to the parallel machines equipped with high performance interconnected networks such as Infiniband, Cray Aries, or Slingshot, and NVLink, hybrid parallelism usually achieves better performance than the pure pipeline parallelism. This is because pure pipeline parallelism has W times D stages in the pipeline, while hybrid parallelism has D stages, W times less, which helps to reduce the point-to-point communication overhead between stages and increase the computation workload of each stage. Although hybrid parallelism leads to gradient synchronization between stage replicas, the overhead of it can be alleviated by the aforementioned high performance interconnected networks. However, as W increases D decreases, pipeline stages become coarser, until at some point the increased gradient synchronization overhead cannot be amortized by the reduced point-to-point communication overhead. Therefore, it is important to find the sweet spot to achieve the best performance.\njie2 | synchronized does not change, but the number of processes participating in the gradient synchronization increases by W times. Also, we use the same communication scheme as discussed in Section three point two to overlap the gradient synchronization overhead in hybrid parallelism. In the next section we will discuss how to find the best configuration of W and D based on performance modelling.\nd0ik | Three point four. Configuration Selection Based on Performance Modelling\n0x0s | Given the mini-batch size and the number of workers P, the configuration of B, W, and D largely affects the training throughput.\n4exe | Larger micro-batch size B usually improves the computational efficiency of the accelerators. Since Chimera greatly alleviates the bubble problem, it greedily chooses to use the maximum micro-batch size fitting in the device memory. Compared with the existing synchronous pipeline approaches which have to consider the trade-off between bubble and computational efficiency, the greedy strategy of Chimera significantly reduces the tuning space.\n0myz | To select the best configuration of W and D, we build a performance model to predict the runtime of a single training iteration represented by T for each available configuration. For the compu-\nxh3i | we measure the runtime of the forward pass on a. The runtime of backward pass represented as\nsyiw | two times of the forward pass if no activation recomputation is used, and three times otherwise. We define the critical path as a series of pipeline stages with dependency that determine the overall computation overhead of a single training iteration. An example of critical path is shown in Figure six. Let C sub F and C sub B denote the number of forward passes and backward passes on the critical path of the pipeline, respectively. For the example shown in Figure six, C sub F equals six and C sub B equals ten. The total computation overhead is",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725394479,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 1061,
    "prompt_tokens": 3105,
    "total_tokens": 4166
  }
}