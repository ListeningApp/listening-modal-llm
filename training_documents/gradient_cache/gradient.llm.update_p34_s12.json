{
  "id": "chatcmpl-A3U7QNftVdpTSjWr1VhUd4qH5vlMZ",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "``` \ncg8u | E. GTN and HMM's\n2hsv | GTN's can be seen as a generalization and an extension of HMM's. On the one hand, the probabilistic interpretation can be either kept (with penalties being log-probabilities), pushed to the final decision stage (with the difference of the constrained forward penalty and the unconstrained forward penalty being interpreted as negative log-probabilities of label sequences), or dropped altogether (the network just represents a decision surface for label sequences in input space). On the other hand, GTN's extend HMM's by allowing to combine in a well-principled framework multiple levels of processing, or multiple models.\n7s7l | Unfolding an HMM in time yields a graph that is very similar to our interpretation graph (at the final stage of processing of the GTN, before Viterbi recognition). It has nodes n(t, i) associated to each time step t and state i in the model. The penalty Ci for an arc from n(t - one, j) to n(t, i) then corresponds to the negative log-probability of emitting observed data ot at position t and going from state j to state i in the time interval t-one, t. With this probabilistic interpretation, the forward penalty is the negative logarithm of the likelihood of whole observed data sequence given the model.\nuwdf | In Section VI we mentioned that the collapsing phenomenon can occur when nondiscriminative loss functions are used to train NN's/HMM hybrid systems. With classical HMM's with fixed preprocessing, this problem does not occur because the parameters of the emission and transition probability models are forced to satisfy certain probabilistic constraints: the sum or the integral of the probabilities of a random variable over its possible values must be one. Therefore, when the probability of certain events is increased, the probability of other events must automatically be decreased. On the other hand, if the probabilistic assumptions in an HMM (or other probabilistic model) are not realistic, discriminative training, discussed in Section VI, can improve performance as this has been\njjgp | clearly shown for speech recognition systems.\ndf4g | The input-output HMM is strongly related to GT's. Viewed as a probabilistic model, an IOHMM represents the conditional distribution of output sequences given input sequences of the same or a different length. It is parameterized from an emission probability module and a transition probability module. The emission probability module computes the conditional emission probability of an output variable given an input value and the value of discrete \"state\" variable. The transition probability module computes conditional transition probabilities of a change in the value of the \"state\" variable, given the input value. Viewed as a GT, it assigns an output graph representing a probability distribution over the sequences of the output variable to each path in the input graph. All these output graphs have the same structure, and the penalties on their arcs are simply added in order to obtain the complete output graph. The input values of the emission and transition modules are read off the data structure on the input arcs of the IOHMM GT. In practice, the output graph may be very large, and needs not be completely instantiated, i.e., it is pruned: only the low penalty paths are created.\nqc2j | Nine. An On-line Handwriting Recognition System\nx3oh | Natural handwriting is often a mixture of different \"styles,\" i.e., lower case printed, upper case, and cursive. A reliable recognizer for such handwriting would greatly improve interaction with pen-based devices, but its implementation presents new technical challenges. Characters taken in isolation can be very ambiguous, but considerable information is available from the context of the whole word. We have built a word recognition system for pen-based devices based on four main modules: One. a preprocessor that normalizes a word, or word group, by fitting a geometrical model to the word structure; Two. a module that produces an \"annotated image\" from the normalized pen trajectory; Three. a replicated convolutional NN that spots and recognizes characters; and Four. a GTN that interprets the networkâ€™s output by taking word-level constraints into account. The network and the GTN are jointly trained to minimize an error measure defined at the word level.\nfqvx | In this work, we have compared a system based on SDNN's, and a system based on HOS. Because of the sequential nature of the information in the pen trajectory, which reveals more information than the purely optical input from an image, HOS can be very efficient in proposing candidate character cuts, especially for non-cursive script.\nviae | A. Preprocessing\np8u9 | Input normalization reduces intracharacter variability, thereby simplifying character recognition. We have used a word normalization scheme based on fitting a geometrical model of the word structure. Our model has four \"flexible\" lines representing respectively the ascenders line,\n49ky | the core line, the base line, and the descenders line. The lines are fitted to local minima or maxima of the pen trajectory. The parameters of the lines are estimated with a modified version of the EM algorithm to maximize the joint probability of observed points and parameter values, using a prior on parameters that prevents the lines from collapsing on each other.\np5wo | The recognition of handwritten characters from a pen trajectory on a digitizing surface is often done in the time domain. Typically, trajectories are normalized and local geometrical or dynamical features are extracted. The recognition may then be performed using curve matching, or other classification techniques such as TDNN's. While these representations have several advantages, their dependence on stroke ordering and individual writing styles makes them difficult to use in high accuracy, writer-independent systems that integrate the segmentation with the recognition.\n74rp | Since the intent of the writer is to produce a legible image, it seems natural to preserve as much of the pictorial nature of the signal as possible, while at the same time exploit the sequential information in the trajectory. For this purpose we have designed a representation scheme called AMAP, where pen trajectories are represented by low-resolution images in which each picture element contains information about the local properties of the trajectory. An AMAP can be viewed as an \"annotated image\" in which each pixel is a five-element feature vector: four features are associated to four orientations of the pen trajectory in the\noolg | area around the pixel and the fifth one is associated to local curvature in the area around the pixel. A particularly useful feature of the AMAP representation is that it makes very few assumptions about the nature of the input trajectory. It does not depend on stroke ordering or writing speed, and it can be used with all types of handwriting, capital, lower case, cursive, punctuation, symbols. Unlike many other representations, AMAP's can be computed for complete words without requiring segmentation.\n3ewf | B. Network Architecture\n```",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725394124,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 1411,
    "prompt_tokens": 3213,
    "total_tokens": 4624
  }
}