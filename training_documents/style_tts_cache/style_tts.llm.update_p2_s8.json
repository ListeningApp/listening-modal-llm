{
  "id": "chatcmpl-A3UFiStkZDJ0xJxKjwZwqWf4yXQ1V",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "afbe | Text Aligner. The text aligner A generates an alignment dalign between mel-spectrograms and phonemes. We train a text aligner A alongside the decoder G during the reconstruction phase. Modeled after the decoder of Tacorton two with attention, A is initially trained on an automatic speech recognition task using the LibriSpeech corpus and then fine-tuned concurrently with our decoder. We call an aligner with this setup (pre-trained on large corpora and fine-tuned for TTS tasks) a Transferable Monotonic Aligner.\n78r8 | Style Encoder. Given an input mel-spectrogram x, our encoder derives a style vector s equals E of x. With various reference audios, E can generate diverse style representations, allowing the decoder G to create speech that mirrors the style s of a reference audio x. E consists of four residual blocks followed by an averaging pooling layer along the time axis.\n320g | Pitch Extractor. As in FastPitch, we extract pitch F zero directly in Hertz without further processing, providing a more straightforward representation and allowing enhanced control of speech pitch. Instead of using the acoustic periodicity detection method employed in FastPitch to estimate the ground truth pitch, we train a pitch extractor F end-to-end with our decoder G for a more accurate estimation. Our pitch extractor F is a JDC network, pre-trained on LibriSpeech with ground truth F zero estimated using YIN. This extractor is\ntp8g | fine-tuned with the decoder to predict pitch p x equals F of x for the reconstruction of x.\ngxfd | Decoder. Our decoder G is trained to reconstruct the input mel-spectrogram x, represented by c equals G of htext times dalign, S, P x, n x. Here, htext times dalign is aligned hidden representation of phonemes, s equals E of s is the style vector of x, P x is pitch contour of x, and n x is energy (represented by the log norm) of x per frame. The decoder is comprised of seven residual blocks with AdaIN, defined as follows:\ns2v1 | AdaIN of c, s equals L a of s times c minus µ of c divided by σ of c plus L u of s,\njkp9 | where c is a single channel of the feature maps, s is the style vector, u of dot and o of dot denote the channel mean and standard deviation and L o and L u are learned linear projections for computing the adaptive gain and bias using the style vector s. The use of AdaIN is one of the major differences between our model and other TTS models with style encoders. The advantages of AdaIN for diverse speech synthesis are further discussed in Appendix A-B.\nz8tt | To prevent dilution of import features, we concatenate the pitch P x, energy n x, and residual phoneme features hres and deliver them to subsequent residual blocks after AdaIN. This process is further detailed in Table, and its effectiveness is discussed in Section IV-D.\n9g22 | Discriminator. We include a discriminator D to facilitate training of our decoder for better sound quality. The discriminator shares the same architecture as our style encoder.\n41fx | Duration Predictor. Our duration predictor consists of a three-layer bidirectional LSTM R with adaptive layer normalization module followed by a linear projection L, where instance normalization is replaced by layer normalization in equation one. We use adaptive layer normalization because R takes discrete tokens similar to those in NLP applications, where layer normalization is preferred. R is shared with the prosody predictor P through hprosody equals R of htext as input to P.\n7um8 | Prosody Predictor. Our prosody predictor P predicts both the pitch p x and energy n x with given text and style vector. The aligned shared representation hprosody times at is processed through a shared bidirectional LSTM layer to generate hprosody, which is then fed into two sets of three residual blocks with AdaIN and a linear projection layer, one for the pitch output and another for the energy output (see Appendix D for details).\nuow2 | B. Training Objectives\niat7 | Our model training process is divided into two stages to allow the integration of duration-invariant prosody data augmentation, a key contribution of our work. During the first stage, the model learns to reconstruct the mel-spectrogram from the text, pitch, energy, and style. The second stage fixes all modules except the duration and prosody predictors, which are trained to predict the duration, pitch, and energy from the given text. One First Stage Objectives:\nywwr | Mel reconstruction. Given a mel-spectrogram x and its corresponding text t, we train our decoder under the L one reconstruction loss\n0i6j | C mel equals Expected value of x, t of L one of x minus G of htext times Malign, S, P a x, n a.\n87cq | where htext equals T of t is the encoded phoneme representation, a align equals A of x, t is the attention alignment from the text aligner, s equals E of x is the style vector of x, P x equals F of x is the pitch F zero of x and n e is the energy of x. For end-to-end training with the decoder and the text aligner, we apply a novel fifty percent-fifty percent strategy: half the time, we use the raw attention output as the alignment, which allows gradient backpropagation through the text aligner; for the other half, we use the non-differentiable monotonic version of alignment through dynamic programming algorithms to train the decoder for generating intelligible speech from monotonic hard alignment during inference. This innovative approach effectively fine-tunes the pre-trained text aligner to produce the optimal alignments for speech reconstruction, thus enhancing the sample quality of generated speech. The effectiveness of this strategy is analyzed in section IV-D.\n0qs3 | TMA objectives. We fine-tune our text aligner with the original sequence-to-sequence ASR objectives L s two s to ensure that correct attention alignment is kept during the end-to-end training:\nz8f7 | L s two s equals Expected value of x, t sum from i equals one to N of C E of t i, t i given t i,\n7w7j | \n51vd | where N is the number of phonemes in t, t i is the i-th phoneme token of t, t i is the i-th predicted phoneme token, and C E of dot is the cross-entropy loss function.\nbaj1 | Since this alignment is not necessarily monotonic, we use a simple L one loss L mono that forces the soft attention alignment to be close to its non-differentiable monotonic version:\nn1wo | L mono equals Expected value of x, t L one of a align minus a hard, where a align equals A of x, t is the attention alignment and a hard is the monotonic hard alignment obtained through dynamic programming algorithms (see Appendix A-A for details).\n5bq3 | (b) Duration and prosody prediction\nezla | Adversarial objectives. We employ two adversarial loss functions, the original cross-entropy loss function L adv for adversarial training and the additional feature-matching loss L f m, to improve the sound quality of the reconstructed mel-spectrogram:\nrsbu | L adv equals Expected value of x, t log D of x plus log of one minus D of x,\nmagd | \ntb74 | one sum from l equals one to T of N i of D prime of x minus D prime of x absolute value one,\n8l69 | L f m equals Expected value of x, t,\n1jyv | where & is the reconstructed mel-spectrogram by G, T is the total number of layers in D and D prime denotes the output feature map of l-th layer with N i number of features. The feature matching loss can be seen as a reconstruction loss of hidden layer features of real and generated speech as judged by the discriminator.",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725394638,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 1654,
    "prompt_tokens": 3354,
    "total_tokens": 5008
  }
}