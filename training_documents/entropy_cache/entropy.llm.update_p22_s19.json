{
  "id": "chatcmpl-A0ZgTUsUIgcVBV3R82FAdCH0OHdqJ",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "```\nd3h0 | The integral from negative infinity to infinity of x squared sine x d x. There's no good answer. We say a function f from X to R is integrable if it is measurable and its integral over X, defined in a certain way I won't explain here, gives a well-defined real number.\nvb88 | SHANNON ENTROPY: A FIRST TASTE\n0dzg | When you learn an event of probability p has happened, the amount of information you get is negative log p.\nqrj3 | Question. Suppose you know a coin lands heads up two-thirds of the time and tails up the rest of the time. What is the average or 'expected' amount of information you get when you learn which side landed up?\nerv1 | Answer. Two-thirds of the time you get negative log two-thirds of information, and the rest of the time you get negative log the rest. So, the expected amount of information you get is\n4u8c | negative three times log two-thirds minus three times log the rest.\nm3ho | You can do the same thing whenever you have any number of probabilities that add to one. The expected information is called the Shannon entropy.\npse1 | You flip a coin. You know the probability that it lands heads up. How much information do you get, on average, when you discover which side lands up? It's not hard to work this out. It's a simple example of 'Shannon entropy'. Roughly speaking, entropy is information that you don't know, that you could get if you did enough experiments. Here the experiment is simply flipping the coin and looking at it.\n9wyj | Puzzle eighteen. Suppose you know a coin lands heads up half the time and tails up the other half of the time. What is the expected amount of information you get from a coin flip? If you use base two for the logarithm, you get the expected information measured in bits. What do you get?\nhmko | Puzzle nineteen. Suppose you know a coin lands heads up a third of the time and tails up the other two-thirds of the time. What is the expected amount of information you get from a coin flip?\ntsxi | Puzzle twenty. Suppose you know a coin lands heads up a quarter of the time and tails up three-quarters of the time. What is the expected amount of information you get from a coin flip, in bits?\nfidk | If you solve these you'll see a pattern: the Shannon entropy is biggest when the coin is fair. As it becomes more and more likely for one side to land up than the other, the entropy drops. You're more sure about what will happen, so you learn less, on average, from seeing what happens!\nju2k | We've been doing examples where your experiment has just two possible outcomes: heads up or down. But you can do Shannon entropy for any number of outcomes. It measures how ignorant you are of what will happen. That is: how much you learn on average when it does!\nwldo | SHANNON ENTROPY: A SECOND TASTE\nct9a | According to the weather report there's a one-fourth chance that it will rain one centimeter, one-half chance it will rain two centimeters, and one-fourth chance it will rain three centimeters.\nrh8p | Question. What is the 'expected' amount of rainfall?\nvopc | Answer. One-fourth times one plus one-half times two plus one-fourth times three equals two centimeters.\nk4qz | Question. What is the 'expected' amount of information you learn when you find out how much it rains?\nk3w9 | Answer. Three-halves minus a fourth log a fourth minus one-half log one-half minus a fourth log a fourth equals three-halves log two, or in other words, three bits. This is the Shannon entropy of the weather report.\nlijr | If the weather report tells you it'll rain different amounts with different probabilities, you can figure out the 'expected' amount of rain. You can also figure out the expected amount of information you'll learn when it rains. This is called the 'Shannon entropy'.\n43dz | Shannon entropy is closely connected to information, but we can also think of it as a measure of ignorance. This may seem paradoxical. But it's not. Shannon entropy is the expected amount of information that you don't know when all you know is a probability distribution, which you will know when you see a specific outcome chosen according to this probability distribution.\nfyrt | For example, consider a weather report that says it will rain one centimeter with probability zero, two centimeters with probability one, and three centimeters with probability zero. The Shannon entropy of this weather report is\n2nsv | negative zero log zero minus one log one minus zero log zero equals zero since by convention we set p log p equals zero when p equals zero, this being the limit of p natural log p as p approaches zero from above.\nuqlj | What does it mean that this weather report has zero Shannon entropy? It means that when we see a specific outcome chosen according to this probability distribution, we learn nothing! The weather report says it will rain two centimeters with probability one. When this happens, we learn nothing that the weather report didn't already tell us.\n013l | The Shannon entropy doesn't depend on the amounts of rain, or even whether the forecast is about centimeters of rain or dollars of income. It only depends on the probabilities of the various outcomes. So Shannon entropy is a universal, abstract concept.\n4flw | Shannon entropy is closely connected to Gibbs entropy, which was already known in physics. But by lifting entropy to a more general level and connecting it to digital information, Shannon helped jump-start the information age. In fact a paper of his was the first to use the word 'bit'!\npzyx | THE DEFINITION OF SHANNON ENTROPY\nqc4k | Suppose you believe there are n possible outcomes with probabilities p one, ... , p n that are greater than or equal to zero that sum to one.\nbtsb | The average amount of information you learn when one of these outcomes happens, chosen according to this probability distribution, is the Shannon entropy:\ns1u0 | H equals the sum over i from one to n of negative pi log pi.\nju0a | Shannon entropy is larger for probability distributions that are more spread out, and smaller for probability distributions that are more sharply peaked.\ne4y0 | I've been leading up to it with examples, but here it is in general: Shannon entropy! Gibbs had already used a similar formula in physics-but with base e for the logarithm, an integral instead of a sum, and multiplying the answer by Boltzmann's constant. Shannon applied it to digital information.\nkpra | Here's where the formula for Shannon entropy comes from. We have some set of outcomes, say X. We have a probability distribution on this set, meaning a function p from X to the interval zero to one such that \negbi | the sum of pi over all elements of X equals one.\nbnhs | If we have any function A from X to R, we define its expected value to be the sum over all elements i in the set X of pi times Ai.\n```",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1724700533,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_a2ff031fb5",
  "usage": {
    "completion_tokens": 1486,
    "prompt_tokens": 3413,
    "total_tokens": 4899
  }
}