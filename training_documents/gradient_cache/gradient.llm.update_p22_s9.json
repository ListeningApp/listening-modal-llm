{
  "id": "chatcmpl-A3U7P1AYRaFBlW8y1PoYyBJiq5qr7",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "mnaf | Training methods for graph-based sequence recognition systems such as HMM's have been extensively studied in the context of speech recognition. Those methods require that the system be based on probabilistic generative models of the data, which provide normalized likelihoods over the space of possible input sequences. Popular HMM learning methods, such as the Baum-Welsh algorithm, rely on this normalization. The normalization cannot be preserved when nongenerative models such as NN's are integrated into the system. Other techniques, such as discriminative training methods, must be used in this case. Several authors have proposed such methods to train NN/HMM speech recognizers at the word or sentence level.\n0yxe | Other globally trainable sequence recognition systems avoid the difficulties of statistical modeling by not resorting to graph-based techniques. The best example is recurrent NN's (RNN's). Unfortunately, despite early enthusiasm, the training of RNN's with gradient-based techniques has proven very difficult in practice.\nerre | The GTN techniques presented below simplify and generalize the global training methods developed for speech recognition.\n835g | A. Viterbi Training\nx3l9 | During recognition, we select the path in the interpretation graph that has the lowest penalty with the Viterbi algorithm. Ideally, we would like this path of lowest penalty to be associated with the correct label sequence as often as possible. An obvious loss function to minimize is therefore the average over the training set of the penalty of the path associated with the correct label sequence that has the lowest penalty. The goal of training will be to find the set of recognizer parameters (the weights, if the recognizer is an NN) that minimize the average penalty of this \"correct\" lowest penalty path. The gradient of this loss function can be computed by back propagation through the GTN architecture shown in Fig. nineteen. This training architecture is almost identical to the recognition architecture described in the previous section, except that an extra GT called a\nd2fj | path selector is inserted between the interpretation graph and the Viterbi transformer. This transformer takes the interpretation graph and the desired label sequence as input. It extracts from the interpretation graph those paths that contain the correct (desired) label sequence. Its output graph Gc is called the constrained interpretation graph (also known as forced alignment in the HMM literature) and contains all the paths that correspond to the correct label sequence. The constrained interpretation graph is then sent to the Viterbi transformer which produces a graph Gevit with a single path. This path is the \"correct\" path with the lowest penalty. Finally, a path scorer transformer takes Gevit and simply computes its cumulated penalty Cevit by adding up the penalties along the path. The output of this GTN is the loss function for the current pattern\ngeyi | Eevit equals Cevit.\n2181 | The only label information that is required by the above system is the sequence of desired character labels. No knowledge of the correct segmentation is required on the part of the supervisor, since it chooses among the segmentations in the interpretation graph the one that yields the lowest penalty.\n9ubl | The process of back propagating gradients through the Viterbi training GTN is now described. As explained in Section four, the gradients must be propagated backward through all modules of the GTN in order to compute gradients in preceding modules and thereafter tune their parameters. Back propagating gradients through the path scorer is quite straightforward. The partial derivatives of the loss function with respect to the individual penalties on the constrained Viterbi path Gevit are equal to one, since the loss function is simply the sum of those penalties. Back propagating through the Viterbi Transformer is equally simple. The partial derivatives of Evit with respect to the penalties on the arcs of the constrained graph Gc are one for those arcs that appear in the constrained Viterbi path Gcvit and zero for those that do not. Why is it legitimate to back\no8pj | propagate through an essentially discrete function such as the Viterbi transformer? The answer is that the Viterbi transformer is nothing more than a collection of min functions and adders put together. It was shown in Section four that gradients can be back propagated through min functions without adverse effects. Back propagation through the path selector transformer is similar to back propagation through the Viterbi transformer. Arcs in Gint that appear in Gc have the same gradient as the corresponding arc in Gc, i.e., one or zero, depending on whether the arc appear in Gevit. The other arcs, i.e., those that do not have an alter ego in Gc because they do not contain the right label have a gradient of zero. During the forward propagation through the recognition transformer, one instance of the recognizer for single character was created for each arc in the segmentation graph. The state of recognizer instances was stored. Since each arc penalty in Gint is produced by an individual output of a recognizer instance, we now have a gradient (one or zero) for each output of each instance of the recognizer. Recognizer outputs that have a nonzero gradient are part of the correct answer and will therefore have their value pushed down. The gradients present on the recognizer outputs can be back propagated through each recognizer instance. For each recognizer instance, we obtain a vector of partial derivatives of the loss function with respect to the recognizer instance parameters. All the recognizer instances share the same parameter vector, since they are merely clones of each other, therefore the full gradient of the loss function with respect to the recognizer's parameter vector is simply the sum of the gradient vectors produced by each recognizer instance. Viterbi training, though formulated differently, is often use in HMM-based speech recognition systems. Similar algorithms have been applied to speech recognition systems that integrate NN's with time alignment or hybrid neural- network/HMM systems.\n27b9 | While it seems simple and satisfying, this training architecture has a flaw that can potentially be fatal. The problem was already mentioned in Section two-C. If the recognizer is a simple NN with sigmoid output units, the minimum of the loss function is attained, not when the recognizer always gives the right answer, but when it ignores the input and sets its output to a constant vector with small values for all the components. This is known as the collapse problem. The collapse only occurs if the recognizer outputs can simultaneously take their minimum value. If, on the other hand, the recognizer's output layer contains RBF units with fixed parameters, then there is no such trivial solution. This is due to the fact that a set of RBF with fixed distinct parameter vectors cannot simultaneously take their minimum value. In this case, the complete collapse described above does not occur. However, this does not totally prevent the occurrence of a milder collapse because the loss function still has a \"flat spot\" for a trivial solution with constant recognizer output. This flat spot is a saddle point, but it is attractive in almost all directions and is very difficult to get out of using gradient-based minimization procedures. If the parameters of the RBF's are allowed\n09xa | to adapt, then the collapse problems reappear because the RBF centers can all converge to a single vector, and the underlying NN can learn to produce that vector and ignore the input. A different kind of collapse occurs if the width of the RBF's are also allowed to adapt. The collapse only occurs if a trainable module such as an NN feeds the RBF's. The collapse does not occur in HMM-based speech recognition systems because they are generative systems that produce normalized likelihoods for the input data. Another way to avoid the collapse is to train the whole system with respect to a discriminative training criterion, such as maximizing the conditional probability of the correct interpretations (correct sequence of class labels) given the input image.",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725394123,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 1601,
    "prompt_tokens": 3336,
    "total_tokens": 4937
  }
}