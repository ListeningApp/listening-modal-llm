{
  "id": "chatcmpl-A0ZgYqrQf7WeIMkoXEGO7KMaPsy9V",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "```\n3qp5 | Here's how you can compute the expected energy E and the entropy S of any system starting from its partition function Z as a function of the coolness beta. I'll show you why these formulas are true, and then we'll test them out on the harmonic oscillator, where we have already computed the expected energy and entropy by other methods.\nguux | sixty-six\nfhkp | THE PARTITION FUNCTION KNOWS THE EXPECTED ENERGY\nur19 | If a system has partition function Z equals the integral of E(x) exp(-beta E(x)) dx, then\nytow | the derivative of the natural log of Z with respect to beta equals the integral over x of the derivative of E(x) exp(-beta E(x)) divided by Z.\nozjl | the integral over x of E(x) exp(-beta E(x)), divided by the integral over x of exp(-beta E(x)).\nxux5 | the integral over x of exp(-beta E(x)) dx\n53mt | equals the expected value of E\nowo1 | In short, the expected energy is the derivative of the natural log of Z with respect to beta\nzz70 | The partition function is all-powerful! For starters, if you know the partition function of a physical system, you can figure out its expected energy. The expected energy is minus the derivative of the natural log of Z with respect to the coolness beta equals one over k T.\nslcr | How do we show this? Easy: just look at the calculation above! We get a fraction, which is the expected value of E with respect to the Gibbs distribution.\nz9at | By the way, this trick of taking the derivative of the logarithm of a function is famous: it's called a 'logarithmic derivative'. Notice that\n7dov | f prime of x\nznos | divided by f of x\nbngy | Thus the logarithmic derivative says how fast a function is growing compared to the value of the function itself-like the interest rate in compound interest.\nxja2 | THE PARTITION FUNCTION KNOWS THE ENTROPY\nb9a5 | If a system has Boltzmann distribution\nthi8 | p(x) equals exp(-beta E(x)) divided by Z, where Z equals the integral over x of exp(-beta E(x)) dx,\nentm | then its entropy in thermal equilibrium is\nknql | S equals negative k times the integral over x of p(x) natural log of p(x) dx, which equals negative k times the integral over x of p(x) natural log of exp(-beta E(x)) divided by Z dx.\nez57 | equals k times the integral over x of p(x) natural log Z plus beta E(x) dx.\nxwza | equals k natural log Z plus beta times the expected value of E.\nkw2g | But since the expected value of E equals negative the derivative of the natural log of Z, this gives\nevst | S equals k natural log Z minus beta times the derivative of the natural log of Z with respect to beta.\nr4ei | The entropy is a bit more complicated. But don't be scared! The Boltzmann distribution p(x) is a fraction, so the log of this fraction breaks into two parts:\nv3un | the natural log of p(x) equals the natural log of exp(-beta E(x)), minus the natural log of Z, which equals negative the natural log of Z plus beta E(x).\nmmh7 | Thus our integral for entropy breaks into two parts:\n15mo | S equals negative k times the integral over x of p(x) natural log p(x) dx, which equals k times the integral over x of p(x) natural log Z dx plus k times beta times the integral over x of p(x) E(x) dx.\nhaqw | The first part is just k natural log Z since the integral of p(x) is one. The second part is k times beta times the expected value of E. If we use what we just learned about the expected value of E:\n89i1 | the expected value of E equals negative the derivative of the natural log of Z.\ngfu5 | we get this formula for entropy in terms of the partition function:\nmvzl | This formula seems hard to understand at first. To extract its inner meaning, we need a new concept: 'free energy'.\nbuwu | sixty-eight\nzsfu | THE PARTITION FUNCTION KNOWS THE FREE ENERGY\nhyvo | To maximize entropy while holding expected energy constant, you can just minimize the free energy\n5l9v | F equals the expected value of E minus T S. We've seen\n6tv1 | the expected value of E equals negative the derivative of the natural log of Z with respect to beta and S equals k times the natural log of Z minus beta times the derivative of the natural log of Z with respect to beta.\nl79g | so with beta equals one over k T, a little algebra shows\nd8ia | F equals negative the natural log of Z.\nv012 | We can understand the relation between entropy, energy and the partition function if we bring in a concept I haven't mentioned yet: the free energy\ncrl5 | F equals the expected value of E minus T S.\nxqom | Since we know formulas for the expected value of E and S in terms of the partition function, we can work out a formula for F. And it's really simple! Much simpler than S, for example. It's just\nmwss | F equals negative the natural log of Z.\nqba8 | But what's the meaning of free energy? Remember: to maximize the Shannon entropy H subject to a constraint on expected energy, we introduced the Lagrange multiplier beta equals one over k T and maximized the quantity H minus beta times the expected value of E. But if you multiply this quantity by negative k T, you get free energy:\n24cj | negative k T times the quantity H minus beta times the expected value of E, which equals the expected value of E minus T S, which equals F.\nkkr4 | So, as long as T is greater than zero, maximizing entropy subject to a constraint on expected energy is equivalent to minimizing free energy!\np3um | Thus, free energy turns a problem of maximizing entropy subject to a constraint into a minimization problem without a constraint. The point is not that we've turned maximization into minimization: that's just an arbitrary business with signs. The point is that free energy lets us stop thinking about the constraint.\nfcs1 | There's a huge amount to say about the free energy, which is also called the 'Helmholtz free energy', since there are other kinds. You can think of T S as the amount of energy in useless random form, since it comes from entropy. Since the expected value of E is the total expected energy, F equals the expected value of E minus T S is the amount of 'useful' energy. More precisely, the free energy is the maximum amount of work obtainable from a system at a constant temperature. But showing this would take us out of our way.\n5rxi | THE PARTITION FUNCTION KNOWS ALL: REVISITED\ncm65 | If Z is the partition function of a system, in thermal equilibrium at coolness beta, its expected energy is the expected value of E equals negative the derivative of the natural log of Z\nux5y | and its free energy is\nc8vn | F equals negative the natural log of Z\nalzc | We can compute its entropy from these using\n0dos | F equals the expected value of E minus T S and we get\nbc48 | S equals k natural log Z minus the derivative of natural log of Z with respect to beta.\nf3v7 | Now we can tell a simpler story, which is easier to remember. Free energy, being the energy in useful form, is the expected energy minus the useless energy, which is temperature times entropy. Thus\n8s6f | F equals the expected value of E minus T S.\nx7h9 | so\nygmb | S equals the expected value of E minus F divided by T.\n0o5y | equals k beta times the quantity negative F plus the expected value of E.\ndnu3 | and using our formulas for F and the expected value of E in terms of the partition function Z, we get\ndsty | S equals k natural log Z minus beta times the derivative of natural log of Z.\nci0z | The story here is more of a mnemonic than a true explanation, because I'm not saying much what it means for energy to be 'useful' or 'useless'. I've only given this hint: when a system is in thermal equilibrium, its free energy is minimized. For more on the meaning of free energy, try a good book on thermodynamics, like this:\nsban | Right now I'd rather say a bit about the meaning of the partition function.\nhy6w | THE MEANING OF THE PARTITION FUNCTION\nwlk5 | Say X is a set where each point i has an 'energy' E; E R. Its partition function is\nais9 | Z equals the sum over i in X of exp(-beta E sub i)\ncdkq | where beta belongs to the real numbers is the coolness.\najvs | The partition function counts the points of X-but it counts points with large energy less, since they're less likely to be 'occupied'.\nmwef | If beta equals one over k T, points with energy E sub i much greater than k T count for very little.\nk8pl | But as T approaches positive infinity, all points get fully counted and Z approaches the size of the set X. In physics we call Z the number of accessible states.\n04f6 | Say we have a system with some countable set of states X. In thermal equilibrium at temperature T, the probability that the system is in its i-th state is proportional to exp(-beta E sub i), where E sub i is the energy of that state and beta is the coolness. Thus, physicists say the partition function\n3nhr | Z equals the sum over i in X of exp(-beta E sub i)\n4hg2 | is the number of accessible states: roughly, the number of states the system can easily be in at temperature T, where beta equals one over k T.\n```",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1724700538,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_a2ff031fb5",
  "usage": {
    "completion_tokens": 2143,
    "prompt_tokens": 3558,
    "total_tokens": 5701
  }
}