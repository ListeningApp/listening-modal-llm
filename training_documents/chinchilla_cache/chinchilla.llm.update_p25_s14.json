{
  "id": "chatcmpl-A0YBW7tOiFctuFtEaMslkiYZOY6JU",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "```\nww6u | We use eight equals ten to the power of negative three for the Huber loss. We find that using larger values of & pushes the model to overfit the small compute regime and poorly predict held-out data from larger runs. We find that using a & smaller than ten to the power of negative three does not impact the resulting predictions.\nhpi6 | D point three. Predicted compute optimal frontier for all three methods\n4uhb | For Approaches two and three, we show the estimated model size and number of training tokens for a variety of compute budgets in Table A3. We plot the predicted number of tokens and parameters for a variety of FLOP budgets for the three methods in Figure A3.\nepyi | D point four. Small-scale comparison to Kaplan et al.\ny51p | For ten to the power of twenty-one FLOPs, we perform a head-to-head comparison of a model predicted by Approach one and that predicted by Kaplan et al. For both models, we use a batch size of zero point five million tokens and a\nx95m | maximum learning rate of one point five times ten to the power of negative four that decays by ten times. From Kaplan et al., we find that the optimal model size should be four point six eight billion parameters. From our approach one, we estimate a two point eight six billion parameter model should be optimal. We train a four point seven four billion parameter and a two point eight zero billion parameter transformer to test this hypothesis, using the same depth-to-width ratio to avoid as many confounding factors as possible. We find that our predicted model outperforms the model predicted by Kaplan et al. as shown in Figure A4.\nyfum | E. Curvature of the FLOP-loss frontier\nppx4 | We observe that as models increase there is a curvature in the FLOP-minimal loss frontier. This means that projections from very small models lead to different predictions than those from larger models. In Figure A5 we show linear fits using the first, middle, and final third of frontier-points. In this work, we do not take this into account and we leave this as interesting future work as it suggests that even smaller models may be optimal for large FLOP budgets.\nqf11 | F. FLOPs computation\n8oqt | We include all training FLOPs, including those contributed to by the embedding matrices, in our analysis. Note that we also count embeddings matrices in the total parameter count. For large models the FLOP and parameter contribution of embedding matrices is small. We use a factor of two to describe the multiply accumulate cost. For the forward pass, we consider contributions from:\n6zah | · Embeddings\n4v5o | · Attention (Single Layer)\nha9e | - Key, query and value projections: two times three times sequence length times d_model x (key_size × num_heads)\nxrnt | - Key at Query logits: two times seq_len times seq_len times (key_size times num_heads)\nc8ra | - Softmax: three times num_heads times seq_len times seq_len\np1l0 | - Softmax at query reductions: two times seq_len times seq_len times (key_size times num_heads)\n1i8b | - Final Linear: two times seq_len times (key_size times num_heads) times d_model\n5sfj | · Dense Block (Single Layer)\nvk4h | - two times seq_len times (d_model times ffw_size plus d_model times ffw_size)\nnsm3 | · Final Logits\njdfw | - two times seq_len times d_model times vocab_size\nuc9r | · Total forward pass FLOPs: embeddings plus number_layers times (total_attention plus dense_block) plus logits\nl0ap | As in Kaplan et al. we assume that the backward pass has twice the FLOPs of the forward pass. We show a comparison between our calculation and that using the common approximation C equals sixDN where C is FLOPs, D is the number of training tokens, and N is the number of parameters in Table A4. We find the differences in FLOP calculation to be very small and they do not impact our analysis. Compared to the results presented in Rae et al., we use a slightly more\nx815 | accurate calculation giving a slightly different value (six point three times ten to the power of twenty-three compared to five point seven six times ten to the power of twenty-three).\nepbj | G. Other differences between Chinchilla and Gopher\nnmn2 | Beyond differences in model size and number of training tokens, there are some additional minor differences between Chinchilla and Gopher. Specifically, Gopher was trained with Adam whereas Chinchilla was trained with AdamW. Furthermore, as discussed in Lessons Learned in Rae et al., Chinchilla stored a higher-precision copy of the weights in the sharded optimizer state.\ng3tm | We show comparisons of models trained with Adam and AdamW in Figure A6 and Figure A7. We find that, independent of the learning rate schedule, AdamW trained models outperform models trained with Adam. In Figure A6 we show a comparison of a six hundred eighty million parameter model trained\ntfra | with and without the higher precision copy of the weights and with Adam or AdamW for comparison.\n5lm9 | H. Results\nwp2c | H point one. The Pile\nus5v | H point two. MMLU\nrs2m | In Table A6 we show the performance of Chinchilla and Gopher on each subset of MMLU.\nuz9y | H point three. Winogender Setup\n8ov0 | We follow the same setup as in Rae et al. To test coreference resolution in Chinchilla, we input a sentence which includes a pronoun reference, then measure the probability of the model completing the sentence \"Pronoun refers to the\" with different sentence roles (\"librarian\" and \"child\" in this example). Each example is annotated with the correct pronoun resolution (the pronoun corresponds to the librarian in this example). Each sentence is tested with a female, male, and gender-neutral pronoun. An unbiased model would correctly predict which word the pronoun refers to regardless of pronoun gender.\nmj9m | H point four. BIG-bench\n94h3 | I. Model Card\n9jx1 | We present the Chinchilla model card in Table A8, following the framework presented by Mitchell et al.\nc3lz | Primary Intended Users DeepMind researchers. We will not make this model available publicly.\n8bwk | Datasets\nez9s | We did not investigate intersectional biases.\nmqot | Intersectional Results\noc99 | J. List of trained models\ngipz | In Table A9 we list the model size and configuration of all models used in this study. Many models have been trained multiple times, for a different number of training steps.\n```",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1724694770,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_3aa7262c27",
  "usage": {
    "completion_tokens": 1437,
    "prompt_tokens": 3237,
    "total_tokens": 4674
  }
}