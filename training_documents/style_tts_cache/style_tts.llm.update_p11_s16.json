{
  "id": "chatcmpl-A3UFgv3Kt8OdXekYxxb0ZQvYUXtAg",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "``` \no7qm | H text equals H text dot W text plus S dot W style\nd52l | H text equals H text dot W text plus Concat (H text, S)\nqlbb | where W text and W style are block matrix notation of the corresponding weights for H style and S and Concat (H text, S) equals S dot W style denotes the concatenation operation as a function of input H text and style vector S. This Concat (X, S) function is almost like AdaIN in equation one where Lu(S) equals W style except we do not have the temporal modulation term La(S). The modulation term is very important in style transfer, and some works argue that modulation alone is enough for diverse style representations. In contrast, concatenation only provides the addition term Lu but no modulation term La. Intuitively, the modulation term can determine the variance of the pitch and energy, for example, and therefore without such a term, correlations for pitch and energy standard deviation are much lower than AdaIN and AdaLN as shown in Table eight.\nnhsg | AdaLN versus AdaIN. Generative models for speech synthesis learn to generate mel-spectrograms, which is essentially a one-dimensional feature map with eighty channels. Each channel in the mel-spectrogram represents a single frequency range. When we apply AdaIN, we learn a distribution with a style-specific mean and variance for each channel, compared to AdaLN, where a single mean and variance are learned for the entire feature map. This inherent difference between feature distributions makes AdaIN more expressive in terms of style reflection than AdaLN.\nhy0l | C. Pitch Extractor\n4jmz | Acoustic methods for pitch estimation sometimes fail because of the presence of non-stationary speech intervals and sensitivity of hyper-parameters as discussed in the original papers that propose these methods. A neural network trained with ground truth from these methods, however, can leverage the problems of failed pitch estimation because the failed pitch estimation can be regarded as noises in the training set, so it does not affect the generalization of the pitch extractor network. Moreover, since the pitch extractor is fine-tuned along with the decoder, there is no ground truth for the pitch beside the sole objective that the decoder needs to use extracted pitch information to reconstruct the speech. This fine-tuning allows better pitch representations beyond the original F-zero in Hertz, but it also allows flexible pitch control as we can still recognize the pitch curves and edit them later when needed during inference.\nb7cq | We used the publicly available pre-trained models as baselines for comparison. For the single-speaker experiment on the LJSpeech dataset, we used pre-trained Tacotron two four, FastSpeech two five, HiFiGAN six from ESPnet. We used VITS seven and YourTTS eight from the official implementation. We randomly selected one hundred text samples from the test set to synthesize the speech. Since audios from our model were synthesized using HiFi-GAN trained with audios sampled at twenty-four kilohertz, for a fair comparison, we resampled all the audios into twenty-two kilohertz and then normalized their amplitude. We used the pre-trained model for StyleSpeech nine from a public repository in GitHub for comparison of zero-shot speaker adaptation in Appendix C. We did not use the official implementation because the vocoder used was MelGAN sampled at sixteen kilohertz while the implementation we employed uses HiFi-GAN sampled at twenty-two kilohertz, which is comparable to other models.\n0b39 | To reduce the listening fatigue, we randomly divided these one hundred sets of audios into five batches ten with each batch containing twenty sets of audios for comparison. We launched the five batches sequentially on Amazon Mechanical Turk. We required participating subjects to be native English speakers located in the United States. For each batch, we made sure that we had collected completed responses from at least ten self-reported native speakers whose IP addresses were within the United States and residential. We used the average score that a subject rated on ground truth audios to check whether this subject carefully finished the survey as the subjects did not know which audio was the ground truth. We then disqualified and dropped all ratings from the subjects whose average ground truth score was not ranked top two among all the models. Finally, forty-six out of fifty subjects were qualified for this experiment.\nqecd | In the multi-speaker experiments, we used pre-trained FastSpeech two twelve, VITS thirteen, and HiFiGAN fourteen from ESPnet. We used pre-trained VITS from ESPnet instead of the official repository because we need the model to be trained on the LibriTTS dataset; however, the official models were trained on the LJSpeech or VCTK dataset.\n1o7p | Similar to the single-talker experiment, we launched five batches fifteen on AMT when we tested the multi-talker models on the LibriTTS dataset. Forty-eight out of fifty-eight subjects were qualified. We launched three batches sixteen with batch sizes thirty-three, thirty-three, thirty-four, respectively, when we tested the multi-talker models on the VCTK dataset. Twenty-eight out of thirty subjects were qualified.\ncp13 | Since our text encoder, text aligner, pitch extractor, and decoder are trained in a speaker-agnostic manner, our decoder can reconstruct speech from any aligned phonemes, pitch, energy, and reference speakers. Therefore, our model can perform any-to-any voice conversion by extracting the alignment, pitch, and energy from an input mel-spectrogram and generating speech using a style vector of reference audio from an arbitrary target speaker. Our voice conversion scheme is transcription-guided, similar to Mellotron and Cotatron. We provide one example in Figure six with both source and target speaker unseen from the LJSpeech and VCTK datasets. We refer our readers to our demo page for more examples.\nbgre | In this section, we provide detailed model architectures of StyleTTS, which consists of eight modules. Since we use the same text encoder as in Tacotron two, a very similar architecture to the decoder of Tacotron two for text aligner and the \n57lp | same architecture as the JDC network for pitch extractor, we leave the readers to the above references for detailed descriptions of these modules. Here, we only provide detailed architectures for the other five modules. All activation functions used are leaky ReLU with a negative slope of zero point two. We apply spectral normalization to all trainable parameters in style encoder and discriminator and weight normalization to those in the decoder because they are shown to be beneficial for adversarial training.\nukth | Decoder (Table nine). Our decoder takes four inputs: the aligned phoneme representation, the pitch F-zero, the energy, and the style code. It consists of seven one-dimensional residual blocks along with three sub-modules for processing the input F-zero, energy, and residual of the phoneme representation. The normalization consists of both instance normalization and adaptive instance normalization. We concatenate the processed F-zero, energy, and residual of phonemes with the output from each residual block as the input to the next block for the first three blocks.\nkpvt | Style Encoder and Discriminator (Table ten). Our style encoder and discriminator share the same architecture, which consists of four two-dimensional residual blocks. The dimension of the style vector is set to one hundred twenty-eight. We use learned weights for pooling through a dilated convolution layer with a kernel size of three by three. We apply an adaptive average pooling along the time axis of the feature map to make the output independent of the size of the input mel-spectrogram.\n```",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725394636,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 1558,
    "prompt_tokens": 3356,
    "total_tokens": 4914
  }
}