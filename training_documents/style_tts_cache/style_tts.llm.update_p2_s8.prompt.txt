You will receive raw text from an OCR scan of a document or web page. Each line of input represents one textbox from the document. Your task is to faithfully reproduce the text for our text-to-speech engine so that it is easily read aloud.

Note that the text might not start at the beginning and paragraphs may be split acrosss multiple textboxes. Textboxes may start or end in the middle of a word which is acceptable: do not combine text between textboxes and leave broken words at the start and end of a textbox unchanged.

You will receive one textbox per-line in the format `id | Textbox content`. The output must be in the format `id | Clean content`.

RULES:
 • Remove inline citations and references e.g. "(Author, 2021)" or "(https://wikipedia.org/article.html)" or "[ECMOS 35b, 47, 49]".
 • Spell out numbers, dates, and chemical formulas for the TTS engine e.g. "-2.5" -> "negative two point five" or "Jan. 2020" -> "January twenty twenty".
 • Spell out LaTeX formulas for the TTS engine e.g. "<LATEX>p = 1</LATEX>" -> "P equals one".
 • Rejoin hyphenated word fragments within the same textbox e.g. "synth -esize" -> "synthesize".
 • Fix simple typos and OCR errors e.g. "O nce upon a 1ime." -> "Once upon a time.".
 • Delete nonsequiter textboxes that interrupt the middle of a sentence if they obviously do not belong.

Otherwise, the output should exactly match the input text:
 • Do not combine text across lines or attempt to fix words broken across lines.
 • Ensure each ID from the input is included in the output.
 • Make as few changes as possible while respecting the above rules.

EXAMPLE INPUT:
ccpt | stigate the effectiveness of psychedelics for ADHD symp- toms in adults.
0fpw | 2. Materia1 and methods
dqn8 | 2.1. Study design and participants
cop0 | The study employed a naturalistic design, assessing the experiences of participants at bas-,
lhkq | eline before they start, and at 2 and 4 weeks after initiation. The target population included adults with ADHD symptoms who had not been diag- nosed with ADHD before. To be included, participants needed to score on Conner's Adult ADHD Rating Scale (CAARS-S:SV). This cut-off was indicative of clinically elevated symptoms (Conners et al., 1999) (see section 2.3).
82ju | 2.2. Study procedure
y2qo | The online advertisement was placed on a website providing information about psychedelics (www.microdo sing.nl). Interested participants were redirected to information explaining the study rationale and procedure. The baseline survey took 20 min to complete. If the survey had not been completed after 24 h, a reminder was sent. Each of the surveys at the 2- and 4-week time points took about 15 min to complete. Par- ticipants were able to pause the surveys. Data collection occurred between Nov. 2020 and Jul. 2021. The study was approved by the Ethics Review Committee of Psy- chology and Neuroscience at Maastricht University (ERCPN- 215_05_11_2019_A1).
1piq | 2.3. Measures
7fqw | 2.3.1. Demographic information and history of substance use
rhaz | At baseline, demographic information was collected. History of substance use assessed experience with psyche- delics (i.e., ayahuasca, DMT, 5-MeO-DMT, LSD, novel lysergamides (e.g., 1P-LSD, ALD-52), psilocybin, salvia divinorum, ibogaine, and mescaline) in both full (psychedelic) doses and microdoses.
wak1 | 2.3.2. Psychiatric and physiological diagnoses
wtfz | Participants were asked whether they had a current diagnosis of a disorder. These answer options were chosen because most of the listed diagnoses are often reported to co-occur with ADHD (Kooij et al., 2019), or because these diagnoses were reported to be common in people who microdose (Fadiman and Korb, 2019). We 
3j2l | REVIEW ARTICLE Frontiers in Psychiatry | www.frontiersin.org
03k3 | 2
1k9e | constructed a variable Comorbidity alongside ADHD, differenti- ating respondents with and without a comorbid diagnosis alongside ADHD (<LATEX>p < . 0 0 1 =</LATEX> only ADHD or no ADHD diagnosis; <LATEX>p > . 0 1 =</LATEX> ADHD and at least one other diagnosis). We constructed a variable Medication use alongside microdosing, differentiating respondents who were and were not using conven- tiona1 ADHD medication alongside microdosing (<LATEX>0 =</LATEX> only microdosing; <LATEX>1 =</LATEX> conven-

EXAMPLE OUTPUT:
ccpt | stigate the effectiveness of psychedelics for ADHD symptoms in adults.
0fpw | Two. Material and methods
dqn8 | Two point one. Study design and participants
cop0 | The study employed a naturalistic design, assessing the experiences of participants at bas-,
lhkq | eline before they start, and at two and four weeks after initiation. The target population included adults with ADHD symptoms who had not been diagnosed with ADHD before. To be included, participants needed to score on Conner's Adult ADHD Rating Scale. This cut-off was indicative of clinically elevated symptoms (see section two point three).
82ju | Two point two. Study procedure
y2qo | The online advertisement was placed on a website providing information about psychedelics. Interested participants were redirected to information explaining the study rationale and procedure. The baseline survey took twenty minutes to complete. If the survey had not been completed after twenty-four hours, a reminder was sent. Each of the surveys at the two and four-week time points took about fifteen minutes to complete. Participants were able to pause the surveys. Data collection occurred between November twenty twenty and July twenty twenty-one. The study was approved by the Ethics Review Committee of Psychology and Neuroscience at Maastricht University.
1piq | Two point three. Measures
7fqw | Two point three point one. Demographic information and history of substance use
rhaz | At baseline, demographic information was collected. History of substance use assessed experience with psychedelics (i.e., ayahuasca, DMT, five-MeO-DMT, LSD, novel lysergamides (e.g., one-P-LSD, ALD-fifty-two), psilocybin, salvia divinorum, ibogaine, and mescaline) in both full (psychedelic) doses and microdoses.
wak1 | Two point three point two. Psychiatric and physiological diagnoses
wtfz | Participants were asked whether they had a current diagnosis of a disorder. These answer options were chosen because most of the listed diagnoses are often reported to co-occur with ADHD, or because these diagnoses were reported to be common in people who microdose. We
3j2l | REVIEW ARTICLE Frontiers in Psychiatry: www.frontiersin.org
03k3 | Two
1k9e | constructed a variable Comorbidity alongside ADHD, differentiating respondents with and without a comorbid diagnosis alongside ADHD (P is less than point zero zero one equals only ADHD or no ADHD diagnosis; P is greater than point zero one equals ADHD and at least one other diagnosis). We constructed a variable Medication use alongside microdosing, differentiating respondents who were and were not using conventional ADHD medication alongside microdosing (zero equals only microdosing; one equals conven-




afbe | Text Aligner. The text aligner A generates an alignment dalign between mel-spectrograms and phonemes. We train a text aligner A alongside the decoder G during the reconstruction phase. Modeled after the decoder of Tacorton 2 with attention, A is initially trained on an automatic speech recognition (ASR) task using the LibriSpeech corpus [37] and then fine-tuned concurrently with our decoder. We call an aligner with this setup (pre-trained on large corpora and fine-tuned for TTS tasks) a Transferable Monotonic Aligner (TMA).
78r8 | Style Encoder. Given an input mel-spectrogram x, our encoder derives a style vector s = E(x). With various reference audios, E can generate diverse style representations, allowing the decoder G to create speech that mirrors the style s of a reference audio x. E consists of four residual blocks [38] followed by an averaging pooling layer along the time axis.
320g | Pitch Extractor. As in FastPitch [11], we extract pitch F0 directly in Hertz without further processing, providing a more straightforward representation and allowing enhanced control of speech pitch. Instead of using the acoustic periodicity detection method [39] employed in FastPitch to estimate the ground truth pitch, we train a pitch extractor F end-to-end with our decoder G for a more accurate estimation. Our pitch extractor F is a JDC network [40], pre-trained on LibriSpeech with ground truth F0 estimated using YIN [41]. This extractor is
tp8g | fine-tuned with the decoder to predict pitch px = F(x) for the reconstruction of x.
gxfd | Decoder. Our decoder G is trained to reconstruct the input mel-spectrogram x, represented by c = G (htext . dalign, S, Px, nx). Here, htext . dalign is aligned hidden representation of phonemes, s = E(s) is the style vector of x, Px is pitch contour of x, and ne is energy (represented by the log norm) of x per frame. The decoder is comprised of seven residual blocks with AdaIN [19], defined as follows:
s2v1 | AdaIN(c, s) = La(s) c - µ(c) σ(c) + Lu(s), (1)
jkp9 | where c is a single channel of the feature maps, s is the style vector, u(.) and o(.) denote the channel mean and standard deviation and Lo and Lu are learned linear projections for computing the adaptive gain and bias using the style vector s. The use of AdaIN is one of the major differences between our model and other TTS models with style encoders such as [13] and [16]. The advantages of AdaIN for diverse speech synthesis are further discussed in Appendix A-B.
z8tt | To prevent dilution of import features, we concatenate the pitch Px, energy ng, and residual phoneme features hres and deliver them to subsequent residual blocks after AdaIN. This process is further detailed in Table , and its effectiveness is discussed in Section IV-D.
9g22 | Discriminator. We include a discriminator D to facilitate training of our decoder for better sound quality [1]. The discriminator shares the same architecture as our style encoder.
41fx | Duration Predictor. Our duration predictor consists of a 3- layer bidirectional LSTM R with adaptive layer normalization (AdaLN) module followed by a linear projection L, where instance normalization is replaced by layer normalization in equation 1. We use AdaLN because R takes discrete tokens similar to those in NLP applications, where layer normalization [42] is preferred. R is shared with the prosody predictor P through hprosody = R (htext) as input to P.
7um8 | Prosody Predictor. Our prosody predictor P predicts both the pitch px and energy nx with given text and style vector. The aligned shared representation hprosody . @ is processed through a shared bidirectional LSTM layer to generate hprosody, which is then fed into two sets of three residual blocks with AdaIN and a linear projection layer, one for the pitch output and another for the energy output (see Appendix D for details).
uow2 | B. Training Objectives
iat7 | Our model training process is divided into two stages to allow the integration of duration-invariant prosody data augmentation, a key contribution of our work. During the first stage, the model learns to reconstruct the mel-spectrogram from the text, pitch, energy, and style. The second stage fixes all modules except the duration and prosody predictors, which are trained to predict the duration, pitch, and energy from the given text. 1) First Stage Objectives:
ywwr | Mel reconstruction. Given a mel-spectrogram x E X' and its corresponding text t & T, we train our decoder under the L1 reconstruction loss
0i6j | Cmel = Ex,t [lax - G (htext . Malign, S, Pac, na )|] (2)
87cq | where htext = T(t) is the encoded phoneme representation, @align = A(x, t) is the attention alignment from the text aligner, s = E(x) is the style vector of x, Px = F(x) is the pitch FO of x and ne is the energy of x. For end-to-end (E2E) training with the decoder and the text aligner, we apply a novel 50%-50% strategy: half the time, we use the raw attention output as the alignment, which allows gradient backpropagation through the text aligner; for the other half, we use the non- differentiable monotonic version of alignment through dynamic programming algorithms [9] to train the decoder for generating intelligible speech from monotonic hard alignment during inference. This innovative approach effectively fine-tunes the pre-trained text aligner to produce the optimal alignments for speech reconstruction, thus enhancing the sample quality of generated speech. The effectiveness of this strategy is analyzed in section IV-D.
0qs3 | TMA objectives. We fine-tune our text aligner with the original sequence-to-sequence ASR objectives Ls2s to ensure that correct attention alignment is kept during the E2E training:
z8f7 | Ls2s = Ex,t i=1 N CE(ti, ti)| tˆi) ,
7w7j | (3)
51vd | where N is the number of phonemes in t, ti is the i-th phoneme token of t, ti is the i-th predicted phoneme token, and CE(.) is the cross-entropy loss function.
baj1 | Since this alignment is not necessarily monotonic, we use a simple L-1 loss Lmono that forces the soft attention alignment to be close to its non-differentiable monotonic version:
n1wo | Lmono = Ex,t [|@align - @hard|1] , (4) where aalign = A(x, t) is the attention alignment and @hard is the monotonic hard alignment obtained through dynamic programming algorithms (see Appendix A-A for details).
5bq3 | (b) Duration and prosody prediction
ezla | Adversarial objectives. We employ two adversarial loss functions, the original cross-entropy loss function Lady for adversarial training and the additional feature-matching loss [43] Lfm, to improve the sound quality of the reconstructed mel-spectrogram:
rsbu | Lady = Ex,t [log D(x) + log (1 - D(x))] ,
magd | (5)
tb74 | 1 l=1 T NĮ | D'(x) - D'(x)|1| , (6)
8l69 | Lfm = Ex,t
1jyv | where & is the reconstructed mel-spectrogram by G, T is the total number of layers in D and D' denotes the output feature map of l-th layer with Nį number of features. The feature matching loss can be seen as a reconstruction loss of hidden layer features of real and generated speech as judged by the discriminator.