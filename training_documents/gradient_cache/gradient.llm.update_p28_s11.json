{
  "id": "chatcmpl-A3U7QGuSR0JV8XIXPvKsm8Ud4IH0w",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "ssp0 | These three problems are elegantly circumvented if a convolutional network is replicated over the input field. First of all, as shown in Section three, convolutional N N's are very robust to shifts and scale variations of the input image, as well as to noise and extraneous marks in the input. These properties take care of the latter two problems mentioned in the previous paragraph. Second, convolutional networks provide a drastic saving in computational requirement when replicated over large input fields. A replicated convolutional network, also called an SDNN, is shown in Fig. twenty-three. While scanning a recognizer can be prohibitively expe- nsive in general, convolutional networks can be scanned or replicated very efficiently over large, variable-size input fields. Consider one instance of a convolutional net and its\n108o | alter ego at a nearby location. Because of the convolutional nature of the network, units in the two instances that look at identical locations on the input have identical outputs, therefore their states do not need to be computed twice. Only a thin \"slice\" of new states that are not shared by the two network instances needs to be recomputed. When all the slices are put together, the result is simply a larger convolutional network whose structure is identical to the original network, except that the feature maps are larger in the horizontal dimension. In other words, replicating a convolutional network can be done simply by increasing the size of the fields over which the convolutions are performed and by replicating the output layer accordingly. The output layer effectively becomes a convolutional layer. An output whose receptive field is centered on an elementary object will produce the class of this object, while an in-between output may indicate no character or contain rubbish. The outputs can be interpreted as evidences for the presence of objects at all possible positions in the input field.\nd2za | The SDNN architecture seems particularly attractive for recognizing cursive handwriting where no reliable segmentation heuristic exists. Although the idea of SDNN is quite old and very attractive in its simplicity, it has not generated wide interest until recently because, as stated above, it puts enormous demands on the recognizer. In speech recognition, where the recognizer is at least one order of magnitude smaller, replicated convolutional networks are easier to implement, for instance in Haffner's multistate TDNN model.\nr38m | A. Interpreting the Output of an SDNN with a GTN\nqq0z | The output of an SDNN is a sequence of vectors which encode the likelihoods, penalties, or scores of finding character of a particular class label at the corresponding location in the input. A postprocessor is required to pull out the best possible label sequence from this vector sequence. An example of SDNN output is shown in Fig. twenty-five. Very often, individual characters are spotted by several neighboring instances of the recognizer, a consequence of the robustness of the recognizer to horizontal translations. Also quite often, characters are erroneously detected by recognizer instances that see only a piece of a character. For example a recognizer instance that only sees the right third of a \"four\" might output the label one. How can we eliminate those extraneous characters from the output sequence and pull out the best interpretation? This can be done using a new type of GT with two input graphs as shown in Fig. twenty-four. The sequence of vectors produced by the SDNN is first coded into a linear graph with multiple arcs between pairs of successive nodes. Each arc between a particular pair of nodes contains the label of one of the possible categories, together with the penalty produced by the SDNN for that class label at that location. This graph is called the SDNN output graph. The second input graph to the transformer is a grammar transducer, more specifically a finite-state transducer, that encodes the relationship between input strings of class labels and corresponding output strings of recognized characters. The transducer is a weighted\nrgva | finite state machine (a graph) where each arc contains a pair of labels and possibly a penalty. Like a finite-state machine, a transducer is in a state and follows an arc to a new state when an observed input symbol matches the first symbol in the symbol pair attached to the arc. At this point the transducer emits the second symbol in the pair together with a penalty that combines the penalty of the input symbol and the penalty of the arc. A trans- ducer therefore transforms a weighted symbol sequence into another weighted symbol sequence. The GT shown in Fig. twenty-four performs a composition between the recognition graph and the grammar transducer. This operation takes every possible sequence corresponding to every possible path in the recognition graph and matches them with the paths in the grammar transducer. The composition produces the interpretation graph, which contains a path for each corresponding output label sequence. This composition operation may seem combinatorially intractable, but it turns out there exists an efficient algorithm for it described in more details in Section eight.\nmlcd | B. Experiments with SDNN\n2qe7 | In a series of experiments, LeNet-five was trained with the goal of being replicated so as to recognize multiple characters without segmentations. The data were generated from\nlpzf | the previously described MNIST set as follows. Training images were composed of a central character, flanked by two side characters picked at random in the training set. The separation between the bounding boxes of the characters were chosen at random between negative one and four pixels. In other instances, no central character was present, in which case the desired output of the network was the blank space class. In addition, training images were degraded with ten percent salt and pepper noise (random pixel inversions).\n24rz | Figs. twenty-five and twenty-six show a few examples of successful recognitions of multiple characters by the LeNet-five SDNN. Standard techniques based on HOS would fail miserably on many of those examples. As can be seen on these examples, the network exhibits striking invariance and noise resistance properties. While some authors have argued that invariance requires more sophisticated models than feedforward N N's, LeNet-five exhibits these properties to a large extent.\n091m | Similarly, it has been suggested that accurate recognition of multiple overlapping objects require explicit mechanisms that would solve the so-called feature binding problem. As can be seen on Figs. twenty-five and twenty-six, the network is able to tell the characters apart even when they are closely intertwined, a task that would be impossible to achieve with the more classical HOS technique. The SDNN is also able to correctly group disconnected pieces of ink that form characters. Good examples of that are shown in the upper half of Fig. twenty-six. In the top left example, the four and the zero are more connected to each other than they are connected with themselves, yet the system correctly identifies the four and the zero as separate objects. The top right example is interesting for several reasons. First the system correctly identifies the three individual ones. Second, the left half and right half of disconnected four are correctly grouped, even though no geometrical information could decide to associate the left half to the vertical bar on its left or on its right. The right\nsot6 | half of the four does cause the appearance of an erroneous one on the SDNN output, but this one is removed by the character model transducer which prevents characters from appearing on contiguous outputs.\nd6sf | Another important advantage of SDNN is the ease with which they can be implemented on parallel hardware. Specialized analog/digital chips have been designed and used in character recognition, and in image preprocessing applications. However the rapid progress of conventional processor technology with reduced-precision vector arithmetic instructions (such as Intel's MMX) make the success of specialized hardware hypothetical at best.\n7haq | C. Global Training of SDNN",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725394124,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 1590,
    "prompt_tokens": 3313,
    "total_tokens": 4903
  }
}