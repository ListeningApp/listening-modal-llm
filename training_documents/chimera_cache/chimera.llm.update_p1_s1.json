{
  "id": "chatcmpl-A3UD9uOnLoR3N7G8lv5XZUtnPr5ir",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "ym66 | Chimera: Efficiently Training Large-Scale Neural Networks with Bidirectional Pipelines\nl0a3 | ABSTRACT\nu8nw | Training large deep learning models at scale is very challenging. This paper proposes Chimera, a novel pipeline parallelism scheme which combines bidirectional pipelines for efficiently training large-scale models. Chimera is a synchronous approach and therefore no loss of accuracy, which is more convergence-friendly than asynchronous approaches. Compared with the latest synchronous pipeline approach, Chimera reduces the number of bubbles by up to fifty percent; benefiting from the sophisticated scheduling of bidirectional pipelines, Chimera has a more balanced activation memory consumption. Evaluations are conducted on Transformer-based language models. For a GPT-two model with one point three billion parameters running on two thousand forty-eight GPU nodes of the Piz Daint supercomputer, Chimera improves the training throughput by one point one six times to two point three four times over the state-of-the-art synchronous and asynchronous pipeline approaches.\nl2v9 | CCS CONCEPTS\nafbe | . Theory of computation -> Parallel algorithms; . Computing methodologies -> Neural networks.\n78r8 | KEYWORDS\n320g | One. INTRODUCTION\ntp8g | Deep learning is continuing to deliver groundbreaking results on the path towards human-level intelligence. This path is characterized by growing model size, in just six years, the compute requirements for model training grew by three hundred thousand times. Transformers are a typical representative in this trend. As the model size grows, Transformer-based models have proven their success in the field of natural language processing. Recent work shows that Transformers also achieve promising results in computer vision tasks, i.e., on par or better than other types of models such as convolutional and recurrent networks. These growing models must be trained on distributed accelerator supercomputers. Even today's models are too big to be stored on a single accelerator-for example, GPT-three's one hundred seventy-five billion parameters require three hundred fifty gigabytes main memory if stored with sixteen bits precision. Switch transformers have in their largest configuration one point six trillion parameters, a six point four terabyte storage requirements. Furthermore, the necessary memory for activations, gradients, and optimizer state during training at least triples these memory requirements.\ngxfd | Thus, full model replicas at each accelerator, as used in simple data parallelization schemes, are not sufficient. Large models must instead be distributed among many, often hundreds of accelerators to just fit into main memory. Deep neural networks consist of a layered architecture and can thus be distributed in two ways: (one) the operators of a layer can be split across multiple accelerators in a method called operator parallelism or (two) the model could be distributed layer by layer in a method called pipeline parallelism. Operators of a typical fully-connected layer have computational characteristics similar to matrix multiplication, and splitting such a layer requires a communication volume of O of n squared divided by P for an n by n matrix. By exploiting the inherent structure of Transformer-based language models, operator parallelism requires two allreduce operations on the output activations for each basic Transformer layer. Using a layer-wise model partition, pipeline parallelism on the other hand only requires point-to-point communication to transfer the output activations between pipeline stages, with each stage containing a group of consecutive layers. Therefore, pipeline parallelism commonly has a lower communication cost than operator parallelism. However, pipeline parallelism suffers from bubbles or weight staleness, which are the problems this work aims to solve. Overall, operator parallelism and pipeline parallelism are orthogonal and complementary to each other for distributing large deep learning models.\ns2v1 | Yet, pipeline parallelism is not trivial: The backpropagation algorithm needs to \"remember\" the output activations computed during the forward pass as inputs to the backward pass. This creates wildly different memory requirements for each accelerator in the pipeline, even though each accelerator has an identical compute load. Specifically, for some recently proposed pipeline approaches such as DAPPLE, PipeDream, and PipeDream-two-B-W, the first accelerator of a pipeline of depth D has to store D such activations while the last accelerator requires memory for one. This does not only lead to lower memory utilization in the later pipeline stages (and only fifty percent overall), it also leads to reduced performance because the micro-batch size has to be chosen to fit the first accelerator in the pipeline. This imbalance can be alleviated by restricting the number of micro-batches that are simultaneously allowed in the pipeline. However, this introduces bubbles and limits the overall system utilization.\njkp9 | Both micro-batch size and pipeline utilization are most important for the computational efficiency: larger micro-batches improve performance due to better re-use in the matrix-multiply-like operations and less pipeline bubbles (stalls) utilize the existing accelerators better. The computational efficiency relates directly to the cost and time for training a model. We propose a new pipelining scheme,\nz8tt | called Chimera, that runs fully-packed bidirectional pipelines through the same set of accelerators. Chimera enables\n9g22 | to keep the overall training synchronous without relying on stale weights,\n41fx | a higher pipeline utilization (less bubbles) than existing approaches and thus higher performance,\n7um8 | the same peak activation memory consumption as the state-of-the-art methods, with an extra benefit of more balanced memory consumption, and\nuow2 | easy configurability to various pipelined deep neural networks as well as system architectures guided by an accurate performance model.\niat7 | For example, GPT-three required three hundred fourteen zettaflop (mixed fp sixteen/fp thirty-two) to train, which would take a single A one hundred GPU more than one hundred years. The estimated cost to train GPT-three varies between four point six million to twelve million dollars. We show that Chimera enables end-to-end performance improvements between one point three eight times to two point three four times per iteration for the synchronous training regime for a comparable GPT-two model on two thousand forty-eight GPU nodes of the Piz Daint supercomputer, as shown in Figure one. This enables savings of more than one point two million dollars to five million dollars when training very large models on practical systems.\nywwr | Two. BACKGROUND AND RELATED WORK\n0i6j | Mini-batch stochastic gradient descent (SGD) is the mainstream method to train deep neural networks. Let b be the mini-batch size, W of t the neural network weights at step t, a sample in the mini-batch, and a loss function. During training, it computes the loss in the forward pass for each sample as W of t, x, y, and then a stochastic gradient in the backward pass as\n87cq | the gradient of W of t, x, y.\n0qs3 | The model is trained in iterations such that W of t plus one equals W of t minus n times the gradient of t. In more general terms, first-order stochastic gradient update rules can take different forms (e.g., by adding a momentum term), which is represented as W plus one equals w t plus U of the gradient of t, W, t.\nz8f7 | To scale up the training process to parallel machines, data parallelism is the common method, in which the mini-batch is partitioned among P workers and each worker maintains a copy of the entire model. Gradient synchronization across the workers is often implemented using an allreduce. However, recent deep learning models scale rapidly from millions to billions of parameters. Pure data parallelism may not work for these large models since it either suffers from low efficiency caused by synchronizing gradients of the entire model across the workers or the model is simply too large to fit in a device.",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725394479,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 1569,
    "prompt_tokens": 3406,
    "total_tokens": 4975
  }
}