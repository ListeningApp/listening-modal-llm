{
  "id": "chatcmpl-A0ZgViMxR4t8bQoHR7DY30r4dc8u9",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "```\nj0m4 | The principle of maximum entropy is widely used outside physics, though still controversial. But I think we should use it to figure out some basic properties of a gas-like its energy or entropy per molecule, as a function of pressure and temperature.\nsc0s | To do this, we should generalize Shannon entropy to 'Gibbs entropy', replacing the sum by an integral. Or else we should 'discretize' the gas, assuming each molecule has a finite set of states. It sort of depends on whether you prefer calculus or programming. Either approach is okay if we study our gas using classical statistical mechanics.\npnsc | Quantum statistical mechanics gives a more accurate answer. It uses a more general\np1qs | definition of entropy-but the principle of maximum entropy still applies!\nk5kg | I won't dive into any calculations just yet. Before doing a gas, we should do some simpler examples-like the die whose average roll is five. But I can't resist mentioning one philosophical point. In the box above I was hinting that maximum entropy works when your 'prior' is uniform:\ne002 | Suppose there are n possible outcomes. At first you have no reason to think any is more probable than any other.\n9klo | This is an important assumption: when it's not true, the principle of maximum entropy as we've stated it does not apply. But what if our set of events is something like a line? There's no obvious best probability measure on the line! And even good old Lebesgue measure dx depends on our choice of coordinates. To handle this, we need a generalization of the principle of maximum entropy, called the principle of maximum relative entropy.\npbpw | In short, a deeper treatment of the principle of maximum entropy pays more attention to our choice of 'prior': what we believe before we learn new facts. And it brings in the concept of 'relative entropy': entropy relative to that prior. But we won't get into this here, because we will always be using a very bland prior, like assuming that each of finitely many outcomes is equally likely.\n6tr9 | ADMITTING YOUR IGNORANCE\n06s5 | Suppose you describe your knowledge of a system with n states using a probability distribution P1, . . . , Pn. Then the Shannon information\nf3g2 | H equals negative Pi log Pi i equals one measures your ignorance of the system's state.\nzlrt | So, choosing the maximum-entropy probability distribution consistent with the facts you know amounts to not pretending to know more than you do.\n8szg | Remember: if we describe our knowledge using a probability distribution, its Shannon entropy says how much we expect to learn when we find out what's really going on. We can roughly say it measures our 'ignorance'-though ordinary language can be misleading here.\nhh47 | At first you think this ordinary six-sided die is fair. But then you learn that no, the average of the numbers that come up is five. What are the probabilities P1, . . . , Pn for the different faces to come up?\nadpv | This is tricky: you can imagine different answers!\nsy8a | You could guess the die lands with five up every time. In other words, P5 equals one. This indeed gives the correct average. But the entropy of this probability distribution is zero. So you're claiming to have no ignorance at all of what happens when you roll the die!\n11zj | Next you might guess that it lands with four up half the time and six up half the time. In other words, P4 equals P6 equals two. This probability distribution has one bit of entropy. Now you are admitting more ignorance. But how can you be so sure that five never comes up?\ndza1 | Next you might guess that P4 equals P6 equals four and P5 equals two. We can compute the entropy of this probability distribution. It's higher: one point five bits. Good, you're being more honest now! But how can you be sure that one, two, or three never come up? You are still pretending to know stuff!\n5gil | Keep improving your guess, finding probability distributions with mean five with bigger and bigger entropy. The bigger the entropy gets, the more you're admitting your ignorance! If you do it right, your guess will converge to the unique maximum-entropy solution.\nn91l | But there's a more systematic way to solve this problem.\niknb | THE BOLTZMANN DISTRIBUTION\n3ohm | Suppose you want to maximize the Shannon entropy\nenn6 | negative Pi log Pi i equals one\nvoir | of a probability distribution P1, . . . , Pn, subject to the constraint that the expected value of some quantity Ai is some number c:\nbxm5 | PiAi equals C (star) i equals one n\n6cnd | Then try the Boltzmann distribution:\nz41z | n exp(negative Beta Ai) Pi\nk1hn | Lexp(negative Beta Ai)\n853d | If you can find Beta that makes (star) hold, this is the answer you seek!\npx3f | How do you actually use the principle of maximum entropy?\n28wa | If you know the expected value of some quantity and want to maximize entropy given this, there's a great formula for the probability distribution that usually does the job! It's called the 'Boltzmann distribution'. In physics it also goes by the names 'Gibbs distribution' or 'canonical ensemble', and in statistics it's called an 'exponential family'.\n1jce | In the Boltzmann distribution, the probability Pi is proportional to exp(negative Beta A) where A is the quantity whose expected value you know. Since probabilities must sum to one, we must have\njfbn | n exp(negative Beta Ai) i equals one Lexp(negative Beta Ai)\ncn1a | Pi equals\n0j5u | It is then easy to find the expected value of A as a function of the number Beta: just plug these probabilities into the formula\nu6dp | (A) equals Ai Pi i equals one n\nhnb4 | The hard part is inverting this process and finding Beta if you know what you want (A) to be.\na0sw | When and why does the Boltzmann distribution actually work? That's a bit of a long story, so I'll explain it later. First, let's use the Boltzmann distribution to solve the puzzle I mentioned last time:\naak4 | At first you think this ordinary six-sided die is fair. But then you learn that no, the average of the numbers that come up is five. What are the probabilities P1, . . . , Pn for the different faces to come up? You can use the Boltzmann distribution to solve this puzzle!\n7l7y | To do it, take one is less than or equal to i is less than or equal to six and Ai equals i. Stick the Boltzmann distribution Pi into the formula Ci Ai Pi equals five and get a polynomial equation for exp(negative Beta). You can solve this with a computer and get exp(negative Beta) approximately one point eight seven seven.\np1kn | So, the probability of rolling the die and getting the number one to six is proportional to exp(negative Beta i) approximately one point eight seven seven. You can figure out the constant of proportionality by demanding that the probabilities sum to one-or just look at the formula for the Boltzmann distribution. You should get these probabilities:\n49ym | You can compute the entropy of this probability distribution, and you get roughly one point nine seven bits. You'll remember that last time we got entropies up to one point five bits just by making some rather silly guesses.\nabvj | So, using the Boltzmann distribution, you can find the maximum-entropy die that rolls five on average. Later, we'll see how the same math lets us find the maximum-entropy state of a box of gas that has some expected value of energy!\nk4xq | MAXIMIZATION SUBJECT TO A CONSTRAINT\nok5a | To maximize a smooth function F of several variables subject to a constraint on some smooth function G, look for a point where\n2ngm | for some number X.\n```",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1724700535,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_a2ff031fb5",
  "usage": {
    "completion_tokens": 1707,
    "prompt_tokens": 3395,
    "total_tokens": 5102
  }
}