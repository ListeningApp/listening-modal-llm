You will receive raw text from an OCR scan of a document or web page. Each line of input represents one textbox from the document.

Your task is to label each textbox with exactly one of the following labels:
  title: the primary document title, no more than one per document
  heading: the name of the following chapter or section (should never start in the middle of the previous paragraph)
  subheading: a secondary heading, often following another heading (should never start in the middle of the previous paragraph)
  body: main paragraph content: may start or end in the middle of a word, use context to follow paragraphs that are split across textboxes (if a textbox contains both heading and body text, label it "body")
  math: textbox primarily containing math
  imageDescription: standalone callout that only describes an image, chart, table, or similar (typically begins with "Table...", "Figure...", "Fig. ...", "Above...", "Left...", etc.)
  authors: names of contributors to this document
  institutions: list of universities, business, and other institutions affiliated with this document
  publisher: info about the publisher, or provided by the publisher, including copyright info
  pagenum: the page number
  headerOrFooter: boilerplate text page number, or redundant headings at the top/bottom of the page
  toc: the table of contents
  references: bibliographic references and resources
  acknowledgements: thanks and credits to contributors
  appendix: index, appendix, or any other end matter not part of the main thread (including related headings)
  table: tabular data, label, title, or subheading for a grid or list of data
  datum: numeric data or data label from a chart or list
  advertisement: promotional ads and marketing
  layout: non-content related to the layout of the page, e.g. sidebar navigation
  callout: pull quote or long block text that stands apart from the main text
  footnote: footnotes and endnotes near the bottom of the page
  keywords: list of keywords or tags
  url: web address or email
  other: any other text that does not belong to the main thread

Each line of input is prefixed with a textbox ID in the format `id | Text content`. The output must be in the format `id | label`.

Note that body text may be split across multiple textboxes, and textboxes may start or end in the middle of a sentence or word. Because the text was extracted from a web page or document, paragraphs of body text may be interrupted by multiple textboxes of redundant headers, footnotes, page numbers, tables, images, etc. For example, a real heading will never interrupt the middle of a sentence. Use context clues like this to carefully label each textbox.

EXAMPLE INPUT:
1pjs | Neuroscience Applied 1 (2022) 101012
o2kr | Contents lists available at ScienceDirect
v6sk | Neuroscience Applied
1wj6 | journal homepage: www.journals.elsevier.com/neuroscience-applied
936l | Research Articles
06yq | Microdosing with psychedelics to self-medicate for ADHD symptoms in adults: A naturalistic study
dpv6 | ARTICLE INFO
8r5j | ABSTRACT
2taz | Keywords: ADHD Microdosing Psychedelics Self-medication Well-being Time perception
o3ya | ADHD in adulthood is often overlooked, which negatively affects the individual's well-being. First-line phar- macological interventions are effective in many ADHD patients, relieving symptoms rapidly. However, there seems to be a proportion of individuals who discontinue, or fail to respond to these treatments. For these in- dividuals, alternative treatment options should be explored.
r5il | 1. Introduction
5gui | Attention deficit hyperactivity disorder (ADHD) is one of the most common developmental disorders worldwide. Prevalence research indicates that 2.6% of the adult population has persistent ADHD. ADHD in adults is often overlooked because of the high comorbidity rate and lack of knowledge about how ADHD is expressed in adulthood (Kooij et al.,
9d4o | Fig. 1. Flowchart of included participants for each time point.
82qc | 2019). In addition, ADHD is associated with deficits in various domains of cogni- tive functioning. Twenty-five percent of ADHD cases suffered from ADHD symptoms purely because of de- ficiencies in temporal processing (Sonuga-Barke et al., 2010).
ls7d | First-line ADHD treatments in adults mainly include pharmacological interventions to enhance dopaminergic and noradrenergic neurotrans- mission with stimulants. Overall, they have been proven to work effectively in adults with ADHD, inducing fast symptom relief and thereby enhancing the person's quality of life. In the
5esx | Table 1 Demographic information from respondents at baseline and the two and four- week time points.
t8vv | longer term, approximately twenty percent of ADHD patients discontinue their prescribed medication after six to nine months, thirty percent after one year, and half of them after two years.
xrd1 | 2. Material and methods
5746 | 2.1. Study design and participants
7toz | The study employed a naturalistic design, assessing the experiences of participants at baseline,
j346 | Neuroscience Applied 1 (2022)
m5ka | E.C.H.M. Haijen et al.
kx2c | REVIEW
v8uq | before they start MD on their own initiative, and at two and four weeks after MD initiation. The target population included adults diagnosed with ADHD and individuals who experienced ADHD symptoms to the extent that these interfered with their daily lives and who had not been diag- nosed with ADHD before. To be included in the analyses, participants needed to score above a cut-off point on at least one of the subscales of the Conner's Adult ADHD Rating Scale (CAARS-S:SV).
2zmw | Fig. 2. Mean scores of the CAARS-S:SV DSM-IV total symptoms T-scores at baseline (0W) and two (2W) and four weeks (4W) after MD (A) of the whole sample, and (B) per conventional ADHD medication use. Error bars represent mean error.
sdk3 | <LATEX>\lim _ { x ightarrow \infty } rac { 6 x ^ { 2 } + 1 0 0 } { 7 x ^ { 2 } - 1 0 } =</LATEX>
x972 | 2.2. Study procedure
k221 | Mean performance measures of the CAARS-

EXAMPLE OUTPUT:
1pjs | headerOrFooter
o2kr | publisher
v6sk | publisher
1wj6 | publisher
936l | headerOrFooter
06yq | title
dpv6 | other
8r5j | heading
2taz | keywords
o3ya | body
r5il | heading
5gui | body
9d4o | imageDescription
82qc | body
ls7d | body
5esx | imageDescription
t8vv | body
xrd1 | heading
5746 | subheading
7toz | body
j346 | headerOrFooter
m5ka | authors
kx2c | headerOrFooter
v8uq | body
2zmw | imageDescription
sdk3 | math
x972 | heading
k221 | body




ko3c | 119 · As in the variants of verbal <LATEX>n</LATEX> n-back tasks, we also have "spatial-with-noise", "spatial-with- feedback", and "spatial-with-CoT-reasoning" versions of the task. The prompts for the 121 the with-feedback and with-reasoning versions were basically the same as those for the
xek3 | 122 corresponding verbal tasks (see Table 1). For the spatial-with-noise version, we added a noise character (chosen from "#$%&@ ~~ ") to 1 to 3 unoccupied locations in the <LATEX>3 \times 3</LATEX> grid
bbpr | 124 on every trial. This is a first step to examine the LLM's spatial working memory when it 125 was not able to get the correct answer by simply doing string match.
5zyf | 126 . To further confirm that the LLM can really reason in a spatial way rather than trivially 127 performing some kind of string match under the hood, we further introduced two variants 128 that specifically require abstract spatial reasoning; an model that would otherwise simply 129 match strings would have failed. To achieve so, in these two tasks, a match is defined as 130 when the location of the letter <LATEX>\mathrm { X }</LATEX> is in the same row or column as the <LATEX>\mathrm { X }</LATEX> n trials ago. The 131 difference is whether identical locations also count as a match. We expect the version 132 excluding identical locations to be harder for the LLM to perform.
9own | 133 . We also explored whether the size of the grid <LATEX>\left( 3 \times 3 \right. ,</LATEX> <LATEX>4 \times 4 ,</LATEX> <LATEX>5 \times 5</LATEX> would influence the LLM's 134 performance. To the best of our knowledge, there hasn't been human studies exploring how 135 the number of all possible spatial locations would impact behavioural performance. In light 136 of this, we didn't have specific assumptions for how the LLM would perform differently 137 under these scenarios.
bqfb | 5
wwdt | Figure 5: The results from the different variants of verbal n-back experiments. Error bars represent ±1 SEM.
mn1r | 138 4 Results
5vqw | 139 To analyse the model's performance on our experiments, we used 4 widely accepted performance metrics reported in numerous human behavioral studies:
meyw | 141 Hit Rate: It is a performance measure used in various fields, including computer science, statistics,
2e62 | 142 and information retrieval. It represents the proportion of correct or successful outcomes out of the
75y3 | 143 total number of targets or true positives. Mathematically, it is calculated by
s2g2 | Hit Rate = Number of True Positives + Number of False Negatives Number of True Positives (1)
q1q2 | 144 False Alarm Rate: It quantifies the frequency at which a system or algorithm incorrectly identifies a
cb7r | 145 negative outcome as positive. Mathematically, it is calculated by
68xu | Number of False Positives
hd35 | False Alarm Rate = Number of False Positives + Number of True Negatives (2)
2zpk | 146 Accuracy: It is a commonly used performance metric that measures the correctness or reliability of a
epu7 | 147 system, model, or algorithm in making predictions or classifications. It represents the proportion of
7od1 | 148 correct predictions or classifications out of the total number of predictions or classifications made.
0bqy | 149 Mathematically, it is calculated by
b780 | Accuracy Number of Correct Responses Total Number of Trials
ccmb | (3)
ma93 | 150 Detection Sensitivity (d'): It is a statistical measure used to assess the ability of a diagnostic test
c6hh | 151 or classification model to accurately distinguish between two groups or conditions. It quantifies the
y7ea | 152 extent to which the test or model can correctly identify positive cases relative to negative cases while
nav4 | 153 minimizing false positives and false negatives. Mathematically, it is calculated by
gem8 | d' = ZHit Rate - ZFalse Alarm Rate (4)
e8nr | 154 where ZHit Rate and ZFalse Alarm Rate represent the z-score of Hit Rate and False Alarm Rate, respectively.
k78w | 155 In the current study, we did 50 blocks of tests for n = 1,2,3 in each experiment, which allows us to 156 calculate the standard error of mean (SEM) and draw error bars to visualise the reliability of our
g9du | 157 findings (for further details on the statistics tests we performed, see Supplementary Material).
f682 | 158 4.1 Verbal n-back experiments
li1k | 159 In all versions of the task, we observed a performance pattern strikingly consistent with human participants, with the LLM's performance declining significantly when n increased from 1 to 3 5,
pbea | 161 as shown in hit rate, accuracy, and d'. Compared to the base version, the verbal-with-noise variant
ccpt | 6
0fpw | Table 2: Prompts used for the spatial task variants described in Figure 4. Blue texts are to be selected as appropriate depending on the value of <LATEX>n</LATEX> in the n-back tasks. Other colored texts are inserted as appropriate, depending on the task variant. Note that spatial tasks with the variants described in Figure 3 are instead formatted similarly to Table 1.
dqn8 | 162 significantly made the LLM's performance worse. We observe that while chain-of-thought prompting 163 has significantly improved the performance of the language models in verbal task variants, feedback 164 on whether the model has performed correctly in the previous task failed to meaningfully improve 165 performance.
cop0 | 166 4.2 Spatial n-back experiments
lhkq | 167 In the four versions spatial tasks corresponding to the above verbal tasks, same patterns of performance 168 were basically replicated (Figure 6). CoT reasoning significantly made the LLM perform better,
82ju | 169 adding noise made the model perform worse. But in all versions of the task, ChatGPT suffered significant declines in performance as <LATEX>n</LATEX> increases.
y2qo | 171 When further evaluated whether the LLM could conduct abstract spatial reasoning. The results confirmed so (Figure 7). In line with our prediction, the LLM performed worse when identical 172
1piq | 173 locations are not defined a match, which means more abstract spatial reasoning would be required in
7fqw | 174 this scenario.
rhaz | 7
wak1 | Figure 6: The results from the variants of spatial n-back experiments corresponding to those in verbal ones. Error bars represent ±1 SEM.
wtfz | Figure 7: The results from the abstract reasoning variants of spatial n-back experiments. Error bars represent ±1 SEM.
qy1i | 175 Our explorations on the effect of grid size on the model performance yielded interesting results, too.
1k9e | 176 The LLM performed better when grid size was larger, especially as seen from the hit rate of d' results
5bjs | 177 in Figure 8.
uodj | 178 5 Discussion
k16v | 179 We argue that our experimental results firmly point to the conclusion that ChatGPT has limited 180 working memory capacity similar to humans. Even various prompting techniques (such as the 181 provision of feedback and the use of state-of-the-art chain-of-thought (CoT) prompting [40]) may be 182 used to improve its performance, the trend of performance decline as a function of increasing n still 183 bears striking resemblance to human performance shown in numerous previous work. The consistent 184 pattern of performance declines thus might be reflecting a fundamental constraint emerged from the 185 architecture of the model, suggesting an possibility that the low-level working memory mechanism 186 of LLMs might be similar to human working memory at least in some aspects.
b2lm | 187 In neuroscience, there are many unsolved problems on working memory, especially where and how working memory is encoded and maintained in the brain and why working memory capacity is 189 limited. We propose that, in light of the above observation, ChatGPT and or other large language models of similar calibre could be potentially used and tested as a modelling platform for studying human working memory, just as what neuroscientists have done in recent years using other artificial
ti7f | 192 neural networks [32]. Furthermore, future efforts aimed at interpreting activity of artificial neurons
oksu | 193 in LLMs [3] like ChatGPT would probably hold potential in informing the mechanisms of human 194 working memory.
wtow | 195 Our work also has some limitations. It would be important to test other LLMs on the same task we 196 used here, to test whether they exhibit similar performance patterns and whether they have different
o22k | 8
nwfz | Figure 8: The results from spatial n-back variants with different grid sizes. Error bars <LATEX>\mathrm { r e p r e s e n t } \pm 1</LATEX> SEM.
nvfy | 197 working memory capacity. It would also be helpful to test ChatGPT on other working memory span
mror | 198 tasks used in cognitive sciences [8, 12] to address the generalisability of n-back tasks as measurement tools.
zlxc | 200 Last but not the least, the current work opens a brand new topic in probing the cognitive abilities of 201 LLMs: if the working memory capacity of LLMs are fundamentally limited, then why? How their 202 architecture is related to the capacity limit? One possible explanation would be the self-attention 203 mechanism used in the Transformer architecture [37]. The self-attention mechanism computes a 204 weighted sum of input elements, where each element's weight is determined by its relevance to other
k0bw | 205 elements in the sequence. While this approach allows the model to focus on relevant information, it 206 may also lead to a diffusion of information across multiple input elements, making it challenging to 207 maintain and access specific pieces of information as n increases in n-back tasks.
aq5a | 208 References
sc5y | 9
0yab | 234 [9] Andrew R. A. Conway and Kristof Kovacs. Working Memory and Intelligence, page 504-527. Cambridge Handbooks in Psychology. Cambridge University Press, 2 edition, 2020.
w95z | 277 [25] Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically 278 ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv 279 preprint arXiv: 2104.08786, 2021.
0i1k | 10
dqlg | 280 [26] Jorge F. Mejías and Xiao-Jing Wang. Mechanisms of distributed working memory in a large- 281 scale network of macaque neocortex. eLife, 11:e72136, February 2022. doi : 10.7554/eLife. 282 72136.
p6pq | 326 [42] Aspen H Yoo and Anne GE Collins. How working memory and reinforcement learning 327 are intertwined: A cognitive, neural, and computational perspective. Journal of cognitive neuroscience, 34(4):551-568, 2022.
zse6 | 11