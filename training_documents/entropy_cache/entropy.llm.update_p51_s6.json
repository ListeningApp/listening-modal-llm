{
  "id": "chatcmpl-A3UFzajSClbj2ak9leFEXZ2fvvQ66",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "n8ae | Let's show that for a system with finitely many states i equals one, ..., n with energies E sub i, as the temperature T approaches zero from above, the entropy of the system in thermal equilibrium approaches k log N where N is the number of lowest-energy states. In thermal equilibrium\nojx4 | P sub i is proportional to e to the power of negative E sub i over k T.\nskze | Thus, all states with the lowest energy have the same probability, while as the temperature approaches zero from above, any higher-energy states have P sub i approaching zero. So, as the temperature approaches zero from above, the probability of the system being in any one of its N lowest-energy states approaches one over N, and we get\ns807 | the limit as T approaches zero from above of S(T) equals the limit as T approaches zero from above of the sum from i equals one to n of negative k P sub i log P sub i equals negative k times the sum from i equals one to N of one over N log one over N which is equal to k log N.\n9c3r | equals k log N.\nhqgv | In particular, if the system has just one lowest-energy state, we get the Third Law of Thermodynamics:\ntf20 | the limit as T approaches zero from above of S(T) equals zero.\nar2w | Here T approaching zero from above means that T is approaching zero from positive values.\nu6pz | But beware: for systems with lots of lowest-energy states, their entropy in thermal equilibrium can be large even near absolute zero! Also, a related problem: systems may take a ridiculously long time to reach equilibrium! This is especially true for systems that have many states whose energies are very near the lowest energy, like a piece of glass. You can put a piece of glass in a fancy refrigerator and try to cool it\n4bof | to near absolute zero. If it has one lowest-energy state, its entropy should approach zero. If this happened, the glass would change from glass to a crystal, which has less entropy. But nobody has seen glass turn into a crystal when they cool it down. If this happens, it probably does so only after an absurdly long time, much longer than the age of the Universe. This phenomenon is called 'frustration'. People like to argue about frustration and the Third Law, so I am not trying to give you the final word here, just alert you to the issue. You can learn a bit more here:\n6s2c | By the way: for systems with finitely many states, it's possible to have negative temperatures, and the Third Law has a counterpart saying what happens when the temperature approaches zero from below:\nugrk | Puzzle thirty-four. Show that for a system with finitely many states,\nteua | the limit as T approaches zero from below of S(T) equals k log M\nv8g1 | where M is the number of states of highest energy.\n2y4v | However, most systems we'll be studying won't have a state of highest energy.\nzl6z | MEASURING ENTROPY\nxmyv | If we assume the entropy of a system approaches zero as T approaches zero from above, we have\npu5p | the integral from zero to T sub one of d expected value of E over T equals S of T sub one. Using this assumption, we can do experiments to measure the entropy of different substances at standard temperature and pressure:\nmxd4 | iron: approximately five bits per atom\n6z51 | water: approximately twelve bits per molecule\nhxt0 | hydrogen: approximately twenty-three bits per molecule\nq1mg | People actually do experiments and use the above formula to figure out the entropy of many substances in thermal equilibrium assuming their entropy vanishes as the temperature approaches absolute zero. They slowly heat up a substance and keep track of how much energy is needed to raise its temperature as they go, so they can approximately calculate the integral shown. They usually report the answers in joules per kelvin per mole, but I enjoy 'bits per molecule'.\nbi15 | As you can see, liquids tend to have more entropy than solids, and gases tend to have even more. My goal in this course is to teach you how to approximately compute some of these entropies from first principles. Unfortunately the only substances that are simple enough for us to handle are gases.\nzm4q | This is a good opportunity to explain some conventions. A mole is defined to be exactly six point zero two two one four zero seven six times ten to the power of twenty-three this is called Avogadro's number, and it's close to the number of hydrogen atoms in a gram. A joule per kelvin of Gibbs entropy corresponds to about seven point two four two two nine seven times ten to the power of twenty-two nats of Shannon entropy: the number here is the reciprocal of Boltzmann's constant, which is defined to be exactly one point three eight zero six four nine times ten to the negative twenty-three joules per kelvin. A bit is log base two of e nats. From these three facts, we see one joule per kelvin of Gibbs entropy per mole corresponds to about zero point one seven three five one six bits per molecule of Shannon entropy.\n81dg | By the way, what is 'standard temperature and pressure'? Annoyingly, this phrase means different things to different organizations. I will try to always use it to mean a temperature of two hundred ninety-eight point one five kelvin and a pressure of one bar. The temperature here equals twenty-five degrees Celsius, which seems a bit random compared to zero degrees Celsius but convenient, because it's close to room temperature. A pressure of one bar, or more officially one hundred kilopascals, is slightly less than a 'standard atmosphere', which is a unit of pressure intended to equal the average air pressure at sea level. A pascal is an official SI unit: it's a pressure of one newton per square meter.\n8gco | THE EQUIPARTITION THEOREM\n0sc2 | Suppose the energy of a system with n degrees of freedom is a positive definite quadratic form E from R to R, for example\nyufa | E of x equals the sum from i equals one to n of C sub i x sub i squared over two\n4ggu | C sub i greater than zero\nvbg6 | Then in thermal equilibrium at temperature T, the expected value of the energy is\no5y1 | the expected value of E equals n k T over two\ngxus | where k is Boltzmann's constant.\nkfy5 | Temperature is very different from energy. But sometimes not very often, but sometimes the expected energy of a system in thermal equilibrium is proportional to its temperature. The equipartition theorem says this happens when the energy depends quadratically on several real variables, defining a positive definite quadratic form on R. For example, it happens for a classical harmonic oscillator.\neg1f | Some people get confused and try to apply the equipartition theorem where it doesn't apply. They foolishly conclude that temperature is always proportional to energy.\n4npi | This theorem does not apply to quantum systems. Indeed, when people tried to apply the equipartition theorem to a mirrored box of light they ran into a problem called the ultraviolet catastrophe. Classically the box of light is a system where the energy is a positive definite quadratic form, but n equals infinity, so they got an infinite expected value of the energy! Quantum mechanics saves the day and makes the answer finite.\nq93a | Radiation\ngxvi | Visible\nigdg | The equipartition theorem also doesn't apply to classical systems unless the energy is quadratic. So it's very limited in its applicability, but still useful.\n3u7c | Let's prove this result! To prove a theorem, you have to understand the definitions. We'll start with some background.\ncx2c | THE EQUIPARTITION THEOREM-BACKGROUND\nuoim | Suppose the energy of a system with n degrees of freedom is some function E from R to R\ndwsl | Let k be Boltzmann's constant. Suppose p from R to R is a probability distribution maximizing the entropy\nlpnq | S equals negative k times the integral over R of p of x log p of x d n x subject to a constraint on the expected energy\nc0cp | the expected value of E equals the integral over R of E of x p of x d n x\nvm3e | Then p must be a Boltzmann distribution:\nky8f | the integral over R of e to the power of negative beta E of x d n x times e to the power of negative beta E of x\ngtxr | p of x equals\nehqu | for some number beta greater than zero.\n5hiy | The temperature T is defined so that beta equals one over k T.",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725394655,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 1838,
    "prompt_tokens": 3488,
    "total_tokens": 5326
  }
}