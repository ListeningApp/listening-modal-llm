{
  "id": "chatcmpl-A3U6ibweOAYRXKNgQJ6utx3Hd4oiA",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "```xags | Four. Model details\njc47 | Table one lists the hyper-parameters and network architectures. We use log magnitude spectrogram with Hann windowing, fifty millisecond frame length, twelve point five millisecond frame shift, and two thousand forty-eight-point Fourier transform. We also found pre-emphasis (zero point ninety-seven) to be helpful. We use twenty-four kilohertz sampling rate for all experiments.\nko3c | We use R equals two (output layer reduction factor) for the MOS results in this paper, though larger R values (e.g., R equals five) also work well. We use the Adam optimizer with learning rate decay, which starts from zero point zero zero one and is reduced to zero point zero zero zero five, zero point zero zero zero three, and zero point zero zero zero one after five hundred thousand, one million, and two million global steps, respectively. We use a simple L one loss for both seq2seq decoder (mel-scale spectrogram) and post-processing net (linear-scale spectrogram). The two losses have equal weights.\nxek3 | We train using a batch size of thirty-two, where all sequences are padded to a max length. It's a common practice to train sequence models with a loss mask, which masks loss on zero-padded frames. However, we found that models trained this way don't know when to stop emitting outputs, causing repeated sounds towards the end. One simple trick to get around this problem is to also reconstruct the zero-padded frames.\nbbpr | Experiments\n5zyf | We train Tacotron on an internal North American English dataset, which contains about twenty-four point six hours of speech data spoken by a professional female speaker. The phrases are text normalized, e.g., \"sixteen\" is converted to \"sixteen\".\n9own | Five point one. Ablation analysis\nbqfb | We conduct a few ablation studies to understand the key components in our model. As is common for generative models, it's hard to compare models based on objective metrics, which often do not correlate well with perception. We mainly rely on visual comparisons instead. We strongly encourage readers to listen to the provided samples.\nwwdt | First, we compare with a vanilla seq2seq model. Both the encoder and decoder use two layers of residual RNNs, where each layer has two hundred fifty-six GRU cells (we tried LSTM and got similar results). No pre-net or post-processing net is used, and the decoder directly predicts linear-scale log magnitude spectrogram. We found that scheduled sampling (sampling rate zero point five) is required for this model to learn alignments and generalize. We show the learned attention alignment in Figure three. Figure three a reveals that the vanilla seq2seq learns a poor alignment. One problem is that attention tends to\nmn1r | get stuck for many frames before moving forward, which causes bad speech intelligibility in the synthesized signal. The naturalness and overall duration are destroyed as a result. In contrast, our model learns a clean and smooth alignment, as shown in Figure three c.\n5vqw | Second, we compare with a model with the CBHG encoder replaced by a two-layer residual GRU encoder. The rest of the model, including the encoder pre-net, remain exactly the same. Comparing Figure three b and Figure three c, we can see that the alignment from the GRU encoder is noisier. Listening to synthesized signals, we found that noisy alignment often leads to mispronunciations. The CBHG encoder reduces overfitting and generalizes well to long and complex phrases.\nmeyw | Figures four a and four b demonstrate the benefit of using the post-processing net. We trained a model without the post-processing net while keeping all the other components untouched (except that the decoder RNN predicts linear-scale spectrogram). With more contextual information, the prediction from the post-processing net contains better resolved harmonics (e.g., higher harmonics between bins one hundred and four hundred) and high frequency formant structure, which reduces synthesis artifacts.\n2e62 | Five point two. Mean opinion score tests\n75y3 | We conduct mean opinion score tests, where the subjects were asked to rate the naturalness of the stimuli in a five-point Likert scale score. The MOS tests were crowdsourced from native speakers.\ns2g2 | One hundred unseen phrases were used for the tests and each phrase received eight ratings. When computing MOS, we only include ratings where headphones were used. We compare our model with a parametric (based on LSTM) and a concatenative system, both of which are in production. As shown in Table two, Tacotron achieves an MOS of three point eight two, which outperforms the parametric system. Given the strong baselines and the artifacts introduced by the Griffin-Lim synthesis, this represents a very promising result.\nq1q2 | Six. Discussions\ncb7r | We have proposed Tacotron, an integrated end-to-end generative TTS model that takes a character sequence as input and outputs the corresponding spectrogram. With a very simple waveform synthesis module, it achieves a three point eight two MOS score on U S English, outperforming a production parametric system in terms of naturalness. Tacotron is frame-based, so the inference is substantially faster than sample-level autoregressive methods. Unlike previous work, Tacotron does not need hand-engineered linguistic features or complex components such as an HMM aligner. It can be trained from scratch with random initialization. We perform simple text normalization, though recent advancements in learned text normalization may render this unnecessary in the future.\n68xu | We have yet to investigate many aspects of our model; many early design decisions have gone unchanged. Our output layer, attention module, loss function, and Griffin-Lim-based waveform synthesizer are all ripe for improvement. For example, it's well known that Griffin-Lim outputs may have audible artifacts. We are currently working on fast and high-quality neural-network-based spectrogram inversion.\n```",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725394080,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 1223,
    "prompt_tokens": 3012,
    "total_tokens": 4235
  }
}