You will receive raw text from an OCR scan of a document or web page. Each line of input represents one textbox from the document.

Your task is to label each textbox with exactly one of the following labels:
  title: the primary document title, no more than one per document
  heading: the name of the following chapter or section (should never start in the middle of the previous paragraph)
  subheading: a secondary heading, often following another heading (should never start in the middle of the previous paragraph)
  body: main paragraph content: may start or end in the middle of a word, use context to follow paragraphs that are split across textboxes (if a textbox contains both heading and body text, label it "body")
  math: textbox primarily containing math
  imageDescription: standalone callout that only describes an image, chart, table, or similar (typically begins with "Table...", "Figure...", "Fig. ...", "Above...", "Left...", etc.)
  authors: names of contributors to this document
  institutions: list of universities, business, and other institutions affiliated with this document
  publisher: info about the publisher, or provided by the publisher, including copyright info
  pagenum: the page number
  headerOrFooter: boilerplate text page number, or redundant headings at the top/bottom of the page
  toc: the table of contents
  references: bibliographic references and resources
  acknowledgements: thanks and credits to contributors
  appendix: index, appendix, or any other end matter not part of the main thread (including related headings)
  table: tabular data, label, title, or subheading for a grid or list of data
  datum: numeric data or data label from a chart or list
  advertisement: promotional ads and marketing
  layout: non-content related to the layout of the page, e.g. sidebar navigation
  callout: pull quote or long block text that stands apart from the main text
  footnote: footnotes and endnotes near the bottom of the page
  keywords: list of keywords or tags
  url: web address or email
  other: any other text that does not belong to the main thread

Each line of input is prefixed with a textbox ID in the format `id | Text content`. The output must be in the format `id | label`.

Note that body text may be split across multiple textboxes, and textboxes may start or end in the middle of a sentence or word. Because the text was extracted from a web page or document, paragraphs of body text may be interrupted by multiple textboxes of redundant headers, footnotes, page numbers, tables, images, etc. For example, a real heading will never interrupt the middle of a sentence. Use context clues like this to carefully label each textbox.

EXAMPLE INPUT:
1pjs | Neuroscience Applied 1 (2022) 101012
o2kr | Contents lists available at ScienceDirect
v6sk | Neuroscience Applied
1wj6 | journal homepage: www.journals.elsevier.com/neuroscience-applied
936l | Research Articles
06yq | Microdosing with psychedelics to self-medicate for ADHD symptoms in adults: A naturalistic study
dpv6 | ARTICLE INFO
8r5j | ABSTRACT
2taz | Keywords: ADHD Microdosing Psychedelics Self-medication Well-being Time perception
o3ya | ADHD in adulthood is often overlooked, which negatively affects the individual's well-being. First-line phar- macological interventions are effective in many ADHD patients, relieving symptoms rapidly. However, there seems to be a proportion of individuals who discontinue, or fail to respond to these treatments. For these in- dividuals, alternative treatment options should be explored.
r5il | 1. Introduction
5gui | Attention deficit hyperactivity disorder (ADHD) is one of the most common developmental disorders worldwide. Prevalence research indicates that 2.6% of the adult population has persistent ADHD. ADHD in adults is often overlooked because of the high comorbidity rate and lack of knowledge about how ADHD is expressed in adulthood (Kooij et al.,
9d4o | Fig. 1. Flowchart of included participants for each time point.
82qc | 2019). In addition, ADHD is associated with deficits in various domains of cogni- tive functioning. Twenty-five percent of ADHD cases suffered from ADHD symptoms purely because of de- ficiencies in temporal processing (Sonuga-Barke et al., 2010).
ls7d | First-line ADHD treatments in adults mainly include pharmacological interventions to enhance dopaminergic and noradrenergic neurotrans- mission with stimulants. Overall, they have been proven to work effectively in adults with ADHD, inducing fast symptom relief and thereby enhancing the person's quality of life. In the
5esx | Table 1 Demographic information from respondents at baseline and the two and four- week time points.
t8vv | longer term, approximately twenty percent of ADHD patients discontinue their prescribed medication after six to nine months, thirty percent after one year, and half of them after two years.
xrd1 | 2. Material and methods
5746 | 2.1. Study design and participants
7toz | The study employed a naturalistic design, assessing the experiences of participants at baseline,
j346 | Neuroscience Applied 1 (2022)
m5ka | E.C.H.M. Haijen et al.
kx2c | REVIEW
v8uq | before they start MD on their own initiative, and at two and four weeks after MD initiation. The target population included adults diagnosed with ADHD and individuals who experienced ADHD symptoms to the extent that these interfered with their daily lives and who had not been diag- nosed with ADHD before. To be included in the analyses, participants needed to score above a cut-off point on at least one of the subscales of the Conner's Adult ADHD Rating Scale (CAARS-S:SV).
2zmw | Fig. 2. Mean scores of the CAARS-S:SV DSM-IV total symptoms T-scores at baseline (0W) and two (2W) and four weeks (4W) after MD (A) of the whole sample, and (B) per conventional ADHD medication use. Error bars represent mean error.
sdk3 | <LATEX>\lim _ { x ightarrow \infty } rac { 6 x ^ { 2 } + 1 0 0 } { 7 x ^ { 2 } - 1 0 } =</LATEX>
x972 | 2.2. Study procedure
k221 | Mean performance measures of the CAARS-

EXAMPLE OUTPUT:
1pjs | headerOrFooter
o2kr | publisher
v6sk | publisher
1wj6 | publisher
936l | headerOrFooter
06yq | title
dpv6 | other
8r5j | heading
2taz | keywords
o3ya | body
r5il | heading
5gui | body
9d4o | imageDescription
82qc | body
ls7d | body
5esx | imageDescription
t8vv | body
xrd1 | heading
5746 | subheading
7toz | body
j346 | headerOrFooter
m5ka | authors
kx2c | headerOrFooter
v8uq | body
2zmw | imageDescription
sdk3 | math
x972 | heading
k221 | body




ccpt | 13: Ok < Algorithm 5 with
0fpw | <LATEX>\alpha _ { 0 } = 1 \text { and } \left( 1 1 \right) .</LATEX> 14: else if <LATEX>\mathrm { D } _ { \mathrm { t y p e } } = \mathrm { N P C }</LATEX> then
dqn8 | 15:
cop0 | <LATEX>\alpha _ { k } \leftarrow \text { Algorithm } 6 \text { with } \alpha _ { 0 } = 1 \text { and } \left( 1 1 \right) .</LATEX> 16: end if
lhkq | 17:
82ju | <LATEX>\mathrm { x } _ { k + 1 } = \mathcal{P} \left( \mathrm { x } _ { k } + \alpha _ { k } \mathrm { p } _ { k } \right)</LATEX> 18: end for
y2qo | on the relative residual prior to termination does too. In particular, at first glance, this might suggest that the smaller the inexactness tolerance <LATEX>\eta ,</LATEX> the more iterations MINRES is expected to perform before NPC detection. We argue that this is not the case. Firstly, an upper bound on the number of MINRES iterations until a NPC direction is encountered is independent of the termination criteria <LATEX>\eta</LATEX> Liu & Roosta (2022b, Corollary 2). In fact, by construction, the MIN- RES iterates are independent of the termination tolerance <LATEX>\eta</LATEX> and the magnitude of <LATEX>\| \mathrm { g } \|</LATEX> see discussion and numeri- cal examples around Liu & Roosta (2022b, Assumption 4). Additionally, in the case where <LATEX>\mathrm { g } \notin \mathrm { R a n g e } \left( \mathrm { H } \right) ,</LATEX> we always have <LATEX>\| \mathrm { r } ^ { \left( t - 1 \right) } \| \geq \| \left( \mathrm { I } - \mathrm { H H } ^ { \dagger } \right) \mathrm { g } \| ,</LATEX> which is clearly indepen- <LATEX>\mathrm { d e n t } \mathrm { o f } \eta .</LATEX> Together, these lines of argumentation constitute our justification for Assumption 3.6.
1piq | Recall that Algorithm 1 includes a manual verification of user specified strongly positive curvature over <LATEX>\mathcal{K} _ { t } \left( \mathrm { H } , \mathrm { g } \right)</LATEX> in <LATEX>\mathrm { D } _ { \mathrm { t y p e } } = \mathrm { S O I }</LATEX> case, while Algorithm 2 only certifies strict positive curvature through the NPC condition (9). Liu & Roosta (2022a) demonstrated that as long as the NPC con- dition (9) has not been detected, we have <LATEX>\mathrm { T } _ { t } \succ 0</LATEX> where <LATEX>\mathrm { T } _ { t } \in \mathbb{R} ^ { t \times t }</LATEX> is the symmetric tridiagonal matrix obtained in the <LATEX>t ^ { \mathrm { t h } }</LATEX> iteration of MINRES (see Appendix <LATEX>\mathrm { A }</LATEX> for more
7fqw | Inexact Newton-type Methods for Optimisation with Nonnegativity Constraints
rhaz | details). Our next assumption strengthens this notion.
wak1 | Assumption 3.7. There exists <LATEX>\sigma > 0</LATEX> such that for any <LATEX>\mathrm { x } _ { k }</LATEX> in the sequence of SOL type iterates returned by Algorithm 2, we have
wtfz | <LATEX>\mathrm { T } _ { t } \succ \sigma \mathrm { I } .</LATEX> Assumption 3.7 implies that, as long as the NPC condition (9) has not been detected, for any <LATEX>\mathrm { v } \in \mathcal{K} _ { t } \left( \mathrm { H } , \mathrm { g } \right)</LATEX> we have <LATEX>\langle \mathrm { v } , \mathrm { H v } \rangle \geq \sigma \| \mathrm { v } \| ^ { 2 } .</LATEX> Assumption 3.7 is satisfied by an ob- jective function whose Hessian contains positive g-relevant eigenvalues (eigenspaces not orthogonal to the gradient) uniformly separated from zero. A simple example is an under-determined least-squares problem.
qy1i | Together, Assumptions 3.6 and 3.7 allow us to control the curvature of our step, which is necessary to obtain an im- proved rate over Algorithm 1 using a Lipschitz Hessian upper bound. We now present the convergence result for Algorithm 2. We defer the proof to Appendix C.
1k9e | Theorem 3.8 (Global Complexity of Algorithm <LATEX>\left. 2 \right) .</LATEX> Let <LATEX>\epsilon _ { g } \in \left( 0 , 1 \right) .</LATEX> Under Assumptions 3.1, 3.2 and 3.5 to 3.7, if we choose <LATEX>\delta _ { k } = \epsilon _ { k } = \epsilon _ { g } ^ { 1 / 2 } ,</LATEX> Algorithm 2 produces an Eg-FO point in at most <LATEX>\mathcal{O} \left( \epsilon _ { g } ^ { - 3 / 2 } \right)</LATEX> iterations.
5bjs | Remark 3.9. A direct corollary to Theorems 3.3 and 3.8, under some mild additional assumptions, is a bound on the operational complexity in terms of gradient and Hessian- vector product evaluations. In particular, to produce a <LATEX>\epsilon _ { g } \mathrm { - F O }</LATEX> point, the operation complexity for Algorithms 1 and 2 is, respectively, <LATEX>\mathcal{O} \left( \epsilon _ { g } ^ { - 2 } \right)</LATEX> and <LATEX>\widetilde { \mathcal{O} } \left( \epsilon _ { g } ^ { - 3 / 2 } \right)</LATEX> see Appendix D.
uodj | Remark 3.10. In all our algorithms, each step includes the Newton-MR component. The integration of the gradient and Newton-MR step is feasible in our algorithm due to the properties of the MINRES iterates (Lemmas A.2 and A.3), allowing for a more flexible analysis with only first-order information. In contrast, it appears that second-order infor- mation is crucial for achieving descent with the capped-CG procedure, a central aspect of Xie & Wright (2023). This constraint prevents the algorithm from taking a step simulta- neously comprised of gradient and Newton-CG components.
k16v | 3.4. Local Convergence
b2lm | An advantage of the original TMP method of Bertsekas (1982) is that we get fast local convergence, a property that is shared by many Newton-type methods. We now show that our algorithm, in a slightly modified form, also exhibits this property. The basis for the local convergence is the fact that, under certain conditions, projected gradient algorithms are capable of identifying the true set of active constraints in a finite number of iterations. This result was first establish for projected gradient with bound constraints in Bertsekas (1976) but has been extended to a variety of constraints (Burke & More, 1988; Burke, 1990; Wright, 1993; Sun et al., 2019). In the case of two-metric projection, once the active set is identified, the combined step reduces to an
ti7f | unconstrained Newton step in the inactive set.
oksu | For the analysis, we consider a "local phase" variant of Algorithm 1. Specifically, we maintain flexibility in defining the outer and inner termination conditions and tolerances, eliminate the strongly positive curvature validation, and only perform backtracking line search from <LATEX>\alpha _ { 0 } = 1</LATEX> to ensure the step length remains bounded. The pseudo-code for this local phase version is given in Algorithm 4 for completeness. To show that the active set is identified in finite number of iterations, we need non-degeneracy and second-order sufficiency assumptions, which are standard in this context.
wtow | Assumption 3.11. A local minima, <LATEX>X _ { * } ,</LATEX> is non-degenerate if
o22k | <LATEX>\left[ \mathrm { g } \left( \mathrm { x } _ { * } \right) \right] ^ { i } > 0 ,</LATEX> <LATEX>\forall i \in \mathcal{A} \left( \mathrm { x } _ { * } , 0 \right) .</LATEX> Assumption 3.12. A local minima, <LATEX>\mathrm { X } _ { * } ,</LATEX> satisfies the second- order sufficiency condition if <LATEX>0 < \langle \mathrm { z } , \mathrm { H } \left( \mathrm { x } _ { * } \right) \mathrm { z } \rangle</LATEX> for all <LATEX>\mathrm { z } \neq 0</LATEX> such that <LATEX>\mathrm { z } ^ { i } = 0</LATEX> if
nwfz | <LATEX>i \in \mathcal{A} \left( \mathrm { x } _ { * } , 0 \right) .</LATEX> Theorem 3.13 (Active Set Identification). Let <LATEX>f</LATEX> satisfy <LATEX>A s -</LATEX> sumption 3.1 and x+ be a local minima satisfying Assump- tions 3.11 and 3.12. Let <LATEX>\left\{ \mathrm { x } _ { k } \right\}</LATEX> be the sequence of iterates generated by Algorithm 4 with <LATEX>\delta</LATEX> chosen according to (44). There exists <LATEX>\Delta _ { a c r v } > 0</LATEX> such that if <LATEX>\mathrm { x } _ { \bar { k } } \in B \left( \mathrm { x } _ { * } , \Delta _ { a c w } \right) ,</LATEX> then <LATEX>\mathcal{A} \left( \mathrm { x } _ { k } , \delta \right) = \mathcal{A} \left( \mathrm { x } _ { k } , 0 \right) = \mathcal{A} \left( \mathrm { x } _ { * } , 0 \right)</LATEX> all
nvfy | <LATEX>k \geq \bar { k } + 1 .</LATEX> We defer the proof to Appendix E. Once the active set is identified, our method reduces to unconstrained Newton- MR on the inactive set. Local convergence is therefore a simple corollary of Theorem 3.13.
mror | Corollary 3.14 (Local Convergence). For <LATEX>k \geq \bar { k } + 1</LATEX> (cf. Theorem 3.13), the convergence of Algorithm 4 is driven <LATEX>b y</LATEX> the local properties of the Newton-MR portion of the step.
zlxc | Remark 3.15. The local convergence of Newton-MR is sim- ilar to that of other inexact Newton methods. Suppose that we use a relative residual tolerance, <LATEX>\| \mathrm { r } ^ { \mathcal{I} } \| \leq \eta \| \mathrm { g } ^ { \mathcal{I} } \| ,</LATEX> as the criteria for the MINRES termination. Under Assump- tion 3.12, we know that <LATEX>\mathrm { H } \left( \mathrm { x } _ { * } \right)</LATEX> is positive definite on the inactive indices. Therefore, by applying Nocedal & Wright (2006, Theorem 7.1 and 7.2), we obtain a superlinear conver- gence if we choose <LATEX>\eta = \mathcal{O} \left( 1 \right)</LATEX> and let <LATEX>\mathrm { x } _ { k }</LATEX> be close enough to <LATEX>\mathrm { X } _ { * }</LATEX> *. If we choose <LATEX>\eta = \mathcal{O} \left( \| \mathrm { g } _ { k } \| \right)</LATEX> and the Hessian is Lipschitz then we can improve the rate to quadratic.
k0bw | Remark 3.16. A central ingredient in the projected Newton- CG of Xie & Wright (2023) is the damping of the Hessian in the form of diagonal perturbation (i.e., <LATEX>\mathrm { H } + \epsilon \mathrm { I }</LATEX> for all Newton-CG steps in the inactive set. While this facilitates an optimal global complexity, an unfortunate consequence, at least in theory, is that the algorithm no longer enjoys a guaranteed fast "Newton-type" local convergence rate. In other words, one can at best show linear rates in local regimes.
aq5a | Inexact Newton-type Methods for Optimisation with Nonnegativity Constraints
sc5y | 4. Numerical Experiments
0yab | We now compare the performance of our method for solving (1) with several alternatives using various convex and non- convex examples. Specifically, we consider Algorithm 2 (denoted by MR), projected Newton-CG (denoted by CG) as in Xie & Wright (2023, Algorithm 1), and projected gradient with line search (denoted by PG) (Beck, 2017). For convex problems, we also include FISTA with line search (Beck & Teboulle, 2009), while for non-convex settings, we compare against the proximal gradient with momentum and fine-tuned constant step size (denoted by PGM) from Lin et al. (2020, Algorithm 4.1). We exclude proximal Newton methods due to the difficulty of solving its subproblems at each iteration. We also do not consider the Newton-CG log barrier method (O'Neill & Wright, 2020) due to poor practical performance observed in Xie & Wright (2023).