{
  "id": "chatcmpl-A3U6R7vsbyNzSyfkT02NNWbf36rWt",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "epyi | We use eight equals ten to the negative three for the Huber loss. We find that using larger values of and pushes the model to overfit the small compute regime and poorly predict held-out data from larger runs. We find that using a and smaller than ten to the negative three does not impact the resulting predictions.\ny51p | D point three. Predicted compute optimal frontier for all three methods\nx95m | For Approaches two and three, we show the estimated model size and number of training tokens for a variety of compute budgets in Table A-three. We plot the predicted number of tokens and parameters for a variety of FLOP budgets for the three methods in Figure A-three.\nyfum | D point four. Small-scale comparison to Kaplan et al. twenty twenty\nppx4 | For ten to the twenty-one FLOPs, we perform a head-to-head comparison of a model predicted by Approach one and that predicted by Kaplan et al. twenty twenty. For both models, we use a batch size of zero point five million tokens and a\nqf11 | maximum learning rate of one point five times ten to the power of negative four that decays by ten times. From Kaplan et al. twenty twenty, we find that the optimal model size should be four point six eight billion parameters. From our approach one, we estimate a two point eight six billion parameter model should be optimal. We train a four point seven four billion parameter and a two point eight billion parameter transformer to test this hypothesis, using the same depth-to-width ratio to avoid as many confounding factors as possible. We find that our predicted model outperforms the model predicted by Kaplan et al. twenty twenty as shown in Figure A-four.\n8oqt | E. Curvature of the FLOP-loss frontier\n6zah | We observe that as models increase there is a curvature in the FLOP-minimal loss frontier. This means that projections from very small models lead to different predictions than those from larger models. In Figure A-five we show linear fits using the first, middle, and final third of frontier-points. In this work, we do not take this into account and we leave this as interesting future work as it suggests that even smaller models may be optimal for large FLOP budgets.\n4v5o | F. FLOPs computation\nha9e | We include all training FLOPs, including those contributed to by the embedding matrices, in our analysis. Note that we also count embeddings matrices in the total parameter count. For large models the FLOP and parameter contribution of embedding matrices is small. We use a factor of two to describe the multiply accumulate cost. For the forward pass, we consider contributions from:\nxrnt | · Embeddings\nc8ra | · Attention (Single Layer)\np1l0 | - Key, query and value projections: two times three times sequence length times dimensional model times key size times number of heads\n1i8b | - Key at Query logits: two times sequence length times sequence length times key size times number of heads\n5sfj | - Softmax: three times number of heads times sequence length times sequence length\nvk4h | - Softmax at query reductions: two times sequence length times sequence length times key size times number of heads\nnsm3 | - Final Linear: two times sequence length times key size times number of heads times dimensional model\njdfw | · Dense Block (Single Layer)\nuc9r | - Two times sequence length times dimensional model times feed-forward size plus dimensional model times feed-forward size\nl0ap | · Final Logits\nx815 | - Two times sequence length times dimensional model times vocabulary size\nepbj | · Total forward pass FLOPs: embeddings plus number of layers times total attention plus dense block plus logits\nnmn2 | As in Kaplan et al. twenty twenty we assume that the backward pass has twice the FLOPs of the forward pass. We show a comparison between our calculation and that using the common approximation C equals six D N where C is FLOPs, D is the number of training tokens, and N is the number of parameters in Table A-four. We find the differences in FLOP calculation to be very small and they do not impact our analysis. Compared to the results presented in Rae et al. twenty twenty-one, we use a slightly more\ng3tm | accurate calculation giving a slightly different value six point three times ten to the power of twenty-three compared to five point seven six times ten to the power of twenty-three.\ntfra | G. Other differences between Chinchilla and Gopher\n5lm9 | Beyond differences in model size and number of training tokens, there are some additional minor differences between Chinchilla and Gopher. Specifically, Gopher was trained with Adam whereas Chinchilla was trained with AdamW. Furthermore, as discussed in Lessons Learned in Rae et al. twenty twenty-one, Chinchilla stored a higher-precision copy of the weights in the sharded optimiser state.\nwp2c | We show comparisons of models trained with Adam and AdamW in Figure A-six and Figure A-seven. We find that, independent of the learning rate schedule, AdamW trained models outperform models trained with Adam. In Figure A-six we show a comparison of a six hundred eighty million parameter model trained\nus5v | with and without the higher precision copy of the weights and with Adam or AdamW for comparison.\nrs2m | H. Results\nuz9y | H point one. The Pile\n8ov0 | H point two. MMLU\nmj9m | H point three. Winogender Setup\n94h3 | We follow the same setup as in Rae et al. twenty twenty-one. To test coreference resolution in Chinchilla, we input a sentence which includes a pronoun reference, then measure the probability of the model completing the sentence \"Pronoun refers to the\" with different sentence roles \"librarian\" and \"child\" in this example. Each example is annotated with the correct pronoun resolution. Each sentence is tested with a female, male, and gender-neutral pronoun. An unbiased model would correctly predict which word the pronoun refers to regardless of pronoun gender.\n9jx1 | H point four. BIG-bench\nc3lz | I. Model Card\n8bwk | We present the Chinchilla model card in Table A-eight, following the framework presented by Mitchell et al.\nez9s | Primary Intended Users DeepMind researchers. We will not make this model available publicly.\nmqot | Datasets\noc99 | We did not investigate intersectional biases.\ngipz | Intersectional Results\nb4wj | J. List of trained models\n5xlt | In Table A-nine we list the model size and configuration of all models used in this study. Many models have been trained multiple times, for a different number of training steps.",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725394063,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 1412,
    "prompt_tokens": 3211,
    "total_tokens": 4623
  }
}