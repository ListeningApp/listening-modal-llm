{
  "id": "chatcmpl-A3UParycmwmImQqrl9MyWW1q173LW",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "p2n3 | We follow and use extracted features obtained by a pre-trained network on ImageNet instead of R G B images as direct input for our models. Such networks have been shown to be universal feature extractors whose outputs\nohh7 | carry relevant semantics for industrial anomaly detection.\nvbni | In addition to R G B data, our approach is easily extendable to multimodal inputs including three-D data. If three-D data is available, we concatenate depth maps to these features along the channels. Since the feature maps are reduced in height and width compared to the depth map resolution by a factor D, we apply pixel-unshuffling by grouping a depth image patch of D by D pixels as one pixel with D squared channels to match the dimensions of the feature maps.\ndgcw | Any three-D data that may be present is used to extract the foreground. This is straightforward and reasonable whenever the background is static or planar, which is the case for almost all real applications. Pixels that are in the background are ignored when optimizing the teacher and student by masking the distance and negative log likelihood loss, which are introduced in Sections three point one and three point two. If not three-D data is available, the whole image is considered as foreground. Details of the foreground extraction are given in Section four point two point one.\ngvwz | Similar to, we use a sinusoidal positional encoding for the spatial dimensions of the input maps as a condition for the normalizing flow F T. In this way, the occurrence of a feature is related to its position to detect anomalies such as misplaced objects. An overview of our pipeline is given in Figure three.\nsk5m | Three point one. Teacher\nqws8 | Similar to we train a normalizing flow based on Real N V P to transform the training distribution to a normal distribution N zero, I. In contrast to previous work, we do not use the outputs to compute likelihoods and thereby obtain anomaly scores directly. Instead, we interpret this training as a pretext task to create targets for our student network.\n877u | The normalizing flow consists of multiple subsequent affine coupling blocks. Let the input X be\nrzhj | feature maps with N feat features of size W by H. Within these blocks, the channels of the input X are split evenly along the channels into the parts X one and X two after randomly choosing a permutation that remains fixed. These parts are each concatenated with a positional encoding C as a static condition. Both are used to compute scaling and shift parameters for an affine transformation of the counterpart by having subnetworks S one and T one for each part:\nbfax | Y two equals X two times S one of X one and C plus T one of X one and C\nf2y8 | One\no7qm | Y one equals X one times E S two of X two and C plus T two of X two and C,\nd52l | where O is the element-wise product and concatenation. The output of one coupling block is the concatenation of Y one and Y two along the channels. Note that the number of dimensions of input and output does not change due to invertibility.\nqlbb | To stabilize training, we apply alpha-clamping of scalar coefficients as in and the gamma-trick as in. Using the change-of-variable formula with Z as our final output\nnhsg | Two a det\nhy0l | Two\n4jmz | we minimize the negative log likelihood with P Z as the normal distribution N zero, I by optimizing the mean of\nb7cq | C I equals minus log P X X I J equals X I J minus twelve O G det A Z I J O X I J three\n0b39 | over all foreground pixels at pixel position I, J.\nqecd | Three point two. Student\n1o7p | As opposed to the teacher, the student is a conventional feed-forward network that does not map injectively or surjectively. We propose a simple fully convolutional network with residual blocks which is shown in Figure four. Each residual block consists of two sequences of three by three convolutional layers, batch normalization and leaky ReLU activations. We add convolutions as the first and last layer to increase and decrease the feature dimensions.\ncp13 | Similarly to the teacher, the student takes image features as input which are concatenated with three-D data if available. In addition, the positional encoding C is concatenated. The output dimensions match the teacher to enable pixel-wise distance computation. We minimize the squared L two-distance between student outputs F S X and the teacher outputs F T X on training samples X in X, given the training set X, at a pixel position I, J of the output:\nbgre | C I J equals one F X I J minus F T X I J squared.\n57lp | Four\nukth | Averaging over all foreground pixels gives us the final loss. The distance C S is also used in testing to obtain an anomaly score on image level: Ignoring the anomaly scores of background pixels, we aggregate the pixel distances of one sample by computing either the maximum or the mean over the pixels.\nkpvt | Four. Experiments\njie2 | Four point one. Datasets\nd0ik | To demonstrate the benefits of our method on a wide range of industrial inspection scenarios, we evaluate with a diverse set of twenty-five scenarios in total, including natural objects, industrial components and textures in two-D and three-D. Table one shows an overview of the used benchmark datasets MVTec A D and MVTec three-D A D. For both datasets, the training set only contains defect-free data and the test set contains defect-free and defective examples. In addition to image-level labels, the datasets also provide pixel-level annotations about defective regions which we use to evaluate the segmentation of defects.\n0x0s | MVTec A D, which will be called MVT two-D in the following, is a high-resolution two-D R G B image dataset containing\n4exe | ten object and five texture categories. The total of seventy-three defect types in the test set appear, for example, in the form of displacements, cracks or scratches in various sizes and shapes. The images have a side length of seven hundred to one thousand twenty-four pixels.\n0myz | MVTec three-D A D, to which we refer to as MVT three-D, is a very recent three-D dataset containing two-D R G B images paired with three-D scans for ten categories. These categories include deformable and non-deformable objects, partially with natural variations (e.g. peach and carrot). In addition to the defect types in MVT two-D there are also defects that are only recognizable from the depth map, such as indentations. On the other hand, there are anomalies such as discoloration that can only be perceived from the R G B data. The R G B images have a resolution of four hundred to eight hundred pixels per side, paired with rasterized three-D point clouds at the same resolution.\nxh3i | Four point two. Implementation Details\nsyiw | Four point two point one Image Preprocessing\nkg3t | Following, we use the layer thirty-six output of EfficientNet B five pre-trained on ImageNet as a feature extractor. This feature extractor is not trained during training of the student and teacher networks. The images are resized to a resolution of seven hundred sixty-eight by seven hundred sixty-eight pixels resulting in feature maps of size twenty-four by twenty-four with three hundred-four channels.\nlvae | Four point two point two Three-D Preprocessing",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725395250,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 1551,
    "prompt_tokens": 3377,
    "total_tokens": 4928
  }
}