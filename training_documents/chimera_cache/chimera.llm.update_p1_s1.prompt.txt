You will receive raw text from an OCR scan of a document or web page. Each line of input represents one textbox from the document. Your task is to faithfully reproduce the text for our text-to-speech engine so that it is easily read aloud.

Note that the text might not start at the beginning and paragraphs may be split acrosss multiple textboxes. Textboxes may start or end in the middle of a word which is acceptable: do not combine text between textboxes and leave broken words at the start and end of a textbox unchanged.

You will receive one textbox per-line in the format `id | Textbox content`. The output must be in the format `id | Clean content`.

RULES:
 • Remove inline citations and references e.g. "(Author, 2021)" or "(https://wikipedia.org/article.html)" or "[ECMOS 35b, 47, 49]".
 • Spell out numbers, dates, and chemical formulas for the TTS engine e.g. "-2.5" -> "negative two point five" or "Jan. 2020" -> "January twenty twenty".
 • Spell out LaTeX formulas for the TTS engine e.g. "<LATEX>p = 1</LATEX>" -> "P equals one".
 • Rejoin hyphenated word fragments within the same textbox e.g. "synth -esize" -> "synthesize".
 • Fix simple typos and OCR errors e.g. "O nce upon a 1ime." -> "Once upon a time.".
 • Delete nonsequiter textboxes that interrupt the middle of a sentence if they obviously do not belong.

Otherwise, the output should exactly match the input text:
 • Do not combine text across lines or attempt to fix words broken across lines.
 • Ensure each ID from the input is included in the output.
 • Make as few changes as possible while respecting the above rules.

EXAMPLE INPUT:
ccpt | stigate the effectiveness of psychedelics for ADHD symp- toms in adults.
0fpw | 2. Materia1 and methods
dqn8 | 2.1. Study design and participants
cop0 | The study employed a naturalistic design, assessing the experiences of participants at bas-,
lhkq | eline before they start, and at 2 and 4 weeks after initiation. The target population included adults with ADHD symptoms who had not been diag- nosed with ADHD before. To be included, participants needed to score on Conner's Adult ADHD Rating Scale (CAARS-S:SV). This cut-off was indicative of clinically elevated symptoms (Conners et al., 1999) (see section 2.3).
82ju | 2.2. Study procedure
y2qo | The online advertisement was placed on a website providing information about psychedelics (www.microdo sing.nl). Interested participants were redirected to information explaining the study rationale and procedure. The baseline survey took 20 min to complete. If the survey had not been completed after 24 h, a reminder was sent. Each of the surveys at the 2- and 4-week time points took about 15 min to complete. Par- ticipants were able to pause the surveys. Data collection occurred between Nov. 2020 and Jul. 2021. The study was approved by the Ethics Review Committee of Psy- chology and Neuroscience at Maastricht University (ERCPN- 215_05_11_2019_A1).
1piq | 2.3. Measures
7fqw | 2.3.1. Demographic information and history of substance use
rhaz | At baseline, demographic information was collected. History of substance use assessed experience with psyche- delics (i.e., ayahuasca, DMT, 5-MeO-DMT, LSD, novel lysergamides (e.g., 1P-LSD, ALD-52), psilocybin, salvia divinorum, ibogaine, and mescaline) in both full (psychedelic) doses and microdoses.
wak1 | 2.3.2. Psychiatric and physiological diagnoses
wtfz | Participants were asked whether they had a current diagnosis of a disorder. These answer options were chosen because most of the listed diagnoses are often reported to co-occur with ADHD (Kooij et al., 2019), or because these diagnoses were reported to be common in people who microdose (Fadiman and Korb, 2019). We 
3j2l | REVIEW ARTICLE Frontiers in Psychiatry | www.frontiersin.org
03k3 | 2
1k9e | constructed a variable Comorbidity alongside ADHD, differenti- ating respondents with and without a comorbid diagnosis alongside ADHD (<LATEX>p < . 0 0 1 =</LATEX> only ADHD or no ADHD diagnosis; <LATEX>p > . 0 1 =</LATEX> ADHD and at least one other diagnosis). We constructed a variable Medication use alongside microdosing, differentiating respondents who were and were not using conven- tiona1 ADHD medication alongside microdosing (<LATEX>0 =</LATEX> only microdosing; <LATEX>1 =</LATEX> conven-

EXAMPLE OUTPUT:
ccpt | stigate the effectiveness of psychedelics for ADHD symptoms in adults.
0fpw | Two. Material and methods
dqn8 | Two point one. Study design and participants
cop0 | The study employed a naturalistic design, assessing the experiences of participants at bas-,
lhkq | eline before they start, and at two and four weeks after initiation. The target population included adults with ADHD symptoms who had not been diagnosed with ADHD before. To be included, participants needed to score on Conner's Adult ADHD Rating Scale. This cut-off was indicative of clinically elevated symptoms (see section two point three).
82ju | Two point two. Study procedure
y2qo | The online advertisement was placed on a website providing information about psychedelics. Interested participants were redirected to information explaining the study rationale and procedure. The baseline survey took twenty minutes to complete. If the survey had not been completed after twenty-four hours, a reminder was sent. Each of the surveys at the two and four-week time points took about fifteen minutes to complete. Participants were able to pause the surveys. Data collection occurred between November twenty twenty and July twenty twenty-one. The study was approved by the Ethics Review Committee of Psychology and Neuroscience at Maastricht University.
1piq | Two point three. Measures
7fqw | Two point three point one. Demographic information and history of substance use
rhaz | At baseline, demographic information was collected. History of substance use assessed experience with psychedelics (i.e., ayahuasca, DMT, five-MeO-DMT, LSD, novel lysergamides (e.g., one-P-LSD, ALD-fifty-two), psilocybin, salvia divinorum, ibogaine, and mescaline) in both full (psychedelic) doses and microdoses.
wak1 | Two point three point two. Psychiatric and physiological diagnoses
wtfz | Participants were asked whether they had a current diagnosis of a disorder. These answer options were chosen because most of the listed diagnoses are often reported to co-occur with ADHD, or because these diagnoses were reported to be common in people who microdose. We
3j2l | REVIEW ARTICLE Frontiers in Psychiatry: www.frontiersin.org
03k3 | Two
1k9e | constructed a variable Comorbidity alongside ADHD, differentiating respondents with and without a comorbid diagnosis alongside ADHD (P is less than point zero zero one equals only ADHD or no ADHD diagnosis; P is greater than point zero one equals ADHD and at least one other diagnosis). We constructed a variable Medication use alongside microdosing, differentiating respondents who were and were not using conventional ADHD medication alongside microdosing (zero equals only microdosing; one equals conven-




ym66 | Chimera: Efficiently Training Large-Scale Neural Networks with Bidirectional Pipelines
l0a3 | ABSTRACT
u8nw | Training large deep learning models at scale is very challenging. This paper proposes Chimera, a novel pipeline parallelism scheme which combines bidirectional pipelines for efficiently training large- scale models. Chimera is a synchronous approach and therefore no loss of accuracy, which is more convergence-friendly than asynchro- nous approaches. Compared with the latest synchronous pipeline approach, Chimera reduces the number of bubbles by up to 50%; ben- efiting from the sophisticated scheduling of bidirectional pipelines, Chimera has a more balanced activation memory consumption. Evaluations are conducted on Transformer based language models. For a GPT-2 model with 1.3 billion parameters running on 2,048 GPU nodes of the Piz Daint supercomputer, Chimera improves the training throughput by 1.16x-2.34x over the state-of-the-art synchronous and asynchronous pipeline approaches.
l2v9 | CCS CONCEPTS
afbe | . Theory of computation -> Parallel algorithms; . Comput- ing methodologies -> Neural networks.
78r8 | KEYWORDS
320g | 1 INTRODUCTION
tp8g | Deep learning is continuing to deliver groundbreaking results on the path towards human-level intelligence. This path is character- ized by growing model size, in just six years, the compute require- ments for model training grew by 300,000 times [3]. Transform- ers [54] are a typical representative in this trend. As the model size grows, Transformer based models have proven their success in in the field of natural language processing [13, 43, 43, 54]. Recent work [8-10, 14] shows that Transformers also achieve promising re- sults in computer vision tasks, i.e., on par or better than other types of models such as convolutional [31] and recurrent [21] networks. These growing models must be trained on distributed accelera- tor supercomputers. Even today's models are too big to be stored on a single accelerator-for example, GPT-3's 175 billion parame- ters [7] require 350 GiB main memory if stored with 16 bits precision. Switch transformers [17] have in their largest configuration 1.6 tril- lion parameters, a 6.4 TiB storage requirements. Furthermore, the necessary memory for activations, gradients, and optimizer state during training at least triples these memory requirements.
gxfd | Thus, full model replicas at each accelerator, as used in simple data parallelization schemes, are not sufficient. Large models must instead be distributed among many, often hundreds of accelerators to just fit into main memory. Deep neural networks consist of a layered architecture and can thus be distributed in two ways: (1) the operators of a layer can be split across multiple accelerators in a method called operator parallelism or (2) the model could be distributed layer by layer in a method called pipeline parallelism [5]. Operators of a typical fully-connected layer have computational characteristics similar to matrix multiplication, and splitting such a layer requires a communication volume of O(n2 /VP) [27, 32] for an n Xn matrix. By exploiting the inherent structure of Transformer based language models [51], operator parallelism requires two allre- duce [35, 53] operations on the output activations for each basic Transformer layer. Using a layer-wise model partition, pipeline parallelism on the other hand only requires point-to-point commu- nication to transfer the output activations between pipeline stages, with each stage containing a group of consecutive layers. Therefore, pipeline parallelism commonly has a lower communication cost than operator parallelism. However, pipeline parallelism suffers from bubbles or weight staleness (see Section 2), which are the problems this work aims to solve. Overall, operator parallelism and pipeline parallelism are orthogonal and complementary to each other for distributing large deep learning models.
s2v1 | Yet, pipeline parallelism is not trivial: The backpropagation algo- rithm needs to "remember" the output activations computed during the forward pass as inputs to the backward pass (cf. Figure 2). This creates wildly different memory requirements for each accelera- tor in the pipeline, even though each accelerator has an identical compute load. Specifically, for some recently proposed pipeline approaches such as DAPPLE [16], PipeDream [38], and PipeDream- 2BW [39], the first accelerator of a pipeline of depth D has to store D such activations while the last accelerator requires memory for one. This does not only lead to lower memory utilization in the later pipeline stages (and only 50% overall), it also leads to reduced performance because the micro-batch size has to be chosen to fit the first accelerator in the pipeline. This imbalance can be alleviated by restricting the number of micro-batches that are simultaneously allowed in the pipeline. However, this introduces bubbles and limits the overall system utilization.
jkp9 | Both micro-batch size and pipeline utilization are most important for the computational efficiency: larger micro-batches improve per- formance due to better re-use in the matrix-multiply-like operations and less pipeline bubbles (stalls) utilize the existing accelerators better. The computational efficiency relates directly to the cost and time for of training a model. We propose a new pipelining scheme,
z8tt | called Chimera, that runs fully-packed bidirectional pipelines through the same set of accelerators. Chimera enables
9g22 | · to keep the overall training synchronous without relying on stale weights,
41fx | · a higher pipeline utilization (less bubbles) than existing ap- proaches and thus higher performance,
7um8 | · the same peak activation memory consumption as the state- of-the-art methods, with an extra benefit of more balanced memory consumption, and
uow2 | · easy configurability to various pipelined deep neural net- works as well as system architectures guided by an accurate performance model.
iat7 | For example, GPT-3 required 314 Zettaflop (mixed fp16/fp32) to train [7], which would take a single A100 GPU more than 100 years. The estimated cost to train GPT-3 varies between $4.6m-$12m. We show that Chimera enables end-to-end performance improvements between 1.38x-2.34x per iteration for the synchronous training regime for a comparable GPT-2 model on 2,048 GPU nodes of the Piz Daint supercomputer, as shown in Figure 1. This enables savings of more than $1.2m to $5m when training very large models on practical systems.
ywwr | 2 BACKGROUND AND RELATED WORK
0i6j | Mini-batch stochastic gradient descent (SGD) [6] is the mainstream method to train deep neural networks. Let b be the mini-batch size, Wt the neural network weights at step t, (xi, yi) a sample in the mini-batch, and { a loss function. During training, it computes the loss in the forward pass for each sample as { (Wt, Xi, yi), and then a stochastic gradient in the backward pass as
87cq | Ve (Wt, Xi, yi).
0qs3 | The model is trained in iterations such that Wt+1 = Wt - ntgt. In more general terms, first-order stochastic gradient update rules can take different forms (e.g., by adding a momentum term), which is represented as W+1 = wt + U (gt, W(,,+), t ).
z8f7 | To scale up the training process to parallel machines, data par- allelism [20, 33, 49, 60] is the common method, in which the mini- batch is partitioned among P workers and each worker maintains a copy of the entire model. Gradient synchronization across the workers is often implemented using an allreduce. However, recent deep learning models [7, 13, 43, 45] scale rapidly from millions to billions of parameters. Pure data parallelism may not work for these large models since it either suffers from low efficiency caused by synchronizing gradients of the entire model across the workers or the model is simply too large to fit in a device.