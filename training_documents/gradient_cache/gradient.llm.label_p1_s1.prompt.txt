You will receive raw text from an OCR scan of a document or web page. Each line of input represents one textbox from the document.

Your task is to label each textbox with exactly one of the following labels:
  title: the primary document title, no more than one per document
  heading: the name of the following chapter or section (should never start in the middle of the previous paragraph)
  subheading: a secondary heading, often following another heading (should never start in the middle of the previous paragraph)
  body: main paragraph content: may start or end in the middle of a word, use context to follow paragraphs that are split across textboxes (if a textbox contains both heading and body text, label it "body")
  math: textbox primarily containing math
  imageDescription: standalone callout that only describes an image, chart, table, or similar (typically begins with "Table...", "Figure...", "Fig. ...", "Above...", "Left...", etc.)
  authors: names of contributors to this document
  institutions: list of universities, business, and other institutions affiliated with this document
  publisher: info about the publisher, or provided by the publisher, including copyright info
  pagenum: the page number
  headerOrFooter: boilerplate text page number, or redundant headings at the top/bottom of the page
  toc: the table of contents
  references: bibliographic references and resources
  acknowledgements: thanks and credits to contributors
  appendix: index, appendix, or any other end matter not part of the main thread (including related headings)
  table: tabular data, label, title, or subheading for a grid or list of data
  datum: numeric data or data label from a chart or list
  advertisement: promotional ads and marketing
  layout: non-content related to the layout of the page, e.g. sidebar navigation
  callout: pull quote or long block text that stands apart from the main text
  footnote: footnotes and endnotes near the bottom of the page
  keywords: list of keywords or tags
  url: web address or email
  other: any other text that does not belong to the main thread

Each line of input is prefixed with a textbox ID in the format `id | Text content`. The output must be in the format `id | label`.

Note that body text may be split across multiple textboxes, and textboxes may start or end in the middle of a sentence or word. Because the text was extracted from a web page or document, paragraphs of body text may be interrupted by multiple textboxes of redundant headers, footnotes, page numbers, tables, images, etc. For example, a real heading will never interrupt the middle of a sentence. Use context clues like this to carefully label each textbox.

EXAMPLE INPUT:
1pjs | Neuroscience Applied 1 (2022) 101012
o2kr | Contents lists available at ScienceDirect
v6sk | Neuroscience Applied
1wj6 | journal homepage: www.journals.elsevier.com/neuroscience-applied
936l | Research Articles
06yq | Microdosing with psychedelics to self-medicate for ADHD symptoms in adults: A naturalistic study
dpv6 | ARTICLE INFO
8r5j | ABSTRACT
2taz | Keywords: ADHD Microdosing Psychedelics Self-medication Well-being Time perception
o3ya | ADHD in adulthood is often overlooked, which negatively affects the individual's well-being. First-line phar- macological interventions are effective in many ADHD patients, relieving symptoms rapidly. However, there seems to be a proportion of individuals who discontinue, or fail to respond to these treatments. For these in- dividuals, alternative treatment options should be explored.
r5il | 1. Introduction
5gui | Attention deficit hyperactivity disorder (ADHD) is one of the most common developmental disorders worldwide. Prevalence research indicates that 2.6% of the adult population has persistent ADHD. ADHD in adults is often overlooked because of the high comorbidity rate and lack of knowledge about how ADHD is expressed in adulthood (Kooij et al.,
9d4o | Fig. 1. Flowchart of included participants for each time point.
82qc | 2019). In addition, ADHD is associated with deficits in various domains of cogni- tive functioning. Twenty-five percent of ADHD cases suffered from ADHD symptoms purely because of de- ficiencies in temporal processing (Sonuga-Barke et al., 2010).
ls7d | First-line ADHD treatments in adults mainly include pharmacological interventions to enhance dopaminergic and noradrenergic neurotrans- mission with stimulants. Overall, they have been proven to work effectively in adults with ADHD, inducing fast symptom relief and thereby enhancing the person's quality of life. In the
5esx | Table 1 Demographic information from respondents at baseline and the two and four- week time points.
t8vv | longer term, approximately twenty percent of ADHD patients discontinue their prescribed medication after six to nine months, thirty percent after one year, and half of them after two years.
xrd1 | 2. Material and methods
5746 | 2.1. Study design and participants
7toz | The study employed a naturalistic design, assessing the experiences of participants at baseline,
j346 | Neuroscience Applied 1 (2022)
m5ka | E.C.H.M. Haijen et al.
kx2c | REVIEW
v8uq | before they start MD on their own initiative, and at two and four weeks after MD initiation. The target population included adults diagnosed with ADHD and individuals who experienced ADHD symptoms to the extent that these interfered with their daily lives and who had not been diag- nosed with ADHD before. To be included in the analyses, participants needed to score above a cut-off point on at least one of the subscales of the Conner's Adult ADHD Rating Scale (CAARS-S:SV).
2zmw | Fig. 2. Mean scores of the CAARS-S:SV DSM-IV total symptoms T-scores at baseline (0W) and two (2W) and four weeks (4W) after MD (A) of the whole sample, and (B) per conventional ADHD medication use. Error bars represent mean error.
sdk3 | <LATEX>\lim _ { x ightarrow \infty } rac { 6 x ^ { 2 } + 1 0 0 } { 7 x ^ { 2 } - 1 0 } =</LATEX>
x972 | 2.2. Study procedure
k221 | Mean performance measures of the CAARS-

EXAMPLE OUTPUT:
1pjs | headerOrFooter
o2kr | publisher
v6sk | publisher
1wj6 | publisher
936l | headerOrFooter
06yq | title
dpv6 | other
8r5j | heading
2taz | keywords
o3ya | body
r5il | heading
5gui | body
9d4o | imageDescription
82qc | body
ls7d | body
5esx | imageDescription
t8vv | body
xrd1 | heading
5746 | subheading
7toz | body
j346 | headerOrFooter
m5ka | authors
kx2c | headerOrFooter
v8uq | body
2zmw | imageDescription
sdk3 | math
x972 | heading
k221 | body




1pjs | Gradient-Based Learning Applied to Document Recognition
o2kr | Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient- based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of two dimensional (2-D) shapes, are shown to outperform all other techniques.
v6sk | Real-life document recognition systems are composed of multiple modules including field extraction, segmentation, recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN's), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure.
1wj6 | Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks.
936l | A graph transformer network for reading a bank check is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal checks. It is deployed commercially and reads several million checks per day.
06yq | Keywords- Convolutional neural networks, document recog- nition, finite state transducers, gradient-based learning, graph transformer networks, machine learning, neural networks, optical character recognition (OCR).
dpv6 | NOMENCLATURE
8r5j | GT Graph transformer.
2taz | GTN Graph transformer network.
o3ya | HMM Hidden Markov model.
r5il | HOS Heuristic oversegmentation.
5gui | K-NN K-nearest neighbor.
82qc | 0018-9219/98$10.00 @ 1998 IEEE
ls7d | NN OCR PCA RBF RS-SVM SDNN Space displacement neural network.
tzt3 | Neural network. Optical character recognition. Principal component analysis. Radial basis function. Reduced-set support vector method.
t8vv | SVM Support vector method.
qvnu | TDNN Time delay neural network.
kggw | V-SVM Virtual support vector method.
xrd1 | I. INTRODUCTION
5746 | Over the last several years, machine learning techniques, particularly when applied to NN's, have played an increas- ingly important role in the design of pattern recognition systems. In fact, it could be argued that the availability of learning techniques has been a crucial factor in the recent success of pattern recognition applications such as continuous speech recognition and handwriting recognition.
7toz | The main message of this paper is that better pattern recognition systems can be built by relying more on auto- matic learning and less on hand-designed heuristics. This is made possible by recent progress in machine learning and computer technology. Using character recognition as a case study, we show that hand-crafted feature extraction can be advantageously replaced by carefully designed learning machines that operate directly on pixel images. Using document understanding as a case study, we show that the traditional way of building recognition systems by manually integrating individually designed modules can be replaced by a unified and well-principled design paradigm, called GTN's, which allows training all the modules to optimize a global performance criterion.
j346 | Since the early days of pattern recognition it has been known that the variability and richness of natural data, be it speech, glyphs, or other types of patterns, make it almost impossible to build an accurate recognition system entirely by hand. Consequently, most pattern recognition systems are built using a combination of automatic learning techniques and hand-crafted algorithms. The usual method
v8uq | PROCEEDINGS OF THE IEEE, VOL. 86, NO. 11, NOVEMBER 1998
x972 | of recognizing individual patterns consists in dividing the system into two main modules shown in Fig. 1. The first module, called the feature extractor, transforms the input patterns so that they can be represented by low-dimensional vectors or short strings of symbols that: 1) can be easily matched or compared and 2) are relatively invariant with respect to transformations and distortions of the input pat- terns that do not change their nature. The feature extractor contains most of the prior knowledge and is rather specific to the task. It is also the focus of most of the design effort, because it is often entirely hand crafted. The classifier, on the other hand, is often general purpose and trainable. One of the main problems with this approach is that the recognition accuracy is largely determined by the ability of the designer to come up with an appropriate set of features. This turns out to be a daunting task which, unfortunately, must be redone for each new problem. A large amount of the pattern recognition literature is devoted to describing and comparing the relative merits of different feature sets for particular tasks.
cwrw | Historically, the need for appropriate feature extractors was due to the fact that the learning techniques used by the classifiers were limited to low-dimensional spaces with easily separable classes [1]. A combination of three factors has changed this vision over the last decade. First, the availability of low-cost machines with fast arithmetic units allows for reliance on more brute-force "numerical" methods than on algorithmic refinements. Second, the avail- ability of large databases for problems with a large market and wide interest, such as handwriting recognition, has enabled designers to rely more on real data and less on hand-crafted feature extraction to build recognition systems. The third and very important factor is the availability of powerful machine learning techniques that can handle high-dimensional inputs and can generate intricate decision functions when fed with these large data sets. It can be argued that the recent progress in the accuracy of speech and handwriting recognition systems can be attributed in large part to an increased reliance on learning techniques and large training data sets. As evidence of this fact, a large proportion of modern commercial OCR systems use some form of multilayer NN trained with back propagation.
4i0e | In this study, we consider the tasks of handwritten
h2l3 | character recognition (Sections I and II) and compare the performance of several learning techniques on a benchmark data set for handwritten digit recognition (Section III). While more automatic learning is beneficial, no learning technique can succeed without a minimal amount of prior knowledge about the task. In the case of multilayer NN's, a good way to incorporate knowledge is to tailor its archi- tecture to the task. Convolutional NN's [2], introduced in Section II, are an example of specialized NN architectures which incorporate knowledge about the invariances of two- dimensional (2-D) shapes by using local connection patterns and by imposing constraints on the weights. A comparison of several methods for isolated handwritten digit recogni- tion is presented in Section III. To go from the recognition of individual characters to the recognition of words and sentences in documents, the idea of combining multiple modules trained to reduce the overall error is introduced in Section IV. Recognizing variable-length objects such as handwritten words using multimodule systems is best done if the modules manipulate directed graphs. This leads to the concept of trainable GTN, also introduced in Section IV. Section V describes the now classical method of HOS for recognizing words or other character strings. Discriminative and nondiscriminative gradient-based techniques for train- ing a recognizer at the word level without requiring manual segmentation and labeling are presented in Section VI. Section VII presents the promising space-displacement NN approach that eliminates the need for segmentation heuris- tics by scanning a recognizer at all possible locations on the input. In Section VIII, it is shown that trainable GTN's can be formulated as multiple generalized transductions based on a general graph composition algorithm. The connections between GTN's and HMM's, commonly used in speech recognition, is also treated. Section IX describes a globally trained GTN system for recognizing handwriting entered in a pen computer. This problem is known as "online" handwriting recognition since the machine must produce immediate feedback as the user writes. The core of the system is a convolutional NN. The results clearly demonstrate the advantages of training a recognizer at the word level, rather than training it on presegmented, hand-labeled, isolated characters. Section X describes a complete GTN-based system for reading handwritten and machine-printed bank checks. The core of the system is the convolutional NN called LeNet-5, which is described in Section II. This system is in commercial use in the NCR Corporation line of check recognition systems for the banking industry. It is reading millions of checks per month in several banks across the United States.
dfzb | A. Learning from Data
u6ty | There are several approaches to automatic machine learn- ing, but one of the most successful approaches, popularized in recent years by the NN community, can be called "nu- merical" or gradient-based learning. The learning machine computes a function YP = F(ZP,W) where ZP is the pth input pattern, and W represents the collection of adjustable parameters in the system. In a pattern recognition setting,
awvu | the output YP may be interpreted as the recognized class label of pattern ZP, or as scores or probabilities associated with each class. A loss function EP = D(DP, F(W,ZP)) measures the discrepancy between DP, the "correct" or desired output for pattern ZP, and the output produced by the system. The average loss function Etrain(W) is the average of the errors EP over a set of labeled examples called the training set {(Z1, D1), ... , (ZP, DP)}. In the simplest setting, the learning problem consists in finding the value of W that minimizes Etrain(W). In practice, the performance of the system on a training set is of little interest. The more relevant measure is the error rate of the system in the field, where it would be used in practice. This performance is estimated by measuring the accuracy on a set of samples disjoint from the training set, which is called the test set. Much theoretical and experimental work [3]-[5] has shown that the gap between the expected error rate on the test set Etest and the error rate on the training set Etrain decreases with the number of training samples approximately as
on9b | Etest - Etrain = k(h/P)ª
a8ge | (1)
h27a | where P is the number of training samples, h is a measure of "effective capacity" or complexity of the machine [6], [7], o is a number between 0.5 and 1.0, and k is a constant. This gap always decreases when the number of training samples increases. Furthermore, as the capacity h increases, Etrain decreases. Therefore, when increasing the capacity h, there is a tradeoff between the decrease of Etrain and the increase of the gap, with an optimal value of the capacity h that achieves the lowest generalization error Etest. Most learning algorithms attempt to minimize Etrain as well as some estimate of the gap. A formal version of this is called structural risk minimization [6], [7], and it is based on defin- ing a sequence of learning machines of increasing capacity, corresponding to a sequence of subsets of the parameter space such that each subset is a superset of the previous subset. In practical terms, structural risk minimization is implemented by minimizing Etrain + BH(W), where the function H(W) is called a regularization function and ß is a constant. H(W) is chosen such that it takes large values on parameters W that belong to high-capacity subsets of the parameter space. Minimizing H(W) in effect limits the capacity of the accessible subset of the parameter space, thereby controlling the tradeoff between minimizing the training error and minimizing the expected gap between the training error and test error.
pdjh | B. Gradient-Based Learning
xmgs | The general problem of minimizing a function with respect to a set of parameters is at the root of many issues in computer science. Gradient-based learning draws on the fact that it is generally much easier to minimize a reasonably smooth, continuous function than a discrete (combinatorial) function. The loss function can be mini- mized by estimating the impact of small variations of the parameter values on the loss function. This is measured
bd9h | by the gradient of the loss function with respect to the parameters. Efficient learning algorithms can be devised when the gradient vector can be computed analytically (as opposed to numerically through perturbations). This is the basis of numerous gradient-based learning algorithms with continuous-valued parameters. In the procedures described in this article, the set of parameters W is a real-valued vector, with respect to which E(W) is continuous, as well as differentiable almost everywhere. The simplest mini- mization procedure in such a setting is the gradient descent algorithm where W is iteratively adjusted as follows:
m047 | JE(W) aw (2)
gy8c | WK =Wk-1 -6-
y4mj | In the simplest case, € is a scalar constant. More sophis- ticated procedures use variable €, or substitute it for a diagonal matrix, or substitute it for an estimate of the inverse Hessian matrix as in Newton or quasi-Newton methods. The conjugate gradient method [8] can also be used. However, Appendix B shows that despite many claims to the contrary in the literature, the usefulness of these second-order methods to large learning machines is very limited.
vpgq | A popular minimization procedure is the stochastic gra- dient algorithm, also called the online update. It consists in updating the parameter vector using a noisy, or approxi- mated, version of the average gradient. In the most common instance of it, W is updated on the basis of a single sample
ousl | Wk =WK-1-2EPK(W) aw (3)
m4ju | With this procedure the parameter vector fluctuates around an average trajectory, but usually it converges considerably faster than regular gradient descent and second-order meth- ods on large training sets with redundant samples (such as those encountered in speech or character recognition). The reasons for this are explained in Appendix B. The properties of such algorithms applied to learning have been studied theoretically since the 1960's [9]-[11], but practical successes for nontrivial tasks did not occur until the mid eighties.
a0mb | C. Gradient Back Propagation
ki8m | Gradient-based learning procedures have been used since the late 1950's, but they were mostly limited to linear systems [1]. The surprising usefulness of such simple gradient descent techniques for complex machine learning tasks was not widely realized until the following three events occurred. The first event was the realization that, despite early warnings to the contrary [12], the presence of local minima in the loss function does not seem to be a major problem in practice. This became apparent when it was noticed that local minima did not seem to be a major impediment to the success of early nonlinear gradient-based learning techniques such as Boltzmann machines [13], [14]. The second event was the popularization by Rumelhart et al. [15] and others of a simple and efficient procedure to compute the gradient in a nonlinear system composed
km8w | of several layers of processing, i.e., the back-propagation algorithm. The third event was the demonstration that the back-propagation procedure applied to multilayer NN's with sigmoidal units can solve complicated learning tasks. The basic idea of back propagation is that gradients can be computed efficiently by propagation from the output to the input. This idea was described in the control theory literature of the early 1960's [16], but its application to ma- chine learning was not generally realized then. Interestingly, the early derivations of back propagation in the context of NN learning did not use gradients but "virtual targets" for units in intermediate layers [17], [18], or minimal disturbance arguments [19]. The Lagrange formalism used in the control theory literature provides perhaps the best rigorous method for deriving back propagation [20] and for deriving generalizations of back propagation to recurrent networks [21] and networks of heterogeneous modules [22]. A simple derivation for generic multilayer systems is given in Section I-E.