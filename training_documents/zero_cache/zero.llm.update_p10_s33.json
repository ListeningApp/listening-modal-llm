{
  "id": "chatcmpl-A3UAMhnwmlOoPeMaLG6LZQ6Vzmeix",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "molj | Five point two. Are the generated action plans grounded in the environment?\n5pto | Since successful execution of correct action plans directly measures grounding, we calculate the percentage of generated action plans that are both correct and executable. We deem an action plan to be correct if seventy percent or more human annotators decide it is correct. Human-written plans are one hundred percent executable, of which sixty-five point nine one percent are deemed correct. Results for LMs are shown in Figure Four.\nmo6k | Although smaller LMs such as GPT-Two can generate highly executable action plans as shown in Table One, these executable plans mostly are not correct, as they often repeat the given example or do not contain all necessary steps. Increasing model parameters can lead to some improvement in generating plans that are both executable and correct, yet it scales poorly with the parameter count. In the meantime, action translation offers a promising way towards grounding actionable knowledge by producing executable and correct plans, though a large gap remains to be closed to reach human-level performance, sixty-five point nine one percent.\nzh6q | Five point three. Effect of Different Translation LMs\nkp04 | In this section, we study the effect of using different Translation LM. We compare two size variants of Sentence BERT and Sentence RoBERTa trained on the STS benchmark and a baseline using averaged Glo Ve embeddings. Results are shown in Table Three. Notably, we do not observe significant differences in executability and LCS across different variants of BERT and RoBERTa. We hypothesize that this is because any language models trained on reasonably large datasets should be capable of the single-step action phrase translation considered in this work. However, simply using average Glo Ve embeddings would lead to significantly reduced performance.\n3e8r | Five point four. Can LLMs generate actionable programs by following step-by-step instructions?\n8g7g | Prior works often focus on translating step-by-step instructions into executable programs. Specifically, instead of only providing a high-level task name, how-to instructions are also provided, as shown in Figure Five. Although this setting is easier as it does not require rich prior knowledge, how-to instructions can help resolve much ambiguity of exactly how to perform a high-level task when multiple solutions are possible. To investigate whether pre-trained LLMs are capable of doing this without additional training, we include these instructions in the prompt and evaluate LLMs with the proposed procedure. We compare to a supervised baseline from VirtualHome that trains an LSTM from scratch on human-annotated data. Since the code to train the baseline is not publicly released and a different train/test split is likely used, we only show results reported in Puig et al. as a crude reference. We also cannot compare executability as it is not reported. Results are shown in Table Four. Surprisingly, without being fine-tuned on any domain data, Translated Codex/GPT-Three can attain LCS close to supervised methods while generating highly executable programs.\n5j4f | Five point five. Analysis of program length\n4xe7 | Shorter programs have a natural advantage of being more executable as they need to satisfy less pre/post-conditions, albeit being prone to incompleteness. To validate the proposed approach does not simply generate very short programs, we calculate the average program length across the eighty-eight evaluated tasks. Results are shown in Table Five. Mirroring the observations made in Section Four point one and Section Four point two, we find smaller LMs such as GPT-Two tend to generate shorter programs than larger models do while frequently repeating the given executable example. In contrast, larger models like Codex and GPT-Three can generate more expressive programs with high realism, yet consequently, they often suffer from executability. We show proposed procedure can find appropriate balance and is capable of generating programs that are highly executable while maintaining reasonable expressiveness as measured by program length.\npg4b | Six. Related Works\nb0d0 | Large-scale natural language modeling has witnessed rapid advances since the inception of the Transformer architecture. It has been shown by recent works that large language models pre-trained on large unstructured text corpus not only can perform strongly on various down-stream NLP tasks but the learned representations can also be used to model relations of entities, retrieve matching visual features, synthesize code from docstrings, solve math problems, and even as valuable priors when applied to diverse tasks from different modalities. Notably, by pre-training on large-scale data, these models can also internalize an implicit knowledge base containing rich information about the world from which factual answers (e.g. \"Dante was born in PLACE\") can be extracted. Compared to prior works in single-step knowledge extraction, we aim to extract sequential action plans to complete open-ended human activities while satisfying various constraints of an interactive environment.\nd7eu | Many prior works have looked into grounding natural language in embodied environments. A series of them parse language instructions into formal logic or rely mainly on lexical analysis to resolve various linguistic ambiguities for embodied agents. However, they often require many hand-designed rules or scale inadequately to more complex tasks and environments. Recently, many efforts have been put into creating more realistic environments with the goal to further advances in this area. At the same time, by leveraging the better representation power of neural architectures, a number of works have looked into creating instruction-following agents that can perform manipulation, navigation, or both. Recent works also use language as hierarchical abstractions to plan actions using imitation learning and to guide exploration in reinforcement learning.\nxndr | Notably, many prior works do not leverage full-blown pre-trained LLMs; most investigate smaller LMs that require considerable domain-specific data for fine-tuning to obtain reasonable performance. Perhaps more importantly, few works have evaluated LLMs in an embodiment setting that realizes the full potential of the actionable knowledge these models already contain by pre-training on large-scale unstructured text: the tasks evaluated are often generated from a handful of templates, which do not resemble the highly diverse activities that humans perform in daily lives. The development of VirtualHome environment enables such possibility. However, relevant works rely on human-annotated data and perform supervised training from scratch. Due to the lack of rich world knowledge, these models can only generate action plans given detailed instructions of how to act or video demonstrations. Concurrent work by Li et al. validates similar hypothesis that\nm6hi | LMs contain rich actionable knowledge. They fine-tune GPT-Two with demonstrations to incorporate environment context and to predict actions in VirtualHome, and evaluate on tasks that are generated from pre-defined predicates. In contrast, we investigate existing knowledge in LLMs without any additional training and evaluate on human activity tasks expressed in free-form language.\nvpkf | Seven. Conclusion, Limitations and Future Work\n4d67 | In this work, we investigate actionable knowledge already contained in pre-trained LLMs without any additional training. We present several techniques to extract this knowledge to perform common-sense grounding by planning actions for complex human activities.\ne7j2 | Despite promising findings, there remain several limitations of this work which we discuss as follows:\nlqzt | Drop in Correctness Although our approach can significantly improve executability of the generated plans, we observe a considerable drop in correctness. In addition to the errors caused by the proposed action translation, this is partially attributed to the limited expressivity of VirtualHome, as it may not support all necessary actions to fully complete all evaluated tasks (correctness is judged by humans). This is also reflected by that Vanilla LMs can even surpass human-written plans, which are restricted by environment expressivity.",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725394306,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 1549,
    "prompt_tokens": 3390,
    "total_tokens": 4939
  }
}