{
  "id": "chatcmpl-A0YxUbwDO2thTwtrT2DaWyT9lBJxY",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "```\nma93 | While the details get murky quite quickly (with part of the problem being that even computer scientists are unsure about exactly how to understand the internal states of LLMs), it seems that neither the input nor the design of LLMs directly target semantic content. The properties that LLMs are trained to recognise are simply statistical occurrence properties. Furthermore, the training data they receive is raw and unencoded (i.e. it has not been labelled in advance by humans using categories like \"noun\" or \"verb\", nor does it contain axioms that could plausibly be taken as assigning meaning to basic symbols, as in classical computational systems). The processing behind LLM outputs thus seems to bypass semantic properties. Yet the surprising result of this kind of processing is that it results in behaviour which gives the very strong impression of linguistic understanding. Try interacting with ChatGPT and its fellow travellers and it is hard, sometimes almost impossible, to resist the belief that one is conversing with another person, a thinking thing (the infamous case of Google engineer Blake Lemoine being a stark case in point). As suggest of GPT-four:\nc6hh | Its primary strength is its unparalleled mastery of natural language. It can not only generate fluent and coherent text, but also understand and manipulate it in various ways, such as summarizing, translating, or answering an extremely broad set of questions. Moreover, by translating we mean not only between different natural languages but also translations in tone and style, as well as across domains such as medicine, law, accounting, computer programming, music, and more.\ny7ea | Furthermore, this exemplary performance means that the role LLM outputs play in our lives is increasingly akin to those of an ordinary conversational partner: we take the systems to convey information, to answer questions, and to advance our understanding, in ways that are often indistinguishable from our use of the testimony of other humans.\nnav4 | From the perspective of behavioural tests, then, it is very easy to believe that LLMs should be attributed rich, semantic properties: LLMs look like they respond to and manipulate genuine content, generating meaningful outputs through their understanding of language. In what follows, I'll label those who hold that LLMs deserve the attribution of rich predicates (like representing, meaning, and perhaps even understanding and thinking) 'Believers'. At the most extreme end of Believers are those who hold that by tracking and manipulating facts about the statistical occurrences of words these kinds of system develop \"a form of general intelligence, indeed showing sparks of artificial general intelligence ... demonstrated by GPT-four's core mental capabilities (such as reasoning, creativity, and deduction), its range of topics on which it has\ngem8 | gained expertise (such as literature, medicine, and coding), and the variety of tasks it is able to perform (e.g., playing games, using tools, explaining itself).\"\ne8nr | 'Sceptics', on the other hand, insist that LLMs are gaming Turing's test - passing the behavioural criteria but by the use of strategies that do not, in fact, warrant ascription of richer properties. For LLMs to gain admittance to the space of meaning, we need to be sure that they are sensitive to the meaning properties of the signs they manipulate, providing answers which are driven by a grasp of semantic content not merely through sensitivity to common word distributions. Yet, Sceptics point out, LLMs lack the sorts of features we might prima facie expect are required for the emergence of genuine sensitivity to meaning: first, they are not embedded in the non-linguistic environment (they can't see, hear or touch things, and they can't manipulate objects in the real or virtual world), and, second, they lack the kind of long-term goals and aims which could drive their interactions with the world and with us. Furthermore, their responses lack the sort of consistency and stability indicative of having a point of view or perspective. As notes:\nk78w | The ability to explain one's own behavior is an important aspect of intelligence, as it allows for a system to communicate with humans and other agents. Self explanation is not only a form of communication, but also a form of reasoning, requiring a good theory of mind for both yourself (the explainer) and the listener. For GPT-four, this is complicated by the fact that it does not have a single or fixed \"self\" that persists across different executions (in contrast to humans). Rather, as a language model, GPT-four simulates some process given the preceding input, and can produce vastly different outputs depending on the topic, details, and even formatting of the input.\ng9du | Yet the ability to act in the world, in a consistent and principled way reflective of long-term goals and needs, seems central to our conception of agency and it is very tempting to think that agency is a prerequisite for getting meaning off the ground. All this leads to a suspicion that LLMs are nothing more than \"stochastic parrots\" - that they are probability-based mechanisms which just parrot empty words back to us.\nf682 | So, who is right here? Perhaps predictably, I'm going to argue that there is right on both sides: Believers are right to think that LLMs are special, that their outputs are meaningful, and perhaps even that the systems succeed in representing and manipulating genuinely semantic properties, even though they are trained on an entirely non-semantic task. Sceptics, however, are right to think that meaning in these systems is derivative - LLM outputs mean things because we mean things when we use language. For a system to have non-derivative meaning I suggest it\nli1k | would have to qualify as an agent (having, in Searle's terms, original intentionality). LLMs don't have that, but neither, I conclude, should we want them to. For creating systems which met this demand would risk drawing artificial systems into the space of moral concern. To see this, in the next two sections I consider LLMs connection with the world, then in $4 I turn to the issue of original intentionality.\npbea | Two. Semantic content requires contact with things beyond the text\nccpt | Perhaps the most obvious objection to the claim that LLMs deserve the attribution of rich predicates like meaning and representing is that such systems lack the kind of connections to the external world which seem necessary to ground semantic content. There is a prominent view ... that models trained only on text cannot acquire realistic meanings because they lack reference, or connection to objects in the real world. The assumption here is that words get their meaning from their connection to things outside the sphere of language, so that, e.g. \"dog\" means what it does because it refers to dogs or picks out the property of doghood. While sentences and utterances are meaningful because they make claims about the world, so that their meaning can be analysed in terms of their conditions of truth. As David Lewis famously put it, it seems that: \"semantics with no treatment of truth conditions is not semantics\". Yet, if all a system has available to it is intra-linguistic information, we might suspect that truth won't enter the picture, since the system is out of touch with the external states of affairs where truth gets its footing.\n```",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1724697744,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_df84d6bd70",
  "usage": {
    "completion_tokens": 1490,
    "prompt_tokens": 3344,
    "total_tokens": 4834
  }
}