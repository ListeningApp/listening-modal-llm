{
  "id": "chatcmpl-A3UAGcpfOtsh32d8rTQhZLK6O45pn",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "l51p | to select as example(s) for prompting language models, or in the case of supervised fine-tuning baselines, they are used to fine-tune pre-trained language models.\ns9zz | Two point two Metrics\nx97k | A program that commands the agent to wander around in a household environment is highly executable but is mostly not correct. On the other hand, a program composed of natural language instructions annotated by humans is likely correct but cannot be executed, because its format is ambiguous and may lack necessary common-sense actions. We thus consider two axes for evaluation: executability and correctness.\nqwix | Executability Executability measures whether an action plan can be correctly parsed and satisfies the common-sense constraints of the environment. To be correctly parsed, an action plan must be syntactically correct and contain only allowed actions and recognizable objects. To satisfy the common-sense constraints, each action step must not violate the set of its preconditions and postconditions. We report the average executability across all eighty-eight tasks and all seven VirtualHome scenes.\n6lnq | Correctness Unlike most embodied environments where the completion of a task can be easily judged, the ambiguous and multimodal nature of natural language task specification makes it impractical to obtain a gold-standard measurement of correctness. Therefore, we conduct human evaluations for the main methods. For the remaining analysis, we rely on a match-based metric that measures how similar a generated program is to human annotations. Specifically, we follow Puig et al. and calculate the longest common subsequence between two programs, normalized by the maximum length of the two. In the presence of multiple human-written programs for a single task, we take the maximum longest common subsequence across them. However, we note that the majority of the tasks only have one human annotation, but there are often many plausible ways to complete a certain task, making\nnxb1 | this metric imperfect at evaluation program correctness. Although correlation between the two is shown by Puig et al., we consider it only as a proxy metric in replacement of unscalable human evaluation.\nkwpw | Three Method\nusfa | In this section, we investigate the possibility of extracting actionable knowledge from pre-trained language models without further training. We first give an overview of the common approach to query large language models and how it may be used for embodied agents in Section three point one. Then we describe an inference-time procedure that addresses several deficiencies of the language model baseline and offers better executability in embodied environments. We break down the proposed procedure into three individual components, each discussed in Section three point two, three point three, three point four. Pseudo-code is in Algorithm one.\ndnjr | Since language models excel at dealing with natural language text instead of the specific format required by VirtualHome as described in Section two point one, we only expose natural language text to language models. To do this, we define a bidirectional mapping for each atomic action that converts between the natural language format and the program format. For instance, \"walk to living room\" is mapped to Walk to living room. Full list of the mappings is in Appendix A point four.\nqser | Three point one Querying Language Models for Action Plans\n8gam | Previous works have shown that large language models pre-trained on a colossal amount of data would internalize rich world knowledge that can be probed to perform various downstream tasks. Notably, autoregressive language models can even perform in-context learning, an ability to solve tasks using only contextual information without gradient updates. Contextual information is given as part of the input prompt and language models are asked to complete the remaining text. It often consists of natural language instructions and or a number of examples containing the desired input-output pairs.\nz4db | We adopt the same approach to query language models to generate action plans for high-level tasks. Specifically, we prepend one example high-level task and its annotated action plan from the demonstration set to the query task, as shown in Figure two. To obtain text completion results, we sample from autoregressive language model using temperature sampling and nucleus sampling. We refer to this language model as Planning Language Model and the approach using this language model for plan generation as Vanilla language model, where language model is replaced by specific language model such as GPT three.\nlw7l | To improve the generation quality, we follow Chen et al. to sample multiple outputs for each query. However, unlike Chen et al. who investigate program synthesis and can choose the sample with highest unit test pass rate, we only consider the setting where one sample is allowed to be evaluated for each task. This is because repetitive trial-and-error is equivalent to probing the environment for privileged information, which should not be considered viable in our setting. For Vanilla language model, to choose the best action plan X star among k samples, each consisting of n i tokens, we select the sample with highest mean log probability as follows:\n2exi | n i one j equals one n i log probability x, x i, less than j where zero parameterizes the Planning Language Model. one X i\n16py | argmax ( Probability X equals equals greater than \nm4ov | Three point two Admissible Action Parsing by Semantic Translation\n9uqd | One issue arises when naively following the above approach to generate action plans: the plan expressed in free-form language often cannot be mapped to unambiguous actionable steps and thus is not executable by a robotic agent. Many reasons can cause such failures: one the output does not follow predefined mappings of any atomic action; two, the output may refer to atomic action and objects using words unrecognizable by the environment; or three, the output contains lexically ambiguous words.\n8szx | Instead of developing a set of rules to transform the free-form text into admissible action steps, we propose to again leverage world knowledge learned by language models to semantically translate the action. For each admissible environment action we calculate its semantic distance to the predicted action phrase by cosine similarity:",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725394300,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 1213,
    "prompt_tokens": 3128,
    "total_tokens": 4341
  }
}