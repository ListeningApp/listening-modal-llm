You will receive raw text from an OCR scan of a document or web page. Each line of input represents one textbox from the document.

Your task is to label each textbox with exactly one of the following labels:
  title: the primary document title, no more than one per document
  heading: the name of the following chapter or section (should never start in the middle of the previous paragraph)
  subheading: a secondary heading, often following another heading (should never start in the middle of the previous paragraph)
  body: main paragraph content: may start or end in the middle of a word, use context to follow paragraphs that are split across textboxes (if a textbox contains both heading and body text, label it "body")
  math: textbox primarily containing math
  imageDescription: standalone callout that only describes an image, chart, table, or similar (typically begins with "Table...", "Figure...", "Fig. ...", "Above...", "Left...", etc.)
  authors: names of contributors to this document
  institutions: list of universities, business, and other institutions affiliated with this document
  publisher: info about the publisher, or provided by the publisher, including copyright info
  pagenum: the page number
  headerOrFooter: boilerplate text page number, or redundant headings at the top/bottom of the page
  toc: the table of contents
  references: bibliographic references and resources
  acknowledgements: thanks and credits to contributors
  appendix: index, appendix, or any other end matter not part of the main thread (including related headings)
  table: tabular data, label, title, or subheading for a grid or list of data
  datum: numeric data or data label from a chart or list
  advertisement: promotional ads and marketing
  layout: non-content related to the layout of the page, e.g. sidebar navigation
  callout: pull quote or long block text that stands apart from the main text
  footnote: footnotes and endnotes near the bottom of the page
  keywords: list of keywords or tags
  url: web address or email
  other: any other text that does not belong to the main thread

Each line of input is prefixed with a textbox ID in the format `id | Text content`. The output must be in the format `id | label`.

Note that body text may be split across multiple textboxes, and textboxes may start or end in the middle of a sentence or word. Because the text was extracted from a web page or document, paragraphs of body text may be interrupted by multiple textboxes of redundant headers, footnotes, page numbers, tables, images, etc. For example, a real heading will never interrupt the middle of a sentence. Use context clues like this to carefully label each textbox.

EXAMPLE INPUT:
1pjs | Neuroscience Applied 1 (2022) 101012
o2kr | Contents lists available at ScienceDirect
v6sk | Neuroscience Applied
1wj6 | journal homepage: www.journals.elsevier.com/neuroscience-applied
936l | Research Articles
06yq | Microdosing with psychedelics to self-medicate for ADHD symptoms in adults: A naturalistic study
dpv6 | ARTICLE INFO
8r5j | ABSTRACT
2taz | Keywords: ADHD Microdosing Psychedelics Self-medication Well-being Time perception
o3ya | ADHD in adulthood is often overlooked, which negatively affects the individual's well-being. First-line phar- macological interventions are effective in many ADHD patients, relieving symptoms rapidly. However, there seems to be a proportion of individuals who discontinue, or fail to respond to these treatments. For these in- dividuals, alternative treatment options should be explored.
r5il | 1. Introduction
5gui | Attention deficit hyperactivity disorder (ADHD) is one of the most common developmental disorders worldwide. Prevalence research indicates that 2.6% of the adult population has persistent ADHD. ADHD in adults is often overlooked because of the high comorbidity rate and lack of knowledge about how ADHD is expressed in adulthood (Kooij et al.,
9d4o | Fig. 1. Flowchart of included participants for each time point.
82qc | 2019). In addition, ADHD is associated with deficits in various domains of cogni- tive functioning. Twenty-five percent of ADHD cases suffered from ADHD symptoms purely because of de- ficiencies in temporal processing (Sonuga-Barke et al., 2010).
ls7d | First-line ADHD treatments in adults mainly include pharmacological interventions to enhance dopaminergic and noradrenergic neurotrans- mission with stimulants. Overall, they have been proven to work effectively in adults with ADHD, inducing fast symptom relief and thereby enhancing the person's quality of life. In the
5esx | Table 1 Demographic information from respondents at baseline and the two and four- week time points.
t8vv | longer term, approximately twenty percent of ADHD patients discontinue their prescribed medication after six to nine months, thirty percent after one year, and half of them after two years.
xrd1 | 2. Material and methods
5746 | 2.1. Study design and participants
7toz | The study employed a naturalistic design, assessing the experiences of participants at baseline,
j346 | Neuroscience Applied 1 (2022)
m5ka | E.C.H.M. Haijen et al.
kx2c | REVIEW
v8uq | before they start MD on their own initiative, and at two and four weeks after MD initiation. The target population included adults diagnosed with ADHD and individuals who experienced ADHD symptoms to the extent that these interfered with their daily lives and who had not been diag- nosed with ADHD before. To be included in the analyses, participants needed to score above a cut-off point on at least one of the subscales of the Conner's Adult ADHD Rating Scale (CAARS-S:SV).
2zmw | Fig. 2. Mean scores of the CAARS-S:SV DSM-IV total symptoms T-scores at baseline (0W) and two (2W) and four weeks (4W) after MD (A) of the whole sample, and (B) per conventional ADHD medication use. Error bars represent mean error.
sdk3 | <LATEX>\lim _ { x ightarrow \infty } rac { 6 x ^ { 2 } + 1 0 0 } { 7 x ^ { 2 } - 1 0 } =</LATEX>
x972 | 2.2. Study procedure
k221 | Mean performance measures of the CAARS-

EXAMPLE OUTPUT:
1pjs | headerOrFooter
o2kr | publisher
v6sk | publisher
1wj6 | publisher
936l | headerOrFooter
06yq | title
dpv6 | other
8r5j | heading
2taz | keywords
o3ya | body
r5il | heading
5gui | body
9d4o | imageDescription
82qc | body
ls7d | body
5esx | imageDescription
t8vv | body
xrd1 | heading
5746 | subheading
7toz | body
j346 | headerOrFooter
m5ka | authors
kx2c | headerOrFooter
v8uq | body
2zmw | imageDescription
sdk3 | math
x972 | heading
k221 | body




82ju | The influence of the training set size was measured by training the network with 15 000, 30 000, and 60 000 examples. The resulting training error and test error are shown in Fig. 6. It is clear that, even with specialized architectures such as LeNet-5, more training data would improve the accuracy.
y2qo | To verify this hypothesis, we artificially generated more training examples by randomly distorting the original train- ing images. The increased training set was composed of the 60 000 original patterns plus 540 000 instances of dis- torted patterns with randomly picked distortion parameters.
1piq | The distortions were combinations of the following planar affine transformations: horizontal and vertical translations; scaling; squeezing (simultaneous horizontal compression and vertical elongation, or the reverse); and horizontal shearing. Fig. 7 shows examples of distorted patterns used for training. When distorted data were used for training, the test error rate dropped to 0.8% (from 0.95% without deformation). The same training parameters were used as without deformations. The total length of the training session was left unchanged (20 passes of 60 000 patterns each). It is interesting to note that the network effectively sees each individual sample only twice over the course of these 20 passes.
7fqw | Fig. 8 shows all 82 misclassified test examples. some of those examples are genuinely ambiguous, but several are perfectly identifiable by humans, although they are written in an under-represented style. This shows that further improvements are to be expected with more training data.
rhaz | C. Comparison with Other Classifiers
wak1 | For the sake of comparison, a variety of other trainable classifiers was trained and tested on the same database. An early subset of these results was presented in [51]. The error rates on the test set for the various methods are shown in Fig. 9.
wtfz | 1) Linear Classifier and Pairwise Linear Classifier: Possibly the simplest classifier that one might consider is a linear classifier. Each input pixel value contributes to a weighted sum for each output unit. The output unit with the highest sum (including the contribution of a bias constant) indicates the class of the input character. On the regular data, the error rate is 12%. The network has 7850 free parameters. On the deslanted images, the test error rate is 8.4%. The network has 4010 free parameters. The deficien- cies of the linear classifier are well documented [1], and it is included here simply to form a basis of comparison for more sophisticated classifiers. Various combinations of sigmoid units, linear units, gradient descent learning, and learning by directly solving linear systems gave similar results.
qy1i | A simple improvement of the basic linear classifier was tested [52]. The idea is to train each unit of a single- layer network to separate each class from each other class. In our case this layer comprises 45 units labeled 0/1,0/2, ... , 0/9,1/2, ... , 8/9. Unit i/j is trained to pro- duce +1 on patterns of class i, -1 on patterns of class j,, and it is not trained on other patterns. The final score for class i is the sum of the outputs all the units labeled i/x minus the sum of the output of all the units labeled y/i, for all x and y. The error rate on the regular test set was 7.6%.
1k9e | 2) Baseline Nearest Neighbor Classifier: Another simple classifier is a K-NN classifier with a Euclidean distance measure between input images. This classifier has the advantage that no training time, and no thought on the part of the designer, are required. However the memory requirement and recognition time are large: the complete 60 000 20 x 20 pixel training images (about 24 megabytes at one byte per pixel) must be available at run time. Much
5bjs | Fig. 7. Examples of distortions of ten training patterns.
uodj | more compact representations could be devised with modest increase in error rate. On the regular test set the error rate was 5.0%. On the deslanted data, the error rate was 2.4%, with k = 3. Naturally, a realistic Euclidean distance nearest-neighbor system would operate on feature vectors
k16v | rather than directly on the pixels, but since all of the other systems presented in this study operate directly on the pixels, this result is useful for a baseline comparison.
b2lm | 3) PCA and Polynomial Classifier: Following [53] and [54], a preprocessing stage was constructed which computes the projection of the input pattern on the 40 principal components of the set of training vectors. To compute the principal components, the mean of each input component was first computed and subtracted from the training vectors. The covariance matrix of the resulting vectors was then computed and diagonalized using singular value decomposition. The 40-dimensional feature vector was used as the input of a second degree polynomial classifier. This classifier can be seen as a linear classifier with 821 inputs, preceded by a module that computes all products of pairs of input variables. The error on the regular test set was 3.3%.
ti7f | 4) RBF Network: Following [55], an RBF network was constructed. The first layer was composed of 1000 Gaussian RBF units with 28x28 inputs, and the second layer was a simple 1000 inputs/ten outputs linear classifier. The RBF units were divided into ten groups of 100. Each group of units was trained on all the training examples of one of the ten classes using the adaptive K-means algorithm. The second-layer weights were computed using a regularized pseudoinverse method. The error rate on the regular test set was 3.6%.
oksu | 5) One-Hidden-Layer Fully Connected Multilayer NN: Another classifier that we tested was a fully connected multilayer NN with two layers of weights (one hidden layer) trained with the version of back-propagation described in Appendix C. Error on the regular test set was 4.7% for a network with 300 hidden units and 4.5% for a network with 1000 hidden units. Using artificial distortions to generate more training data brought only marginal improvement: 3.6% for 300 hidden units and 3.8% for 1000 hidden units. When deslanted images were used, the test error jumped down to 1.6% for a network with 300 hidden units.
wtow | It remains somewhat of a mystery that networks with such a large number of free parameters manage to achieve reasonably low testing errors. We conjecture that the dy- namics of gradient descent learning in multilayer nets has a "self-regularization" effect. Because the origin of weight space is a saddle point that is attractive in al- most every direction, the weights invariably shrink during the first few epochs (recent theoretical analysis seem to confirm this [56]). Small weights cause the sigmoids to operate in the quasi-linear region, making the network essentially equivalent to a low-capacity, single-layer net- work. As the learning proceeds the weights grow, which
o22k | progressively increases the effective capacity of the net- work. This seems to be an almost perfect, if fortuitous, implementation of Vapnik's "structural risk minimization" principle [6]. A better theoretical understanding of these phenomena, and more empirical evidence, are definitely needed.
nwfz | 6) Two-Hidden-Layer Fully Connected Multilayer NN: To see the effect of the architecture, several two-hidden-layer multilayer NN's were trained. Theoretical results have shown that any function can be approximated by a one- hidden-layer NN [57]. However, several authors have ob- served that two-hidden-layer architectures sometimes yield better performance in practical situations. This phenomenon was also observed here. The test error rate of a 28×28- 300-100-10 network was 3.05%, a much better result than the one-hidden-layer network, obtained using marginally more weights and connections. Increasing the network size to 28x28-1000-150-10 yielded only marginally improved error rates: 2.95%. Training with distorted patterns im- proved the performance somewhat: 2.50% error for the 28x28-300-100-10 network, and 2.45% for the 28x28- 1000-150-10 network.
nvfy | 7) A Small Convolutional Network-LeNet-1: Convolu- tional networks are an attempt to solve the dilemma between small networks that cannot learn the training set and large networks that seem overparameterized. LeNet-1 was an early embodiment of the convolutional network architecture which is included here for comparison purposes. The images were down-sampled to 16×16 pixels and centered in the 28×28 input layer. Although about 100 000 multiply/add steps are required to evaluate LeNet-1, its convolutional nature keeps the number of free parameters to only about 2600. The LeNet-1 architecture was developed using our own version of the USPS (U.S. Postal Service zip codes) database and its size was tuned to match the available data [35]. LeNet-1 achieved 1.7% test error. The fact that a network with such a small number of parameters can attain such a good error rate is an indication that the architecture is appropriate for the task.
mror | 8) LeNet-4: Experiments with LeNet-1 made it clear that a larger convolutional network was needed to make optimal use of the large size of the training set. LeNet-4 and later LeNet-5 were designed to address this problem. LeNet- 4 is very similar to LeNet-5, except for the details of the architecture. It contains four first-level feature maps, followed by eight subsampling maps connected in pairs to each first-layer feature maps, then 16 feature maps, followed by 16 subsampling maps, followed by a fully connected layer with 120 units, followed by the output layer (ten units). LeNet-4 contains about 260 000 connections and has about 17 000 free parameters. Test error was 1.1%. In a series of experiments, we replaced the last layer of LeNet- 4 with a Euclidean nearest-neighbor classifier, and with the "local learning" method of Bottou and Vapnik [58], in which a local linear classifier is retrained each time a new test pattern is shown. Neither of those methods improved the raw error rate, although they did improve the rejection performance.
zlxc | 9) Boosted LeNet-4: Following theoretical work by Schapire [59], Drucker et al. [60] developed the "boosting" method for combining multiple classifiers. Three LeNet-4's are combined: the first one is trained the usual way; the second one is trained on patterns that are filtered by the first net so that the second machine sees a mix of patterns, 50% of which the first net got right and 50% of which it got wrong; the third net is trained on new patterns on which the first and the second nets disagree. During testing, the outputs of the three nets are simply added. Because the error rate of LeNet-4 is very low, it was necessary to use the artificially distorted images (as with LeNet-5) in order to get enough samples to train the second and third nets. The test error rate was 0.7%, the best of any of our classifiers. At first glance, boosting appears to be three times more expensive as a single net. In fact, when the first net produces a high confidence answer, the other nets are not called. The average computational cost is about 1.75 times that of a single net.
k0bw | 10) Tangent Distance Classifier: The tangent distance classifier is a nearest-neighbor method where the distance function is made insensitive to small distortions and translations of the input image [61]. If we consider an image as a point in a high-dimensional pixel space (where the dimensionality equals the number of pixels), then an evolving distortion of a character traces out a curve in pixel space. Taken together, all these distortions define a low- dimensional manifold in pixel space. For small distortions in the vicinity of the original image, this manifold can be approximated by a plane, known as the tangent plane. An excellent measure of "closeness" for character images is the distance between their tangent planes, where the set of distortions used to generate the planes includes translations, scaling, skewing, squeezing, rotation, and line thickness variations. A test error rate of 1.1% was achieved using 16×16 pixel images. Prefiltering techniques using simple Euclidean distance at multiple resolutions allowed to reduce the number of necessary tangent distance calculations.
aq5a | 11) SVM: Polynomial classifiers are well studied meth- ods for generating complex decision surfaces. Unfortu- nately, they are impractical for high-dimensional problems because the number of product terms is prohibitive. The support vector technique is an extremely economical way of representing complex surfaces in high-dimensional spaces, including polynomials and many other types of surfaces [6].
sc5y | A particularly interesting subset of decision surfaces is the ones that correspond to hyperplanes that are at a maximum distance from the convex hulls of the two classes in the high-dimensional space of the product terms. Boser et al. [62] realized that any polynomial of degree k in this "maximum margin" set can be computed by first computing the dot product of the input image with a subset of the train- ing samples (called the "support vectors"), elevating the result to the kth power, and linearly combining the numbers thereby obtained. Finding the support vectors and the coef- ficients amounts to solving a high-dimensional quadratic minimization problem with linear inequality constraints. For the sake of comparison, we include here the results
0yab | Fig. 10. Rejection Performance: percentage of test patterns that must be rejected to achieve 0.5% error for some of the systems.
w95z | obtained by Burges and Schölkopf and reported in [63]. With a regular SVM, their error rate on the regular test set was 1.4%. Cortes and Vapnik had reported an error rate of 1.1% with SVM on the same data using a slightly different technique. The computational cost of this technique is very high: about 14 million multiply-adds per recognition. Using Schölkopf's V-SVM technique, 1.0% error was attained. More recently, Schölkopf (personal communication) has reached 0.8% using a modified version of the V-SVM. Unfortunately, V-SVM is extremely expensive: about twice as much as regular SVM. To alleviate this problem, Burges has proposed the RS-SVM technique, which attained 1.1%
0i1k | on the regular test set [63], with a computational cost of only 650 000 multiply-adds per recognition, i.e., only about 60% more expensive than LeNet-5.
dqlg | D. Discussion
p6pq | A summary of the performance of the classifiers is shown in Figs. 9-12. Fig. 9 shows the raw error rate of the classifiers on the 10 000 example test set. Boosted LeNet-4 performed best, achieving a score of 0.7%, closely followed by LeNet-5 at 0.8%.
zse6 | Fig. 10 shows the number of patterns in the test set that must be rejected to attain a 0.5% error for some of
p2n3 | PROCEEDINGS OF THE IEEE, VOL. 86, NO. 11, NOVEMBER 1998
ohh7 | Fig. 12. Memory requirements, measured in number of variables, for each of the methods. Most of the methods only require one byte per variable for adequate performance.