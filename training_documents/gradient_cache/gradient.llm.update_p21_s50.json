{
  "id": "chatcmpl-A3U7Qp1ngB4QOvqket20hQJZfuEcz",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "i22c | is associated with an image that contains all the ink between the cut associated with the source node and the cut associated with the destination node. An arc is created between two nodes if the segmentor decided that the ink between the corresponding cuts could form a candidate character. Typically, each individual piece of ink would be associated with an arc. Pairs of successive pieces of ink would also be included, unless they are separated by a wide gap, which is a clear indication that they belong to different characters. Each complete path through the graph contains each piece of ink once and only once. Each path corresponds to a different way of associating pieces of ink together so as to form characters.\nofle | B. Recognition Transformer and Viterbi Transformer\nezbr | A simple GTN to recognize character strings is shown in Fig. seventeen. It is composed of two GT's called the recognition transformer T rec and the Viterbi transformer T vit. The goal of the recognition transformer is to generate a graph, called the interpretation graph or recognition graph G int, that contains all the possible interpretations for all the possible segmentations of the input. Each path in G int represents one possible interpretation of one particular segmentation\ntpiw | of the input. The role of the Viterbi transformer is to extract the best interpretation from the interpretation graph.\ndgkq | The recognition transformer T rec takes the segmentation graph G seg as input, and applies the recognizer for single characters to the images associated with each of the arcs in the segmentation graph. The interpretation graph G int has almost the same structure as the segmentation graph, except that each arc is replaced by a set of arcs from and to the same node. In this set of arcs, there is one arc for each possible class for the image associated with the corresponding arc in G seg. As shown in Fig. eighteen, to each arc is attached a class label, and the penalty that the image belongs to this class as produced by the recognizer. If the segmentor has computed penalties for the candidate segments, these penalties are combined with the penalties computed by the character recognizer to obtain the penalties on the arcs of the interpretation graph. Although combining penalties of different nature seems highly heuristic, the GTN training procedure will tune the penalties and take advantage of this combination anyway. Each path in the interpretation graph corresponds to a possible interpretation of the input word. The penalty of a particular interpretation for a particular segmentation is given by the sum of the arc penalties along the corresponding path in the interpretation graph. Computing the penalty of an interpretation independently of the segmentation requires to combine the penalties of all the paths with that interpretation. An appropriate rule for combining the penalties of parallel paths is given in Section six-C.\nhxjr | The Viterbi transformer produces a graph G vit with a single path. This path is the path of least cumulated penalty in the interpretation graph. The result of the recognition can be produced by reading off the labels of the arcs along the graph G vit extracted by the Viterbi transformer. The Viterbi transformer owes its name to the famous Viterbi algorithm, an application of the principle of dynamic programming to find the shortest path in a graph efficiently. Let c i be the penalty associated to arc i, with source node S i and destination node d i (note that there can be multiple arcs between two nodes). In the interpretation graph, arcs also have a label l i. The Viterbi algorithm proceeds as follows. Each node n is associated with a cumulated Viterbi penalty U n. Those cumulated penalties are computed in any order that satisfies the partial order defined by the interpretation graph (which is directed and acyclic). The start node is initialized with the cumulated penalty U start equals zero. The other nodes cumulated penalties U n are computed recursively from the v values of their parent nodes, through the upstream arcs U n equals {arc i with destination d i equals n}\n2yv3 | U n equals minimum (c i plus U s i). iEUn\nopoe | Furthermore, the value of i for each node n which minimizes the right-hand side is noted m n, the minimizing entering arc. When the end node is reached we obtain in V end, the total penalty of the path with the smallest total penalty. We call this penalty the Viterbi penalty, and this sequence of arcs and nodes the Viterbi path. To obtain the Viterbi path with nodes n one through n y and arcs i one through i r minus one, we trace back these nodes and arcs as follows, starting with n y equals the end node, and recursively using the minimizing entering arc: i t equals m n plus one, and n t equals S i t until the start node is reached. The label sequence can then be read off the arcs of the Viterbi path.\nvr2v | VI. Global Training for Graph Transformer Networks\nm4jl | Section five described the process of recognizing a string using HOS, assuming that the recognizer is trained so as to give low penalties for the correct class label of correctly segmented characters, high penalties for erroneous categories of correctly segmented characters, and high penalties for all categories for poorly formed characters. This section explains how to train the system at the string level to do the above without requiring manual labeling of character segments. This training will be performed with a GTN whose architecture is slightly different from the recognition architecture described in Section five.\n59hi | In many applications, there is enough a priori knowledge about what is expected from each of the modules in order to train them separately. For example, with HOS one could individually label single-character images and train a character recognizer on them, but it might be difficult to obtain an appropriate set of noncharacter images to train the model to reject wrongly segmented candidates. Although separate training is simple, it requires additional\nua3p | supervision information that is often lacking or incomplete (the correct segmentation and the labels of incorrect candidate segments). Furthermore, it can be shown that separate training is suboptimal.\n0a2t | The following section describes four different gradient-based methods for training GTN-based handwriting recognizers at the string level: Viterbi training, discriminative Viterbi training, forward training, and discriminative forward training. The last one is a generalization to graph-based systems of the maximum a posteriori criterion introduced in Section two-C. Discriminative forward training is somewhat similar to the so-called maximum mutual information criterion used to train HMM in speech recognition. However, our rationale differs from the classical one. We make no recourse to a probabilistic interpretation but show that, within the gradient-based learning approach, discriminative training is a simple instance of the pervasive principle of error correcting learning.\nabow | Training methods for graph-based sequence recognition systems such as HMM's have been extensively studied in the context of speech recognition. Those methods require that the system be based on probabilistic generative models of the data, which provide normalized likelihoods over the space of possible input sequences. Popular HMM learning methods, such as the Baum-Welsh algorithm, rely on this normalization. The normalization cannot be preserved when nongenerative models such as NN's are integrated into the system. Other techniques, such as discriminative training methods, must be used in this case. Several authors have proposed such methods to train NN/HMM speech recognizers at the word or sentence level.\nqx6n | Other globally trainable sequence recognition systems avoid the difficulties of statistical modeling by not resorting to graph-based techniques. The best example is recurrent NN's. Unfortunately, despite early enthusiasm, the training of RNN's with gradient-based techniques has proven very difficult in practice.\np2bm | The GTN techniques presented below simplify and generalize the global training methods developed for speech recognition.\ncxyx | A. Viterbi Training",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725394124,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 1595,
    "prompt_tokens": 3313,
    "total_tokens": 4908
  }
}