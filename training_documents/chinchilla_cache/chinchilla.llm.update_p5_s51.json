{
  "id": "chatcmpl-A0YBTsNBkCpS9lLwCkywugAZmfuqh",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "```\ngocl | For each parameter count N we train four different models, decaying the learning rate by a factor of ten times over a horizon (measured in number of training tokens) that ranges by a factor of sixteen times. Then, for each run, we smooth and then interpolate the training loss curve. From this, we obtain a continuous mapping from FLOP count to training loss for each run. Then, for each FLOP count, we determine which run achieves the lowest loss. Using these interpolants, we obtain a mapping from any FLOP count C, to the most efficient choice of model size N and number of training tokens D such that FLOPs(N,D) equals C. At one thousand five hundred logarithmically spaced FLOP values, we find which model size achieves the lowest loss of all models along with the required number of training tokens. Finally, we fit power laws to estimate the optimal model size and number of training tokens for any given amount of compute, obtaining a relationship Nopt proportional to C to the power of a and Dopt proportional to C to the power of b. We find that a equals zero point five zero and b equals zero point five zero-as summarized in Table two. In Section D. four, we show a head-to-head comparison at ten to the twenty-one FLOPs, using the model size recommended by our analysis and by the analysis of Kaplan et al. using the model size we predict has a clear advantage.\nob1z | Three point two. Approach two: IsoFLOP profiles\nm4cb | In our second approach we vary the model size for a fixed set of nine different training FLOP counts (ranging from six times ten to the power of eighteen to three times ten to the power of twenty-one FLOPs), and consider the final training loss for each point. In contrast with Approach one that considered points (N, D, L) along the entire training runs. This allows us to directly answer the question: For a given FLOP budget, what is the optimal parameter count?\nm61y | For each FLOP budget, we plot the final loss (after smoothing) against the parameter count in Figure three (left). In all cases, we ensure that we have trained a diverse enough set of model sizes to see a clear minimum in the loss. We fit a parabola to each IsoFLOPs curve to directly estimate at what model size the minimum loss is achieved (Figure three (left)). As with the previous approach, we then fit a power law between FLOPs and loss-optimal model size and number of training tokens. Again, we fit exponents of the form Nopt proportional to C to the power of a and Dopt proportional to C to the power of b and we find that a equals zero point four nine and b equals zero point five one-as summarized in Table two.\nu2ob | Three point three. Approach three: Fitting a parametric loss function\n2gnm | Lastly, we model all final losses from experiments in Approach one and two as a parametric function of model parameter count and the number of seen tokens. Following a classical risk decomposition, we propose the following functional form\nemgw | L(N, D) equals E plus N to the power of a plus D to the power of b.\nxaqt | The first term captures the loss for an ideal generative process on the data distribution, and should correspond to the entropy of natural text. The second term captures the fact that a perfectly trained transformer with N parameters underperforms the ideal generative process. The final term captures the fact that the transformer is not trained to convergence, as we only make a finite number of optimisation steps, on a sample of the dataset distribution.\n5rcb | Model fitting. To estimate (A, B, E, a, B), we minimize the Huber loss between the predicted and observed log loss using the L-BFGS algorithm:\n524j | Runs L equals sum of Huber's log of (L(Ni, Di) minus log Li).\nxd3m | \n8qb6 | We account for possible local minima by selecting the best fit from a grid of initialisations. The Huber loss is robust to outliers, which we find important for good predictive performance over held-out data points. Section D. two details the fitting procedure and the loss decomposition.\nwkbn | Efficient frontier. We can approximate the functions Nopt and Dopt by minimizing the parametric loss L under the constraint FLOPs(N, D) approximately equal to six N D. The resulting Nopt and Dopt balance the two terms in Equation (three) that depend on model size and data. By construction, they have a power-law form:\nftfg | Nopt(C) equals (A) to the power of B divided by a, Dopt equals (A) to the power of B divided by b.\n7xi5 | where G equals (A) to the power of B divided by a, b equals a divided by (a plus b).\nw8r5 | \nybik | \ngii5 | We show contours of the fitted function L in Figure four (left), and the closed-form efficient computational frontier in blue. From this approach, we find that a equals zero point four six and b equals zero point five four-as summarized in Table two.\nq5eu | Three point four. Optimal model scaling\na7ao | We find that the three approaches, despite using different fitting methodologies and different trained models, yield comparable predictions for the optimal scaling in parameters and tokens with FLOPs. All three approaches suggest that as compute budget increases, model size and the amount of training data should be increased in approximately equal proportions. The first and second approaches yield very similar predictions for optimal model sizes. The third approach predicts even smaller models being optimal at larger compute budgets. We note that the observed points (L, N, D) for low training FLOPs have larger residuals L minus L(N, D) than points with higher computational budgets. The fitted model places increased weight on the points with more FLOPs-automatically considering the low-computational budget points as outliers due to the Huber loss. As a consequence of the empirically observed negative curvature in the frontier C is proportional to Nopt, this results in predicting a lower Nopt than the two other approaches.\n29a3 | In Table three we show the estimated number of FLOPs and tokens that would ensure that a model of a given size lies on the compute-optimal frontier. Our findings suggests that the current generation of\nsb1e | large language models are considerably over-sized, given their respective compute budgets. For example, we find that a one hundred seventy-five billion parameter model should be trained with a compute budget of four point four times ten to the power of twenty-four FLOPs and on over four point two trillion tokens. A two hundred eighty billion Gopher-like model is the optimal model to train given a compute budget of approximately ten to the power of twenty-five FLOPs and should be trained on six point eight trillion tokens. Unless one has a compute budget of ten to the power of twenty-six FLOPs, a one trillion parameter model is unlikely to be the optimal model to train. Furthermore, the amount of training data that is projected to be needed is far beyond what is currently used to train large models, and underscores the importance of dataset collection in addition to engineering improvements that allow for model scale. While there is significant uncertainty extrapolating out many orders of magnitude, our analysis clearly suggests that given the training compute budget for many current large language models, smaller models should have been trained on more tokens to achieve the most performant model.\nw2yn | In Appendix C, we reproduce the IsoFLOP analysis on two additional datasets: C-four and GitHub code. In both cases we reach the similar conclusion that model size and number of training tokens should be scaled in equal proportions.\nap6l | Four. Chinchilla\n```",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1724694767,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_df84d6bd70",
  "usage": {
    "completion_tokens": 1627,
    "prompt_tokens": 3397,
    "total_tokens": 5024
  }
}