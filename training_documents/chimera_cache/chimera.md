## Chimera: Efficiently Training Large-Scale Neural Networks with Bidirectional Pipelines

ABSTRACT

Training large deep learning models at scale is very challenging. This paper proposes Chimera, a novel pipeline parallelism scheme which combines bidirectional pipelines for efficiently training large-scale models. Chimera is a synchronous approach and therefore no loss of accuracy, which is more convergence-friendly than asynchronous approaches. Compared with the latest synchronous pipeline approach, Chimera reduces the number of bubbles by up to fifty percent; benefiting from the sophisticated scheduling of bidirectional pipelines, Chimera has a more balanced activation memory consumption. Evaluations are conducted on Transformer-based language models. For a GPT-two model with one point three billion parameters running on two thousand forty-eight GPU nodes of the Piz Daint supercomputer, Chimera improves the training throughput by one point one six times to two point three four times over the state-of-the-art synchronous and asynchronous pipeline approaches.


## CCS CONCEPTS

. Theory of computation -> Parallel algorithms; . Computing methodologies -> Neural networks.


## KEYWORDS

One. INTRODUCTION

Deep learning is continuing to deliver groundbreaking results on the path towards human-level intelligence. This path is characterized by growing model size, in just six years, the compute requirements for model training grew by three hundred thousand times. Transformers are a typical representative in this trend. As the model size grows, Transformer-based models have proven their success in the field of natural language processing. Recent work shows that Transformers also achieve promising results in computer vision tasks, i.e., on par or better than other types of models such as convolutional and recurrent networks. These growing models must be trained on distributed accelerator supercomputers. Even today's models are too big to be stored on a single accelerator-for example, GPT-three's one hundred seventy-five billion parameters require three hundred fifty gigabytes main memory if stored with sixteen bits precision. Switch transformers have in their largest configuration one point six trillion parameters, a six point four terabyte storage requirements. Furthermore, the necessary memory for activations, gradients, and optimizer state during training at least triples these memory requirements.

Thus, full model replicas at each accelerator, as used in simple data parallelization schemes, are not sufficient. Large models must instead be distributed among many, often hundreds of accelerators to just fit into main memory. Deep neural networks consist of a layered architecture and can thus be distributed in two ways: (one) the operators of a layer can be split across multiple accelerators in a method called operator parallelism or (two) the model could be distributed layer by layer in a method called pipeline parallelism. Operators of a typical fully-connected layer have computational characteristics similar to matrix multiplication, and splitting such a layer requires a communication volume of O of n squared divided by P for an n by n matrix. By exploiting the inherent structure of Transformer-based language models, operator parallelism requires two allreduce operations on the output activations for each basic Transformer layer. Using a layer-wise model partition, pipeline parallelism on the other hand only requires point-to-point communication to transfer the output activations between pipeline stages, with each stage containing a group of consecutive layers. Therefore, pipeline parallelism commonly has a lower communication cost than operator parallelism. However, pipeline parallelism suffers from bubbles or weight staleness, which are the problems this work aims to solve. Overall, operator parallelism and pipeline parallelism are orthogonal and complementary to each other for distributing large deep learning models.

Yet, pipeline parallelism is not trivial: The backpropagation algorithm needs to "remember" the output activations computed during the forward pass as inputs to the backward pass. This creates wildly different memory requirements for each accelerator in the pipeline, even though each accelerator has an identical compute load. Specifically, for some recently proposed pipeline approaches such as DAPPLE, PipeDream, and PipeDream-two-B-W, the first accelerator of a pipeline of depth D has to store D such activations while the last accelerator requires memory for one. This does not only lead to lower memory utilization in the later pipeline stages (and only fifty percent overall), it also leads to reduced performance because the micro-batch size has to be chosen to fit the first accelerator in the pipeline. This imbalance can be alleviated by restricting the number of micro-batches that are simultaneously allowed in the pipeline. However, this introduces bubbles and limits the overall system utilization.

Both micro-batch size and pipeline utilization are most important for the computational efficiency: larger micro-batches improve performance due to better re-use in the matrix-multiply-like operations and less pipeline bubbles (stalls) utilize the existing accelerators better. The computational efficiency relates directly to the cost and time for training a model. We propose a new pipelining scheme,

called Chimera, that runs fully-packed bidirectional pipelines through the same set of accelerators. Chimera enables to keep the overall training synchronous without relying on stale weights,

a higher pipeline utilization (less bubbles) than existing approaches and thus higher performance,

the same peak activation memory consumption as the state-of-the-art methods, with an extra benefit of more balanced memory consumption, and easy configurability to various pipelined deep neural networks as well as system architectures guided by an accurate performance model.

For example, GPT-three required three hundred fourteen zettaflop (mixed fp sixteen/fp thirty-two) to train, which would take a single A one hundred GPU more than one hundred years. The estimated cost to train GPT-three varies between four point six million to twelve million dollars. We show that Chimera enables end-to-end performance improvements between one point three eight times to two point three four times per iteration for the synchronous training regime for a comparable GPT-two model on two thousand forty-eight GPU nodes of the Piz Daint supercomputer, as shown in Figure one. This enables savings of more than one point two million dollars to five million dollars when training very large models on practical systems.


## Two. BACKGROUND AND RELATED WORK

Mini-batch stochastic gradient descent (SGD) is the mainstream method to train deep neural networks. Let b be the mini-batch size, W of t the neural network weights at step t, a sample in the mini-batch, and a loss function. During training, it computes the loss in the forward pass for each sample as W of t, x, y, and then a stochastic gradient in the backward pass as the gradient of W of t, x, y.

The model is trained in iterations such that W of t plus one equals W of t minus n times the gradient of t. In more general terms, first-order stochastic gradient update rules can take different forms (e.g., by adding a momentum term), which is represented as W plus one equals w t plus U of the gradient of t, W, t.

To scale up the training process to parallel machines, data parallelism is the common method, in which the mini-batch is partitioned among P workers and each worker maintains a copy of the entire model. Gradient synchronization across the workers is often implemented using an allreduce. However, recent deep learning models scale rapidly from millions to billions of parameters. Pure data parallelism may not work for these large models since it either suffers from low efficiency caused by synchronizing gradients of the entire model across the workers or the model is simply too large to fit in a device.

Operator parallelism is a solution to train large models by partitioning the operators of a layer among multiple workers, but it may suffer from high communication volume as discussed in Section One. Hybrid approaches, which combine operator parallelism with data parallelism, suffer from the similar problem to the pure operator parallelism.

To reduce the communication volume of operator parallelism, pipeline parallelism is intensively studied. The key idea is to partition the model in a layer-wise way and treat each worker and the layers on it, as a pipeline stage. The mini-batch is partitioned into multiple micro-batches, that are pipelined across the stages to increase the resources utilization. Recent work also shows improved performance when combining pipeline parallelism with data parallelism. However, to efficiently pipeline deep neural network training is challenging because of One: a training step contains one forward pass followed by one backward pass, Two: the gradient computation in the backward pass rely on the intermediate results of the forward pass, and Three: to achieve good convergence accuracy the mini-batch size is usually not very large.

Bubbles in the pipeline. For better convergence quality, synchronous approaches synchronize the gradients and flush the pipeline at the end of each training iteration, as shown in Figure Two. Therefore, synchronous approaches lead to pipeline bubbles. To utilize the pipeline, GPipe injects N micro-batches into the pipeline concurrently; DAPPLE uses the One-Forward-One-Backward schedule with periodic flushes. Both GPipe and DAPPLE incur two times (D minus one) bubbles (D minus one bubbles in the forward passes and D minus one bubbles in the backward passes). In contrast, Chimera (this work) incurs D minus two bubbles (D divided by two minus one bubbles in the forward passes and D divided by two minus one bubbles in the backward passes), which is about fifty percent reduction compared with DAPPLE and GPipe. We define the bubble ratio as the bubble overhead divided by the overall runtime of the pipeline. In practice, the workload of a backward pass is about two times of a forward pass, which leads to D divided by two minus one bubbles in the middle of the pipeline for Chimera (bubble ratio equals D minus two divided by three N divided by two plus D minus two), as shown in Figure Two. We will discuss how to remove the middle bubbles in Section Three point five (bubble ratio equals D minus two divided by two N plus D minus two).

GEMS is a memory-efficient pipeline approach which schedules micro-batches among two model replicas. Since GEMS is mainly designed for small B and has at most two active micro-batches at the same time, its bubble ratio is much higher than the other approaches and cannot be alleviated by a larger N. Asynchronous approaches do not have periodic pipeline flushes, so they do not have bubble problem but with stale weights.

Memory consumption. Memory overhead mainly comes from two aspects: the weight parameters and the activations (the intermediate results of the forward pass required in the backward pass to compute gradients). For GPipe and DAPPLE, each worker maintains the weights of one pipeline stage. For GEMS and Chimera (with the default setting), each worker maintains the weights of two pipeline stages since there are two pipelines in two directions. PipeDream updates the model after each backward pass on a micro-batch N equals one; therefore, to ensure weight version consistency between forward and backward passes, it requires to stash up to D versions of weights on a worker, which has the same memory cost as pure data parallelism. By using gradient accumulation N is greater than or equal to D, PipeDream-2BW reduces the number of weight versions to be stashed to two.

GEMS injects only one micro-batch at the beginning of the pipeline, and thus the activations of the forward pass on one micro-batch are stored. However, this leads to low pipeline efficiency as discussed previously. Since GPipe injects N micro-batches N is greater than or equal to D to fully utilize the pipeline into the pipeline concurrently, the activations memory consumption is proportional to N, which does not scale well to large mini-batches. Using the classic or a variant of one-F-one-B schedule, PipeDream, PipeDream-2BW, DAPPLE, and Chimera inject up to D micro-batches at the beginning of the pipeline, which scale well to large mini-batches. By counting the number of injected micro-batches on each worker of Chimera, we can observe that Chimera has an extra benefit of a more balanced activations memory consumption among the workers than PipeDream, PipeDream-2BW, and DAPPLE, and therefore a better memory resources utilization. Note that the activations memory consumption can be reduced using the technique of activation recomputation, but this is at a cost of about one-third more computation overhead.

ZeRO removes the memory redundancies by partitioning the three model states i.e., optimizer states, gradients, and parameters across data-parallel processes, with a modest increasement to the communication volume. Note that our pipeline approach is orthogonal to ZeRO. To further reduce the memory consumption of our pipeline approach is an interesting future work.

Convergence friendliness. By periodic pipeline flushes, synchronous approaches ensure that the same version of weights is used across all stages and all micro-batches in a training iteration, without introducing staleness. From the algorithmic perspective, synchronous approaches are equivalent to the standard and well-proved mini-batch SGD, and therefore, guarantee convergence.

To remove pipeline flushes, asynchronous approaches either aggressively lead to weight versions mismatch between forward and backward passes, or conservatively introduce staleness to the weights while ensuring weight versions consistency. Although they empirically show promising convergence results, the generality is lack of proof. More recent work observes that asynchronous training algorithms may result in lower convergence performance.

For the model accuracy, all the synchronous pipeline approaches such as Chimera, DAPPLE, GPipe and GEMS are guaranteed to achieve the same accuracy as the standard mini-batch SGD algorithm. For the asynchronous approaches such as PipeDream-2BW and PipeDream, it is not safe to achieve the ideal accuracy as the standard algorithm because of the introduced weight staleness, and the convergence quality may exhibit variance on different neural networks and tasks.

Overall, Chimera achieves the best balance of all aspects, as presented in Table Two. We will discuss the implementation details of Chimera in the following section.


## THE SCHEME OF CHIMERA Three

Three point one Bidirectional Pipelines

We consider large-scale models with repetitive structures i.e., the same block repeated multiple times, such as Bert and GPT- Two/Three, which can be partitioned into balanced stages with an equal number of blocks. The feature of repetitive structures is also exploited in PipeDream-2BW. How to generally partition any model into stages with efficiency is not the topic of this paper and has been well studied in recent work.

The key idea of Chimera is to combine two pipelines in different directions we call them down and up pipelines, respectively together. Figure Three shows an example with four pipeline stages i.e., D equals four. Here we assume there are D micro-batches executed by each worker within a training iteration, namely N equals D, which is the minimum to keep all the stages active. How to scale to more than D micro-batches i.e., for N is greater than D will be discussed in Section 3.5. In the down pipeline, stage zero through stage three are mapped to PO through P three linearly, while in the up pipeline the stages are mapped in a completely opposite order. The N assuming an even number micro-batches are equally partitioned among the two pipelines. Each pipeline schedules N divided by two micro-batches using one-F-one-B strategy, as shown in the left part of Figure Three. Then, by merging these two pipelines together, we obtain the pipeline schedule of Chimera upper right of Figure Three. Given an even number of stages D which can be easily satisfied in practice, it is guaranteed that there is no conflict i.e., there is at most one micro-batch occupies the same time slot on each worker during merging. We can see that the number of bubbles is reduced to D divided by two minus one in the forward and backward passes, respectively. By considering the uneven workloads between forward and backward passes, we get a more practical schedule of Chimera bottom right of Figure Three.

For the models which have to use a small B hat to guarantee convergence, there maybe less than D micro-batches a training iteration i.e., N is less than D. Chimera also supports the cases of N is less than D by simply partitioning the N micro-batches among the two pipelines as evenly as possible, with an extreme case that N equals one where only one micro-batch runs on a single pipeline.

Note that Chimera can be generalized to combine more than two pipelines, which further reduces the bubbles and balances the activations memory consumption, but at a cost of higher communication overhead and weights memory consumption.


## Three point two. Communication Scheme

Chimera uses point-to-point communication to transfer the intermediate activations and gradients with respect to the inputs between pipeline stages in the forward pass and the backward pass, respectively. Since Chimera combines bidirectional pipelines together, collective communication, i.e., allreduce, is used to synchronize the weight gradients across stage replicas before the next training iteration. Figure four a presents a simple way for gradient synchronization, namely synchronizing the gradients for each stage maintained by the workers after all the local computation of the current iteration is finished. Note that the gradient synchronization for the middle stages is partially overlapped by the computation on the beginning and the end stages.

For a deeper communication overlap, we launch allreduce eagerly by utilizing the bubbles in the pipeline. Taking P zero and P three in Figure four b as an example, after these two workers finish the backward passes on micro-batch three and micro-batch one, respectively, the calculation for the weight gradients of stage three has been finished; therefore, P zero and P three can launch an asynchronous allreduce using nonblocking collectives to synchronize the gradients of stage three as soon as they are finished, and a wait operation is called after all the local computation to make sure the allreduce is finished. In this way, the gradient synchronization for stage three is overlapped and the following computation. However, unlike P zero and P three, we choose not to conduct eager gradient synchronization for stage two, a middle stage, on P one and P two, since there is no bubble from the completion of stage two's gradients to the end of local computation. Although the asynchronous communication may proceed while the computation happens, it may cause additional overheads such as initialization, threading, etc. which could extend the critical path of the pipeline and jeopardize the overall performance. Performance modelling of the communication scheme will be presented in Section three point four.

Three point three. Hybrid of Pipeline and Data Parallelism

Chimera supports a hybrid of pipeline and data parallelism. The bidirectional pipelines of Chimera are replicated W times to scale to W times D workers. Since we consider the large models which can be easily partitioned into balanced stages, all D stages are equally replicated W times in hybrid parallelism. When scaling to the parallel machines equipped with high performance interconnected networks such as Infiniband, Cray Aries, or Slingshot, and NVLink, hybrid parallelism usually achieves better performance than the pure pipeline parallelism. This is because pure pipeline parallelism has W times D stages in the pipeline, while hybrid parallelism has D stages, W times less, which helps to reduce the point-to-point communication overhead between stages and increase the computation workload of each stage. Although hybrid parallelism leads to gradient synchronization between stage replicas, the overhead of it can be alleviated by the aforementioned high performance interconnected networks. However, as W increases D decreases, pipeline stages become coarser, until at some point the increased gradient synchronization overhead cannot be amortized by the reduced point-to-point communication overhead. Therefore, it is important to find the sweet spot to achieve the best performance.

synchronized does not change, but the number of processes participating in the gradient synchronization increases by W times. Also, we use the same communication scheme as discussed in Section three point two to overlap the gradient synchronization overhead in hybrid parallelism. In the next section we will discuss how to find the best configuration of W and D based on performance modelling.


## Three point four. Configuration Selection Based on Performance Modelling

Given the mini-batch size and the number of workers P, the configuration of B, W, and D largely affects the training throughput.

Larger micro-batch size B usually improves the computational efficiency of the accelerators. Since Chimera greatly alleviates the bubble problem, it greedily chooses to use the maximum micro-batch size fitting in the device memory. Compared with the existing synchronous pipeline approaches which have to consider the trade-off between bubble and computational efficiency, the greedy strategy of Chimera significantly reduces the tuning space.

To select the best configuration of W and D, we build a performance model to predict the runtime of a single training iteration represented by T for each available configuration. For the compuwe measure the runtime of the forward pass on a. The runtime of backward pass represented as two times of the forward pass if no activation recomputation is used, and three times otherwise. We define the critical path as a series of pipeline stages with dependency that determine the overall computation overhead of a single training iteration. An example of critical path is shown in Figure six. Let C sub F and C sub B denote the number of forward passes and backward passes on the critical path of the pipeline, respectively. For the example shown in Figure six, C sub F equals six and C sub B equals ten. The total computation overhead is

F sub t C sub f plus B sub t C sub b. To model the communication overhead, we assume bidirectional and direct point-to-point communication between the compute nodes, and use the classic Latency-Bandwidth d t h alpha minus beta. The cost of sending a message of size L is alpha plus beta I, alpha (latency) and beta (the transfer time per word) can be measured using micro benchmarks. As discussed in Section three point two, Chimera has two types of communication: p two p communication (Comm sub p two p) between stages and allreduce (Comm sub allreduce) for gradient synchronization. Comm sub p two p can be simply modelled by the alpha minus beta cost model. The total p two p communication cost is (C sub f plus C sub b) Comm sub p two p. Note that Comm sub P two p can be partially overlapped by the intermediate bubbles if there are any, but to simplify the model we do not consider that effect.

For Comm allreduce, we assume its implementation makes use of Rabenseifner's algorithm, whose cost is equals two (log base two r) alpha plus two (r minus one) beta L divided by r where L is the size of gradients to be synchronized and r is the number of stage replicas. Note that Rabenseifner's algorithm reaches the lower bound on the bandwidth term for host-based allreduce, and therefore works best for large models. We model the effect of communication overlapping (discussed in Section three point two) for Comm allreduce. Figure six shows an example of the free regions (i.e., exceeding which will increase the total runtime) utilized in Chimera to overlap the gradient synchronization. Note that there are two stage replicas on each worker. Regions (a) and (b) can be utilized to overlap the gradient synchronization for the first stage replica (the one with a larger stage ID), and region (c) can be utilized to overlap the gradient synchronization for both stage replicas. Let Comm sub un-overlapped (i) represent the portion of Comm sub allreduce which can not be overlapped by the free regions on worker i, and then the max of Comm sub un-overlapped (i) among the D workers contributes to the total runtime.

Overall, the runtime of a single training iteration is modelled as

T equals (F sub t plus Comm sub p two p) C sub f plus (B sub t plus Comm sub p two p) C sub b plus max of (Comm sub un-overlapped (i) for i in [zero, D minus one]). (one)

We use this model to select the best configuration of W and D.


## Three point five. Scale to More Micro-Batches

For a large B hat, there may be more than D micro-batches in a training iteration for each worker (i.e., N greater than D), especially when the compute resources are limited. To scale to a large B hat, we first choose the

B D and schedule these D micro-batches using bidirectional pipelines as a basic scheduling unit, and scale to a large B hat by concatenating K (K equals N divided by D and N equals B hat divided by W divided by B) basic units together. Figure seven (a) presents an example with N equal two D micro-batches in a training iteration for each worker, which has two basic scheduling units (i.e., K equals two). We propose three methods to concatenate multiple basic units. One. Direct concatenation, as shown in Figure seven (b). The bubbles at the end of the first basic unit can be occupied by the forward passes at the beginning of the second basic unit. If the backward pass has the same workload as the forward pass, basic units can be concatenated seamlessly. However, backward pass has about two times workload of the forward pass, which results in intermediate bubbles.

To remove the intermediate bubbles of direct concatenation, we propose two. forward doubling (shown in Figure seven (d)) and three. backward halving, in which the key idea is to equalize the workloads of forward and backward passes. In forward doubling, we increase the number of micro-batches for each forward pass to two, and then backward passes, each of which has only one micro-batch, as shown in Figure seven (c). Then, we fine-tune the schedule to remove fifty percent bubbles at the beginning of the pipeline, as shown in Figure seven (d). Forward doubling removes the intermediate bubbles, but it leads to two times activation memory consumption and therefore may exceed the device memory capacity. We resort to activation recomputation to reduce memory overhead. Note that recomputation increases the workload of the backward pass, but the p two p communication overhead in the forward passes is also doubled because of the outputs for two micro-batches; therefore, we still treat the forward pass (with two micro-batches) and the backward pass (with one micro-batch and recompute) have approximately equal workload. Forward doubling prefers large models in which even B equals one exceeds the device memory capacity, since in such case activation recomputation must be used. For smaller models which has a larger B, we propose to use backward halving, which uses the same schedule as forward doubling, except that rather than executing two micro-batches in the forward pass but to halve the micro-batch size of the backward pass. Backward halving does not increase the activation memory (thus no activation recomputation), but it may lower the computational using a sub-max B. To select the best of the begin array text d efficiency because of text inree methods is not a end array priori, which we rely on empirical results. Note that both forward doubling and backward halving have total D minus two bubbles (D divided by two minus one in the forward passes and D divided by two minus one in the backward passes), as shown in Figure seven (d), which is about a fifty percent reduction compared with DAPPLE and GPipe. For K greater than two, we use the schedule of two D micro-batches as a basic scheduling unit (as shown in Figure seven (c)) for forward doubling and backward halving, and concatenate floor k divided by two basic units and the residual D micro-batches if K is odd.

One more benefit for both forward doubling and backward halving is that they have more space to overlap p two p communication (in the forward passes) than the classic one F one B schedule. In Figure seven (d), taking the forward pass on micro-batch one as an example, the p two p communication from P one to P two can be overlapped by the intermediate forward pass computation, while for one F one B there may be not enough computation to overlap p two p communication.


## Three point six Generalize to More than Two Pipelines

So far we have only discussed the case that two pipelines (one down and one up) are combined together in Chimera. Yet, Chimera can be generalized to combine more than two pipelines for an even number of pipeline stages (i.e., D is even). For Q equals D divided by two, let F denote the set of all the divisors of Q, including one and Q itself. For any f in F, we can generate a scheme for Chimera, which combines f down pipelines and f up pipelines together and each pipeline has D divided by two f micro-batches scheduled by the one F one B strategy. Figure eight gives an example in which Chimera combines four pipelines with eight stages (i.e., D equals eight and f equals two). For the down pipeline i (i in [zero, f minus one]), the D stages are mapped to the D workers in turn with the first stage (i.e., stage zero) being mapped to the worker i times (D divided by f). For example, for the down pipeline one in Figure eight, stages [zero, one, two, three, four, five, six, seven] are mapped to workers [four, five, six, seven, zero, one, two, three], respectively. For the f up pipelines, the D stages are mapped to the D workers in a completely reverse order of the corresponding down pipeline. It can be easily demonstrated that the schedules of the two f pipelines can be overlaid without conflict.

left parenthesis D minus D over two f plus one right parenthesis M sub a comma left parenthesis D times M sub a right parenthesis For any f in F, Chimera can scale to more micro-batches (i.e., N greater than D) using the methods discussed in Section three point five. For a given f, Chimera incurs two times D over f over two minus one bubbles, but has to maintain two f model replicas and synchronize the gradients of two f stages on each worker.

The larger the value of f, the less bubbles (and the more balanced activations memory consumption), but also the higher gradient synchronization overhead and weights memory consumption. When f equals Q, Chimera degrades to pure data parallelism. Empirical results in Section four point four show that f greater than one rarely brings more performance benefit on the models used for evaluation. Thus, f equals one (i.e., a combination of two pipelines) is the default setting for Chimera in this paper, unless otherwise stated. We expect that f greater than one, whose features are summarized in Table three, would further improve the performance for future deep models with deeper pipeline.


## Four EXPERIMENTAL EVALUATION

We conduct our experiments mainly on the CSCS Piz Daint supercomputer. Each Cray XC fifty compute node contains an Intel Xeon E five twenty-six ninety CPU, and one NVIDIA P one hundred GPU with sixteen gigabytes global memory. The compute nodes of Piz Daint are connected by Cray Aries interconnect network in a Dragonfly topology.

We also evaluate the performance on a small cluster equipped with thirty-two NVIDIA V one hundred GPUs. The cluster has four GPU servers connected by Infiniband, and each server has eight V one hundred GPUS with NVLink. Each V one hundred GPU has thirty-two gigabytes global memory.

We evaluate the performance of the schemes listed in Table two, which covers the state-of-the-art. For a fair comparison, all schemes are implemented in PyTorch with GLOO distributed backend for both the point-to-point p two p communication between pipeline stages and gradient synchronization (allreduce) across the stage replicas, and GPU devices are utilized for acceleration. Although NCCL backend of PyTorch performs better for allreduce greater than five hundred twelve across GPU nodes (with GPUDirect RDMA), it does not support p two p communication. Using NCCL for gradient synchronization and GLOO for p two p at the same time fails, which is also observed in PipeDream. We use the language models summarized in Table four for evaluation, and the max sequence length of Bert minus forty-eight and GPT two are set to one hundred twenty-eight and six hundred thirty-two respectively, unless otherwise stated. The mini-batch size and sequence length we use are consistent with those in the machine learning community. Since Chimera is a synchronous approach without compromising convergence accuracy, we focus on the training throughput comparison.


## Four point one Memory Consumption

Four point two Parallel Scalability

We first find the best configuration for each approach, and compare their best performance in the test of weak scaling.

Four point two point one Performance Optimization Space for the Baselines. Given the mini-batch size B and the number of workers P, the best configuration of B (micro-batch size), D (pipeline stages), and W (the number of replicated pipelines) is not obvious a priori because of the trade-offs (i.e., computational efficiency and bubbles, allreduce and p two p communication overhead). We search the space of the parameters (W, D, and B (for power-of-two)) to find the best performance for each baseline. The results for Bert forty-eight on thirty-two GPU nodes are presented in Figure ten.

For synchronous baselines (such as GPipe and DAPPLE), the value of B affects both computational efficiency and the bubble ratio. The planner of DAPPLE gives an answer for how to select the configuration of W and D based on the profiling information, but it is not clear for how to select the best B. From Figure ten we can see the highest throughput of both DAPPLE and GPipe (with activation recomputation) is achieved by (W equals eight, D equals four, B equals four), under which they hit the sweet spot for the trade-off between p two p communication overhead and allreduce communication overhead by (W equals eight, D equals four), and the sweet spot for the trade-off between bubble ratio and computational efficiency by B equals four (and N equals sixteen). GEMS prefers a large B for high computational efficiency since a smaller B does not help a lot to reduce the bubble ratio, and therefore its best performance is achieved by (W equals eight, D equals four, B equals thirty-two).

Asynchronous baselines (PipeDream-two BW and PipeDream) always prefer the maximum B fitting in the device memory, since there is no bubble problem for them. Note that PipeDream conducts gradient synchronization across W pipelines after each backward pass on a micro-batch, thus its B is limited by the maximum B. Since the frequent gradient synchronization of PipeDream leads to high allreduce overhead, its best performance is achieved with a deeper pipeline than others, namely by (W equals four, D equals eight, B equals forty-eight). PipeDream-two BW scales to large B by accumulating the gradients for more than D micro-batches (i.e., N greater than or equal to D), and its best performance is achieved by (W equals eight, D equals four, B equals sixteen) with activation recomputation.

For GPT two, we present the performance tuning for each baseline by searching the parameter space in Figure eleven.

Four point two point two Performance Modelling of Chimera. We first evaluate the performance of Chimera with different gradient synchronization strategies discussed in Section three point two. We use eager-sync to denote eager synchronization also conducted for the middle stages, and eager-sync-opt to denote eager synchronization not conducted for the middle stages. Results in Figure twelve show that eager-sync-opt achieves higher (e.g., one point zero nine times on sixty-four nodes) throughput than eager-sync. These empirical results support our claim in Section three point two.

Figure thirteen presents the practical training throughput of Chimera and the throughput predicted by the performance model. Note that since Chimera greatly alleviates the bubble problem, it greedily chooses the largest B that fits in the device memory. The performance model is mainly used to select the best configuration of W and D. Therefore, Chimera has a much smaller tuning space compared with the synchronous baselines. The error of the performance model is within ten percent for both Bert-forty-eight and GPT-two. For Bert-forty-eight, the performance model accurately selects the best configuration, i.e., W equals eight and D equals four. For GPT-two, the performance model selects W equals sixteen and D equals thirty-two, but the best performance is achieved by W equals sixty-four and D equals eight. However, the best performance is only 1.7 percent higher than the one selected by the model. The inaccurate prediction is mainly because our model may overestimate the cost of activation recomputation used with W equals sixty-four and D equals eight. Although these two configurations achieve very close performance for GPT-two, it is worth mentioning that D equals eight works better when scaling to large mini-batches because of less computation and point to point communication overhead, while D equals thirty-two works better when scaling to more machines because of less gradient synchronization overhead.

Four point two point three Comparison with the Best Performance. Figures fourteen and fifteen present the results of weak scaling on Bert-forty-eight and GPT-two, respectively. For all the baselines we present the best performance after searching the parameter space at different scales. Especially, to achieve the best performance, GPipe switches from D equals eight to D equals sixteen for GPT-two on more than five hundred twelve GPU nodes. For Chimera, we present the practical throughput using the best configuration predicted by the performance model. The configuration used by each approach for the best performance is annotated in the legends of the figures.

For both Bert-forty-eight and GPT-two, Chimera outperforms all the baselines at all scales. For Bert-forty-eight on sixty-four nodes, Chimera outperforms PipeDream and PipeDream-2BW (asynchronous approaches) by 1.94 times and 1.17 times, respectively, and outperforms GPipe, GEMS, and DAPPLE (synchronous approaches) by 1.32 times, 2.41 times, and 1.19 times, respectively. PipeDream frequently synchronizes the gradients after each backward pass, which compromises the training throughput.

PipeDream-2BW uses B equals sixteen with recomputation to achieve the best performance. Although PipeDream-2BW does not have bubble problem, it may not have enough computation to fully overlap the gradient synchronization overhead. GEMS has the highest bubble ratio and therefore has lower throughput than the others. To achieve the best performance, GPipe and DAPPLE use B equals four to reduce the bubble ratio but at the cost of lower computational efficiency. In contrast, Chimera has low bubble ratio while using B equals eight for higher computational efficiency, and therefore outperforms GPipe and DAPPLE.

For GPT-two on two thousand forty-eight nodes, Chimera outperforms PipeDream and PipeDream-2BW (asynchronous approaches) by 2.01 times and 1.16 times, respectively, and outperforms GPipe, GEMS, and DAPPLE (synchronous approaches) by 1.42 times, 2.34 times, and 1.38 times, respectively. There are two major advantages of Chimera: one, Chimera has a low bubble ratio; two, benefiting from a balanced memory consumption, Chimera with D equals thirty-two fits in the device memory without activation recomputation, while all other approaches except GEMS require recomputation. Chimera outperforms PipeDream-2BW mainly because no recomputation is required, and outperforms GPipe and DAPPLE because of both less bubbles and no recomputation. Using five hundred twelve nodes as the baseline, Chimera achieves 91.4 percent parallel efficiency on two thousand forty-eight nodes in weak scaling for GPT-two, which demonstrates the efficiency of the communication scheme used in Chimera.

Note that we use the same model partition method as the default setting in PipeDream-twobee doubleyou, namely evenly partitioning the basic layers among the workers. Other model partition methods trying to balance the weights among the workers may help to reduce the peak memory consumption of PipeDream-twobee doubleyou, but this is outside the scope of this paper. Generally, Chimera is on par with PipeDream-twobee doubleyou (the latest asynchronous approach) in terms of training throughput, but more convergence-friendly since there is no stale weights in Chimera.

We also conduct the evaluation on the cluster with four by eight equals thirty-two V-one hundred GPUs connected by NVLink (intra-node) and Infiniband (inter-node). Experimental results for Bert-fourty-eight are shown in Figure sixteen. On thirty-two V-one hundred GPUs, Chimera improves the throughput by one point ten X to two point thirty-nine X and one point zero five X to one point eighty-nine X over the synchronous and asynchronous pipeline approaches, respectively, which demonstrates that the same conclusions hold on newer machines.


## Four point three Scale to Large Mini-Batches on a Given Number of Machines

In this section, we evaluate the training throughput when there are a large number of micro-batches available for each worker within a training iteration. In which case the bubble problem of all synchronous approaches is alleviated.

Figure seventeen presents the throughput of Bert-fourty-eight on thirty-two GPU nodes when scaling to large mini-batches. Different from the other approaches, PipeDream updates the model after training on each micro-batch, and therefore its B stops scaling after reaching the memory limit. Consistently, we search the parameter space and present the best performance for each baseline. For example, to achieve the best performance, DAPPLE and GPipe switch from B equals four to B equals eight when B is greater than or equal to one thousand twenty-four for higher computational efficiency, in which case the bubble problem is less important. PipeDream-twobee doubleyou also uses D equals four, and the B increase from sixteen to thirty-two when B is greater than or equal to one thousand twenty-four. Recall that in Section three point five we discuss three methods for scaling Chimera to large mini-batches. Forward doubling with B equals eight and backward halving with B equals four aim at solving the intermediate bubbles problem. However, the former suffers from recomputation overhead while the latter suffers from lower computational efficiency. Direct concatenation with B equals eight achieves the best performance among these three methods on Bert-fourty-eight, which can be explained by the fact that the intermediate bubbles caused by the uneven workloads between forward and backward passes can be utilized to accommodate the point-to-point communication between pipeline stages. For B less than or equal to two thousand forty-eight where bubbles still matter, we observe significant improvement of Chimera (direct) over all the synchronous approaches. Overall, for B greater than or equal to one thousand twenty-four, Chimera (direct) is very close to PipeDream-twobee doubleyou (asynchronous using stale weights), and achieves on average one point thirteen X, two point zero seven X, and one point zero six X speedup over GPipe (suffering from recomputation), GEMS (suffering from high bubble ratio), and DAPPLE, respectively.

Figure eighteen presents the throughput of GPT-two on five hundred twelve GPU nodes when scaling to large mini-batches. For GPT-two, Chimera with D equals eight with forward doubling outperforms direct concatenation, since activation recomputation is required in both methods but the former removes intermediate bubbles. Note that GPipe outperforms DAPPLE when scaling to large mini-batches in GPT-two, this is because both approaches require recomputation but the pipeline scheduling of GPipe is more regular and better to overlap the point-to-point communication. Benefiting from the sophisticated (less bubbles and more communication overlap as discussed in Section three point five) pipeline scheduling of Chimera with forward doubling, our approach outperforms all the baselines, and achieves on average one point thirteen X, one point eighteen X, two point sixty X, and one point thirty-four X speedup over PipeDream-twobee doubleyou, GPipe, GEMS, and DAPPLE, respectively. These results demonstrate that Chimera with forward doubling efficiently scales to large mini-batches for the large models where activation recomputation is commonly required.


## Four point four Chimera with More than Two Pipelines

Figure nineteen presents the throughput of Chimera with more than two pipelines, combined together on a thirty-two-layer GPT-two model. For the case of one pipeline, we use one F one B scheduling with pipeline flushing. With D equals thirty-two, four pipelines achieve the best performance because it hits the sweet spot of the trade-off between less bubbles and higher allreduce overhead. However, as the pipeline stages get coarser by decreasing D to sixteen, four pipelines performs worse then two pipelines because of the increasing of allreduce overhead. Two pipelines (the default setting of Chimera) usually achieve the highest performance among all the configurations. We expect that Chimera with more than two pipelines would further improve the performance on future deep models with deeper pipeline and higher computation density on each stage.


## Five CONCLUSION

Chimera brings new insights for efficiently pipelining large neural networks training at scale. Compared with the state-of-the-art pipeline approaches, Chimera achieves the best balance among pipeline efficiency, memory cost, and convergence friendliness. Empirical results for large language models training on up to two thousand forty-eight GPU nodes show that Chimera significantly improves the training throughput over the counterparts. We foresee that our approach will be one of the major solutions for massively scaling deep learning training. To reduce the communication cost of gradient synchronization by exploiting sparsification and quantization in deep learning training is our next step.


## A point one SUMMARY OF THE EXPERIMENTS REPORTED

We evaluated Chimera on the C.S.C.S. Piz Daint supercomputer. Each Cray XC-fifty compute node contains a twelve-core Intel Xeon E-five twenty-ninety CPU with sixty-four gigabytes RAM, and one NVIDIA Tesla P-one hundred with sixteen gigabytes memory. The compute nodes are connected by Cray Aries interconnect in a Dragonfly topology. We used GLOO in PyTorch as the distributed backend. We utilized the GPU for acceleration in all the experiments, as described in the paper. The source code of Chimera is as follows:


## A point two BASELINE EXPERIMENTAL SETUP, AND MODIFICATIONS MADE FOR THE PAPER

Relevant hardware details: C.S.C.S. Piz Daint supercomputer. Each Cray XC-fifty compute node contains a twelve-core Intel Xeon E-five twenty-ninety CPU and one NVIDIA Tesla P-one hundred GPU. The filesystem is Lustre.

Operating systems and versions: SUSE SLES eleven point three Compilers and versions: GCC nine point three point zero

Applications and versions: Bert, GPT-two Libraries and versions: PyTorch one point six Key algorithms: stochastic gradient descent Input datasets and versions: Wikipedia dataset, Wiki text-two dataset URL to output from scripts that gathers execution environment information.