{
  "id": "chatcmpl-A3UD9zXkmY9V9ROg2JbxQ8aROKFp2",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "1ujy | Figure thirteen presents the practical training throughput of Chimera and the throughput predicted by the performance model. Note that since Chimera greatly alleviates the bubble problem, it greedily chooses the largest B that fits in the device memory. The performance model is mainly used to select the best configuration of W and D. Therefore, Chimera has a much smaller tuning space compared with the synchronous baselines. The error of the performance model is within ten percent for both Bert-forty-eight and GPT-two. For Bert-forty-eight, the performance model accurately selects the best configuration, i.e., W equals eight and D equals four. For GPT-two, the performance model selects W equals sixteen and D equals thirty-two, but the best performance is achieved by W equals sixty-four and D equals eight. However, the best performance is only 1.7 percent higher than the one selected by the model. The inaccurate prediction is mainly because our model may overestimate the cost\nqzor | of activation recomputation used with W equals sixty-four and D equals eight. Although these two configurations achieve very close performance for GPT-two, it is worth mentioning that D equals eight works better when scaling to large mini-batches because of less computation and point to point communication overhead, while D equals thirty-two works better when scaling to more machines because of less gradient synchronization overhead.\na0by | Four point two point three Comparison with the Best Performance. Figures fourteen and fifteen present the results of weak scaling on Bert-forty-eight and GPT-two, respectively. For all the baselines we present the best performance after searching the parameter space at different scales. Especially, to achieve the best performance, GPipe switches from D equals eight to D equals sixteen for GPT-two on more than five hundred twelve GPU nodes. For Chimera, we present the practical throughput using the best configuration predicted by the performance model. The configuration used by each approach for the best performance is annotated in the legends of the figures.\nu1ky | For both Bert-forty-eight and GPT-two, Chimera outperforms all the baselines at all scales. For Bert-forty-eight on sixty-four nodes, Chimera outperforms PipeDream and PipeDream-2BW (asynchronous approaches) by 1.94 times and 1.17 times, respectively, and outperforms GPipe, GEMS, and DAPPLE (synchronous approaches) by 1.32 times, 2.41 times, and 1.19 times, respectively. PipeDream frequently synchronizes the gradients after each backward pass, which compromises the training throughput.\nhtm9 | PipeDream-2BW uses B equals sixteen with recomputation to achieve the best performance. Although PipeDream-2BW does not have bubble problem, it may not have enough computation to fully overlap the gradient synchronization overhead. GEMS has the highest bubble ratio and therefore has lower throughput than the others. To achieve the best performance, GPipe and DAPPLE use B equals four to reduce the bubble ratio but at the cost of lower computational efficiency. In contrast, Chimera has low bubble ratio while using B equals eight for higher computational efficiency, and therefore outperforms GPipe and DAPPLE.\n8hul | For GPT-two on two thousand forty-eight nodes, Chimera outperforms PipeDream and PipeDream-2BW (asynchronous approaches) by 2.01 times and 1.16 times, respectively, and outperforms GPipe, GEMS, and DAPPLE (synchronous approaches) by 1.42 times, 2.34 times, and 1.38 times, respectively. There are two major advantages of Chimera: one, Chimera has a low bubble ratio; two, benefiting from a balanced memory consumption, Chimera with D equals thirty-two fits in the device memory without activation recomputation, while all other approaches except GEMS require recomputation. Chimera outperforms PipeDream-2BW mainly because no recomputation is required, and outperforms GPipe and DAPPLE because of both less bubbles and no recomputation. Using five hundred twelve nodes as the baseline, Chimera achieves 91.4 percent parallel efficiency on two thousand forty-eight nodes in weak scaling for GPT-two, which demonstrates the efficiency of the communication scheme used in Chimera.\n87vz | Note that we use the same model partition method as the default setting in PipeDream-2BW, namely evenly partitioning the basic layers among the workers. Other model partition methods trying to balance the weights among the workers may help to reduce the peak memory consumption of PipeDream-2BW, but this is outside the scope of this paper. Generally, Chimera is on-par with PipeDream-2BW (the latest asynchronous approach) in terms of training throughput, but more convergence-friendly since there is no stale weights in Chimera.",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725394479,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 988,
    "prompt_tokens": 2709,
    "total_tokens": 3697
  }
}