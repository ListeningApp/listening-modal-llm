{
  "id": "chatcmpl-A3U7PojuNiNqNrjQKuP2xGsj0d2Zk",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "v8vk | Three. The Recognition Transformer: Trec iterates over all segment arcs in the segmentation graph and runs a character recognizer on the corresponding segment image. In our case, the recognizer is LeNet-Five, the convolutional N N described in Section Two, whose weights constitute the largest and most important subset of tunable parameters. The recognizer classifies segment images into one of ninety-five classes fully printable ASCII set plus a rubbish class for unknown symbols or badly formed characters. Each arc in the input graph Trec is replaced by ninety-six arcs in the output graph. Each of those ninety-six arcs contains the label of one of the classes, and a penalty that is the sum of the penalty of the corresponding arc in the input segmentation graph, and the penalty associated with classifying the image in the corresponding class, as computed by the recognizer. In other words, the recognition graph represents a weighted trellis of scored character classes. Each path in this graph represents a possible character string for the corresponding\nsr40 | field. We can compute a penalty for this interpretation by adding the penalties along the path. This sequence of characters may or may not be a valid check amount.\n5zjj | Four. The Composition Transformer: Tgram selects the paths of the recognition graph that represent valid character sequences for check amounts. This transformer takes two graphs as input: the recognition graph and the grammar graph. The grammar graph contains all possible sequences of symbols that constitute a well-formed amount. The output of the composition transformer, called the interpretation graph, contains all the paths in the recognition graph that are compatible with the grammar. The operation that combines the two input graphs to produce the output is a generalized transduction. A differentiable function is used to compute the data attached to the output arc from the data attached to the input arcs. In our case, the output arc receives the class label of the two arcs and a penalty computed by simply summing the penalties of the two input arcs the recognizer penalty and the arc penalty in the grammar graph. Each path in the interpretation graph represents one interpretation of one segmentation of one field on the check. The sum of the penalties along the path represents the badness of the corresponding interpretation and combines evidence from each of the modules along the process, as well as from the grammar.\nf56l | Five. The Viterbi Transformer: The Viterbi transformer finally selects the path with the lowest accumulated penalty corresponding to the best grammatically correct interpretations.\nar2z | B. Gradient-Based Learning\nyad0 | Each stage of this check reading system contains tunable parameters. While some of these parameters could be manually adjusted, the vast majority of them must be learned, particularly the weights of the N N recognizer.\n3g38 | Prior to globally optimizing the system, each module parameters are initialized with reasonable values. The parameters of the field locator and the segmenter are initialized by hand, while the parameters of the N N character recognizer are initialized by training on a database of presegmented and labeled characters. Then, the entire system is trained globally from whole check images labeled with the correct amount. No explicit segmentation of the amounts is needed to train the system: it is trained at the check level.\nvtbo | The loss function E minimized by our global training procedure is the discriminative forward criterion described in Section Six: the difference between one the forward penalty of the constrained interpretation graph constrained by the correct label sequence and two the forward penalty of the unconstrained interpretation graph. Derivatives can be back propagated through the entire structure, although it is only practical to do it down to the segmenter.\nq8pa | C. Rejecting Low Confidence Checks\nuc9x | In order to be able to reject checks which are the most likely to carry erroneous Viterbi answers, we must rate them with a confidence and reject the check if this\nzy7o | confidence is below a given threshold. To compare the unnormalized Viterbi penalties of two different checks would be meaningless when it comes to decide which answer we trust the most.\nkcar | The optimal measure of confidence is the probability of the Viterbi answer given the input image. As seen in Section Six-E, given a target sequence which, in this case, would be the Viterbi answer, the discriminative forward loss function is an estimate of the logarithm of this probability. Therefore, a simple solution to obtain a good estimate of the confidence is to reuse the interpretation graph to compute the discriminative forward loss as described in Figure Twenty-one, using as our desired sequence the Viterbi answer. This is summarized in Figure Thirty-four, with\n00aw | confidence equals exp E d forw.\n351p | D. Results\nzzcb | A version of the above system was fully implemented and tested on machine-print business checks. This system is basically a generic GTN engine with task specific heuristics encapsulated in the check and fprop method. As a consequence, the amount of code to write was minimal: mostly the adaptation of an earlier segmenter into the segmentation transformer. The system that deals with handwritten or personal checks was based on earlier implementations that used the GTN concept in a restricted way.\nmxnx | The N N classifier was initially trained on five hundred thousand images of character images from various origins spanning the entire printable ASCII set. This contained both handwritten and machine-printed characters that had been previously size normalized at the string level. Additional images were generated by randomly distorting the original images using simple affine transformations of the images. The network was then further trained on character images that had been automatically segmented from check images and manually truthed. The network was also initially trained to reject noncharacters that resulted from segmentation errors. The recognizer was then inserted in the check-reading system and a small subset of the parameters were trained globally at the field level on whole check images.\n11k8 | On six hundred forty-six business checks that were automatically categorized as machine printed, the performance was eighty-two percent correctly recognized checks, one percent errors, and seventeen percent rejects. This can be compared to the performance of the previous system on the same test set: sixty-eight percent correct, one percent errors, and thirty-one percent rejects. A check is categorized as machine-printed when characters that are near a standard position dollar sign are detected as machine printed, or when, if nothing is found in the standard position, at least one courtesy amount candidate is found somewhere else. The improvement is attributed to three main causes. First the N N recognizer was bigger and trained on more data. Second, because of the GTN architecture, the new system could take advantage of grammatical constraints in a much more efficient way than the previous system. Third, the GTN architecture provided extreme flexibility for testing heuristics, adjusting parameters, and tuning the system. This last point is more important than it seems. The GTN framework separates the algorithmic part of the system from the knowledge-based part of the system, allowing easy adjustments of the latter. The importance of global training was only minor in this task because the global training only concerned a small subset of the parameters.\ntwwy | An independent test performed by systems integrators in nineteen ninety-five showed the superiority of this system over other commercial courtesy amount reading systems. The system was integrated in N C R's line of check reading systems. It has been fielded in several banks across the United States since June nineteen ninety-six, and it has been reading millions of checks per day since then.\nfyp8 | Eleven. Conclusions",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725394123,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 1529,
    "prompt_tokens": 3276,
    "total_tokens": 4805
  }
}