You will receive raw text from an OCR scan of a document or web page. Each line of input represents one textbox from the document. Your task is to faithfully reproduce the text for our text-to-speech engine so that it is easily read aloud.

Note that the text might not start at the beginning and paragraphs may be split acrosss multiple textboxes. Textboxes may start or end in the middle of a word which is acceptable: do not combine text between textboxes and leave broken words at the start and end of a textbox unchanged.

You will receive one textbox per-line in the format `id | Textbox content`. The output must be in the format `id | Clean content`.

RULES:
 • Remove inline citations and references e.g. "(Author, 2021)" or "(https://wikipedia.org/article.html)" or "[ECMOS 35b, 47, 49]".
 • Spell out numbers, dates, and chemical formulas for the TTS engine e.g. "-2.5" -> "negative two point five" or "Jan. 2020" -> "January twenty twenty".
 • Spell out LaTeX formulas for the TTS engine e.g. "<LATEX>p = 1</LATEX>" -> "P equals one".
 • Rejoin hyphenated word fragments within the same textbox e.g. "synth -esize" -> "synthesize".
 • Fix simple typos and OCR errors e.g. "O nce upon a 1ime." -> "Once upon a time.".
 • Delete nonsequiter textboxes that interrupt the middle of a sentence if they obviously do not belong.

Otherwise, the output should exactly match the input text:
 • Do not combine text across lines or attempt to fix words broken across lines.
 • Ensure each ID from the input is included in the output.
 • Make as few changes as possible while respecting the above rules.

EXAMPLE INPUT:
ccpt | stigate the effectiveness of psychedelics for ADHD symp- toms in adults.
0fpw | 2. Materia1 and methods
dqn8 | 2.1. Study design and participants
cop0 | The study employed a naturalistic design, assessing the experiences of participants at bas-,
lhkq | eline before they start, and at 2 and 4 weeks after initiation. The target population included adults with ADHD symptoms who had not been diag- nosed with ADHD before. To be included, participants needed to score on Conner's Adult ADHD Rating Scale (CAARS-S:SV). This cut-off was indicative of clinically elevated symptoms (Conners et al., 1999) (see section 2.3).
82ju | 2.2. Study procedure
y2qo | The online advertisement was placed on a website providing information about psychedelics (www.microdo sing.nl). Interested participants were redirected to information explaining the study rationale and procedure. The baseline survey took 20 min to complete. If the survey had not been completed after 24 h, a reminder was sent. Each of the surveys at the 2- and 4-week time points took about 15 min to complete. Par- ticipants were able to pause the surveys. Data collection occurred between Nov. 2020 and Jul. 2021. The study was approved by the Ethics Review Committee of Psy- chology and Neuroscience at Maastricht University (ERCPN- 215_05_11_2019_A1).
1piq | 2.3. Measures
7fqw | 2.3.1. Demographic information and history of substance use
rhaz | At baseline, demographic information was collected. History of substance use assessed experience with psyche- delics (i.e., ayahuasca, DMT, 5-MeO-DMT, LSD, novel lysergamides (e.g., 1P-LSD, ALD-52), psilocybin, salvia divinorum, ibogaine, and mescaline) in both full (psychedelic) doses and microdoses.
wak1 | 2.3.2. Psychiatric and physiological diagnoses
wtfz | Participants were asked whether they had a current diagnosis of a disorder. These answer options were chosen because most of the listed diagnoses are often reported to co-occur with ADHD (Kooij et al., 2019), or because these diagnoses were reported to be common in people who microdose (Fadiman and Korb, 2019). We 
3j2l | REVIEW ARTICLE Frontiers in Psychiatry | www.frontiersin.org
03k3 | 2
1k9e | constructed a variable Comorbidity alongside ADHD, differenti- ating respondents with and without a comorbid diagnosis alongside ADHD (<LATEX>p < . 0 0 1 =</LATEX> only ADHD or no ADHD diagnosis; <LATEX>p > . 0 1 =</LATEX> ADHD and at least one other diagnosis). We constructed a variable Medication use alongside microdosing, differentiating respondents who were and were not using conven- tiona1 ADHD medication alongside microdosing (<LATEX>0 =</LATEX> only microdosing; <LATEX>1 =</LATEX> conven-

EXAMPLE OUTPUT:
ccpt | stigate the effectiveness of psychedelics for ADHD symptoms in adults.
0fpw | Two. Material and methods
dqn8 | Two point one. Study design and participants
cop0 | The study employed a naturalistic design, assessing the experiences of participants at bas-,
lhkq | eline before they start, and at two and four weeks after initiation. The target population included adults with ADHD symptoms who had not been diagnosed with ADHD before. To be included, participants needed to score on Conner's Adult ADHD Rating Scale. This cut-off was indicative of clinically elevated symptoms (see section two point three).
82ju | Two point two. Study procedure
y2qo | The online advertisement was placed on a website providing information about psychedelics. Interested participants were redirected to information explaining the study rationale and procedure. The baseline survey took twenty minutes to complete. If the survey had not been completed after twenty-four hours, a reminder was sent. Each of the surveys at the two and four-week time points took about fifteen minutes to complete. Participants were able to pause the surveys. Data collection occurred between November twenty twenty and July twenty twenty-one. The study was approved by the Ethics Review Committee of Psychology and Neuroscience at Maastricht University.
1piq | Two point three. Measures
7fqw | Two point three point one. Demographic information and history of substance use
rhaz | At baseline, demographic information was collected. History of substance use assessed experience with psychedelics (i.e., ayahuasca, DMT, five-MeO-DMT, LSD, novel lysergamides (e.g., one-P-LSD, ALD-fifty-two), psilocybin, salvia divinorum, ibogaine, and mescaline) in both full (psychedelic) doses and microdoses.
wak1 | Two point three point two. Psychiatric and physiological diagnoses
wtfz | Participants were asked whether they had a current diagnosis of a disorder. These answer options were chosen because most of the listed diagnoses are often reported to co-occur with ADHD, or because these diagnoses were reported to be common in people who microdose. We
3j2l | REVIEW ARTICLE Frontiers in Psychiatry: www.frontiersin.org
03k3 | Two
1k9e | constructed a variable Comorbidity alongside ADHD, differentiating respondents with and without a comorbid diagnosis alongside ADHD (P is less than point zero zero one equals only ADHD or no ADHD diagnosis; P is greater than point zero one equals ADHD and at least one other diagnosis). We constructed a variable Medication use alongside microdosing, differentiating respondents who were and were not using conventional ADHD medication alongside microdosing (zero equals only microdosing; one equals conven-




molj | 5.2 Are the generated action plans grounded in the environment?
5pto | Since successful execution of correct action plans directly measures grounding, we calculate the percentage of generated action plans that are both correct and executable. We deem an action plan to be correct if 70% or more human annotators decide it is correct. Human-written plans are 100% executable, of which 65.91% are deemed correct. Results for LMs are shown in Figure 4.
mo6k | Although smaller LMs such as GPT-2 can generate highly executable action plans as shown in Table 1, these executable plans mostly are not correct, as they often repeat the given example or do not contain all necessary steps. Increasing model parameters can lead to some improvement in generating plans that are both executable and correct, yet it scales poorly with the parameter count. In the meantime, action translation offers a promising way towards grounding actionable knowledge by producing executable and correct plans, though a large gap remains to be closed to reach human-level performance (65.91%).
zh6q | 5.3 Effect of Different Translation LMs
kp04 | In this section, we study the effect of using different Translation LM. We compare two size variants of Sentence BERT and Sentence RoBERTa [10, 27, 41] trained on the STS benchmark [6] and a baseline using averaged Glo Ve embeddings [35]. Results are shown in Table 3. Notably, we do not observe significant differences in executability and LCS across different variants of BERT and RoBERTa. We hypothesize that this is because any language models trained on reasonably large datasets should be capable of the single-step action phrase translation considered in this work. However, simply using average Glo Ve embeddings would lead to significantly reduced performance.
3e8r | 5.4 Can LLMs generate actionable programs by following step-by-step instructions?
8g7g | Prior works often focus on translating step-by-step instructions into executable programs. Specifically, instead of only providing a high-level task name, how-to instructions are also provided, as shown in Figure 5. Although this setting is easier as it does not require rich prior knowledge, how-to instructions can help resolve much ambiguity of exactly how to perform a high-level task when multiple solutions are possible. To investigate whether pre-trained LLMs are capable of doing this without additional training, we include these instructions in the prompt and evaluate LLMs with the proposed procedure. We compare to a supervised baseline from VirtualHome that trains an LSTM [17] from scratch on human-annotated data. Since the code to train the baseline is not publicly released and a different train/test split is likely used, we only show results reported in Puig et al. [38] as a crude reference. We also cannot compare executability as it is not reported. Results are shown in Table 4. Surprisingly, without being fine-tuned on any domain data, Translated Codex/GPT-3 can attain LCS close to supervised methods while generating highly executable programs.
5j4f | 5.5 Analysis of program length
4xe7 | Shorter programs have a natural advantage of being more executable as they need to satisfy less pre/post-conditions, albeit being prone to incompleteness. To validate the proposed approach does not simply generate very short programs, we calculate the average program length across the 88 evaluated tasks. Results are shown in Table 5. Mirroring the observations made in Section 4.1 and Section 4.2, we find smaller LMs such as GPT-2 tend to generate shorter programs than larger models do while frequently repeating the given executable example. In contrast, larger models like Codex and GPT-3 can generate more expressive programs with high realism, yet consequently, they often suffer from executability. We show proposed procedure can find appropriate balance and is capable of generating programs that are highly executable while maintaining reasonable expressiveness as measured by program length.
pg4b | 6 Related Works
b0d0 | Large-scale natural language modeling has witnessed rapid advances since the inception of the Transformer architecture [53]. It has been shown by recent works that large language models (LLMs) pre-trained on large unstructured text corpus not only can perform strongly on various down-stream NLP tasks [10, 39, 40, 5] but the learned representations can also be used to model relations of entities [23], retrieve matching visual features [19], synthesize code from docstrings [15, 7], solve math problems [8, 46], and even as valuable priors when applied to diverse tasks from different modalities [28, 52]. Notably, by pre-training on large-scale data, these models can also internalize an implicit knowledge base containing rich information about the world from which factual answers (e.g. "Dante was born in (PLACE)") can be extracted [36, 21, 9, 50, 42]. Compared to prior works in single-step knowledge extraction, we aim to extract sequential action plans to complete open-ended human activities while satisfying various constraints of an interactive environment.
d7eu | Many prior works have looked into grounding natural language in embodied environments. A series of them parse language instructions into formal logic or rely mainly on lexical analysis to resolve various linguistic ambiguities for embodied agents [2, 33, 34, 51]. However, they often require many hand-designed rules or scale inadequately to more complex tasks and environments. Recently, many efforts have been put into creating more realistic environments with the goal to further advances in this area [38, 47, 48, 22, 44, 1]. At the same time, by leveraging the better representation power of neural architectures, a number of works have looked into creating instruction-following agents that can perform manipulation [29, 30], navigation [11, 54, 31], or both [49, 16, 12]. Recent works also use language as hierarchical abstractions to plan actions using imitation learning [45] and to guide exploration in reinforcement learning [32].
xndr | Notably, many prior works do not leverage full-blown pre-trained LLMs; most investigate smaller LMs that require considerable domain-specific data for fine-tuning to obtain reasonable performance. Perhaps more importantly, few works have evaluated LLMs in an embodiment setting that realizes the full potential of the actionable knowledge these models already contain by pre-training on large-scale unstructured text: the tasks evaluated are often generated from a handful of templates, which do not resemble the highly diverse activities that humans perform in daily lives [14, 20]. The development of VirtualHome environment [38] enables such possibility. However, relevant works [38, 25] rely on human-annotated data and perform supervised training from scratch. Due to the lack of rich world knowledge, these models can only generate action plans given detailed instructions of how to act or video demonstrations. Concurrent work by Li et al. [24] validates similar hypothesis that
m6hi | LMs contain rich actionable knowledge. They fine-tune GPT-2 with demonstrations to incorporate environment context and to predict actions in VirtualHome, and evaluate on tasks that are generated from pre-defined predicates. In contrast, we investigate existing knowledge in LLMs without any additional training and evaluate on human activity tasks expressed in free-form language.
vpkf | 7 Conclusion, Limitations & Future Work
4d67 | In this work, we investigate actionable knowledge already contained in pre-trained LLMs without any additional training. We present several techniques to extract this knowledge to perform common-sense grounding by planning actions for complex human activities.
e7j2 | Despite promising findings, there remain several limitations of this work which we discuss as follows:
lqzt | Drop in Correctness Although our approach can significantly improve executability of the gen- erated plans, we observe a considerable drop in correctness. In addition to the errors caused by the proposed action translation (discussed in Section 4.3), this is partially attributed to the limited expressivity of VirtualHome, as it may not support all necessary actions to fully complete all evaluated tasks (correctness is judged by humans). This is also reflected by that Vanilla LMs can even surpass human-written plans, which are restricted by environment expressivity.