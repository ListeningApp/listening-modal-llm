You will receive raw text from an OCR scan of a document or web page. Each line of input represents one textbox from the document.

Your task is to label each textbox with exactly one of the following labels:
  title: the primary document title, no more than one per document
  heading: the name of the following chapter or section (should never start in the middle of the previous paragraph)
  subheading: a secondary heading, often following another heading (should never start in the middle of the previous paragraph)
  body: main paragraph content: may start or end in the middle of a word, use context to follow paragraphs that are split across textboxes (if a textbox contains both heading and body text, label it "body")
  math: textbox primarily containing math
  imageDescription: standalone callout that only describes an image, chart, table, or similar (typically begins with "Table...", "Figure...", "Fig. ...", "Above...", "Left...", etc.)
  authors: names of contributors to this document
  institutions: list of universities, business, and other institutions affiliated with this document
  publisher: info about the publisher, or provided by the publisher, including copyright info
  pagenum: the page number
  headerOrFooter: boilerplate text page number, or redundant headings at the top/bottom of the page
  toc: the table of contents
  references: bibliographic references and resources
  acknowledgements: thanks and credits to contributors
  appendix: index, appendix, or any other end matter not part of the main thread (including related headings)
  table: tabular data, label, title, or subheading for a grid or list of data
  datum: numeric data or data label from a chart or list
  advertisement: promotional ads and marketing
  layout: non-content related to the layout of the page, e.g. sidebar navigation
  callout: pull quote or long block text that stands apart from the main text
  footnote: footnotes and endnotes near the bottom of the page
  keywords: list of keywords or tags
  url: web address or email
  other: any other text that does not belong to the main thread

Each line of input is prefixed with a textbox ID in the format `id | Text content`. The output must be in the format `id | label`.

Note that body text may be split across multiple textboxes, and textboxes may start or end in the middle of a sentence or word. Because the text was extracted from a web page or document, paragraphs of body text may be interrupted by multiple textboxes of redundant headers, footnotes, page numbers, tables, images, etc. For example, a real heading will never interrupt the middle of a sentence. Use context clues like this to carefully label each textbox.

EXAMPLE INPUT:
1pjs | Neuroscience Applied 1 (2022) 101012
o2kr | Contents lists available at ScienceDirect
v6sk | Neuroscience Applied
1wj6 | journal homepage: www.journals.elsevier.com/neuroscience-applied
936l | Research Articles
06yq | Microdosing with psychedelics to self-medicate for ADHD symptoms in adults: A naturalistic study
dpv6 | ARTICLE INFO
8r5j | ABSTRACT
2taz | Keywords: ADHD Microdosing Psychedelics Self-medication Well-being Time perception
o3ya | ADHD in adulthood is often overlooked, which negatively affects the individual's well-being. First-line phar- macological interventions are effective in many ADHD patients, relieving symptoms rapidly. However, there seems to be a proportion of individuals who discontinue, or fail to respond to these treatments. For these in- dividuals, alternative treatment options should be explored.
r5il | 1. Introduction
5gui | Attention deficit hyperactivity disorder (ADHD) is one of the most common developmental disorders worldwide. Prevalence research indicates that 2.6% of the adult population has persistent ADHD. ADHD in adults is often overlooked because of the high comorbidity rate and lack of knowledge about how ADHD is expressed in adulthood (Kooij et al.,
9d4o | Fig. 1. Flowchart of included participants for each time point.
82qc | 2019). In addition, ADHD is associated with deficits in various domains of cogni- tive functioning. Twenty-five percent of ADHD cases suffered from ADHD symptoms purely because of de- ficiencies in temporal processing (Sonuga-Barke et al., 2010).
ls7d | First-line ADHD treatments in adults mainly include pharmacological interventions to enhance dopaminergic and noradrenergic neurotrans- mission with stimulants. Overall, they have been proven to work effectively in adults with ADHD, inducing fast symptom relief and thereby enhancing the person's quality of life. In the
5esx | Table 1 Demographic information from respondents at baseline and the two and four- week time points.
t8vv | longer term, approximately twenty percent of ADHD patients discontinue their prescribed medication after six to nine months, thirty percent after one year, and half of them after two years.
xrd1 | 2. Material and methods
5746 | 2.1. Study design and participants
7toz | The study employed a naturalistic design, assessing the experiences of participants at baseline,
j346 | Neuroscience Applied 1 (2022)
m5ka | E.C.H.M. Haijen et al.
kx2c | REVIEW
v8uq | before they start MD on their own initiative, and at two and four weeks after MD initiation. The target population included adults diagnosed with ADHD and individuals who experienced ADHD symptoms to the extent that these interfered with their daily lives and who had not been diag- nosed with ADHD before. To be included in the analyses, participants needed to score above a cut-off point on at least one of the subscales of the Conner's Adult ADHD Rating Scale (CAARS-S:SV).
2zmw | Fig. 2. Mean scores of the CAARS-S:SV DSM-IV total symptoms T-scores at baseline (0W) and two (2W) and four weeks (4W) after MD (A) of the whole sample, and (B) per conventional ADHD medication use. Error bars represent mean error.
sdk3 | <LATEX>\lim _ { x ightarrow \infty } rac { 6 x ^ { 2 } + 1 0 0 } { 7 x ^ { 2 } - 1 0 } =</LATEX>
x972 | 2.2. Study procedure
k221 | Mean performance measures of the CAARS-

EXAMPLE OUTPUT:
1pjs | headerOrFooter
o2kr | publisher
v6sk | publisher
1wj6 | publisher
936l | headerOrFooter
06yq | title
dpv6 | other
8r5j | heading
2taz | keywords
o3ya | body
r5il | heading
5gui | body
9d4o | imageDescription
82qc | body
ls7d | body
5esx | imageDescription
t8vv | body
xrd1 | heading
5746 | subheading
7toz | body
j346 | headerOrFooter
m5ka | authors
kx2c | headerOrFooter
v8uq | body
2zmw | imageDescription
sdk3 | math
x972 | heading
k221 | body




1pjs | arXiv:2210.07829v2 [cs.LG] 18 Oct 2022
o2kr | Asymmetric Student-Teacher Networks for Industrial Anomaly Detection
v6sk | Abstract
1wj6 | Industrial defect detection is commonly addressed with anomaly detection (AD) methods where no or only incom- plete data of potentially occurring defects is available. This work discovers previously unknown problems of student- teacher approaches for AD and proposes a solution, where two neural networks are trained to produce the same output for the defect-free training examples. The core assumption of student-teacher networks is that the distance between the outputs of both networks is larger for anomalies since they are absent in training. However, previous methods suffer from the similarity of student and teacher architecture, such that the distance is undesirably small for anomalies. For this reason, we propose asymmetric student-teacher net- works (AST). We train a normalizing flow for density es- timation as a teacher and a conventional feed-forward net- work as a student to trigger large distances for anomalies: The bijectivity of the normalizing flow enforces a divergence of teacher outputs for anomalies compared to normal data. Outside the training distribution the student cannot imitate this divergence due to its fundamentally different architec- ture. Our AST network compensates for wrongly estimated likelihoods by a normalizing flow, which was alternatively used for anomaly detection in previous work. We show that our method produces state-of-the-art results on the two cur- rently most relevant defect detection datasets MVTec AD and MVTec 3D-AD regarding image-level anomaly detec- tion on RGB and 3D data.
936l | 1. Introduction
06yq | To ensure product quality and safety standards in indus- trial manufacturing processes, products are traditionally in- spected by humans, which is expensive and unreliable in practice. For this reason, image-based methods for auto- matic inspection have been developed recently using ad- vances in deep learning [9, 18, 29, 38, 39]. Since there are no or only very few negative examples, i.e. erroneous prod- ucts, available, especially at the beginning of production, and new errors occur repeatedly during the process, tradi- tional supervised algorithms cannot be applied to this task.
dpv6 | Figure 1. Qualitative results on MVTec 3D-AD [8]. The two left columns show the input, the third the ground truth and the fourth our anomaly detection. Images are masked by foreground extrac- tion. Our method is able to successfully combine RGB and 3D data to detect defects even if only present in one data domain.
8r5j | Instead, it is assumed that only data of a normal class of defect-free examples is available in training which is termed as semi-supervised anomaly detection. This work and oth- ers [9, 22, 36, 38, 39] specialize for industrial anomaly de- tection. This domain differs in contrast to others that normal examples are similar to each other and to defective prod- ucts. In this work, we not only show the effectiveness of our method for common RGB images but also on 3D data and their combination as shown in Figure 1.
2taz | Several approaches try to solve the problem by so-called student-teacher networks [5, 7, 19, 51, 53]. First, the teacher is trained on a pretext task to learn a semantic em- bedding. In a second step, the student is trained to match the output of the teacher. The motivation is that the student can
o3ya | Figure 2. Toy example with mini-MLPs: The students were opti- mized to match the outputs in the grey area. While the symmet- ric student-teacher pair (top) generalizes unintentionally and maps anomalous data very similarly, the distance between student and teacher outputs can be used for anomaly detection in the asym- metric student-teacher pair (bottom).
r5il | only match the outputs of the teacher on normal data since it is trained only on normal data. The distance between the outputs of student and teacher is used as an indicator of an anomaly at test-time. It is assumed that this distance is larger for defective examples compared to defect-free ex- amples. However, this is not necessarily the case in pre- vious work, since we discovered that both teacher and stu- dent are conventional (i. e. non-injective) neural networks with similar architecture. A student with similar architec- ture tends to undesired generalization, such that it extrapo- lates similar outputs as the teacher for inputs that are out of the training distribution, which, in turn, gives an undesired low anomaly score. This effect is shown in Figure 2 us- ing an explanatory experiment with one-dimensional data: If the same neural network with one hidden layer is used for student and teacher, the outputs are still similar for anoma- lous data in the yellow area of the upper plot. In contrast, the outputs for anomalies diverge if an MLP with 3 hidden layers is used as the student.
5gui | In general, it is not guaranteed that an out-of-distribution input will cause a sufficiently large change in both out- puts due to the missing injectivity of common neural net- works. In contrast to normalizing flows, conventional net- works have no guarantee to provide out-of-distribution out- puts for out-of-distribution inputs. These problems moti- vate us to use an asymmetric student-teacher pair (AST): A bijective normalizing flow [34] acts as a teacher while a conventional sequential model acts as a student. In this way, the teacher guarantees to be sensitive to changes in the input caused by anomalies. Furthermore, the usage of different architectures and thus of different sets of learn- able functions enforces the effect of distant outputs for out- of-distribution samples. As a pretext task for the teacher, we optimize to transform the distribution of image features and/or depth maps to a normal distribution via maximum
82qc | likelihood training which is equivalent to a density esti- mation [15]. This optimization itself is used in previous work [22, 38, 39] for anomaly detection by utilizing the likelihoods as an anomaly score: A low likelihood of be- ing normal should be an indicator of anomalies. However, Le and Dinh [28] have shown that even perfect density es- timators cannot guarantee anomaly detection. For example, just reparameterizing the data would change the likelihoods of samples. Furthermore, unstable training leads to mises- timated likelihoods. We show that our student-teacher dis- tance is a better measure for anomaly detection compared to the obtained likelihoods by the teacher. The advantage to using a normalizing flow itself for anomaly detection is that a possible misestimation in likelihood can be compen- sated for: If a low likelihood of being normal is incorrectly assigned to normal data, this output can be predicted by the student, thus still resulting in a small anomaly score. If a high likelihood of being normal is incorrectly assigned to anomalous data, this output cannot be predicted by the stu- dent, again resulting in a high anomaly score. In this way, we combine the benefits of student-teacher networks and density estimation with normalizing flows. We further en- hance the detection by a positional encoding and by mask- ing the foreground using 3D images.
ls7d | Our contributions are summarized as follows:
tzt3 | · Our method avoids the undesired generalization from teacher to student by having highly asymmetric net- works as a student-teacher pair.
t8vv | · We improve student-teacher networks by incorporat- ing a bijective normalizing flow as a teacher.
qvnu | · Our AST outperforms the density estimation capability of the teacher by utilizing student-teacher distances.
kggw | . Code is available on GitHub1.
xrd1 | 2. Related Work
5746 | 2.1. Student-Teacher Networks
7toz | Originally, the motivation for having a student network that learns to regress the output of a teacher network was to distill knowledge and save model parameters [23, 31, 48]. In this case, a student with clearly fewer parameters com- pared to the teacher almost matches the performance. Some previous work exploits the student-teacher idea for anomaly detection by using the distance between their outputs: The larger the distance, the more likely the sample is anoma- lous. Bergmann et al. [7] propose an ensemble of students which are trained to regress the output of a teacher for im- age patches. This teacher is either a distilled version of an ImageNet-pre-trained network or trained via metric learn- ing. The anomaly score is composed of the student uncer- tainty, measured by the variance of the ensemble, and the
j346 | regression error. Wang et al. [51] extend the student task by regressing a feature pyramid rather than a single output of a pre-trained network. Bergmann and Sattlegger [5] adapt the student-teacher concept to point clouds. Local geomet- ric descriptors are learned in a self-supervised manner to train the teacher. Xiao et al. [53] let teachers learn to clas- sify applied image transformations. The anomaly score is a weighted sum of the regression error and the class score en- tropy of an ensemble of students. By contrast, our method requires only one student and the regression error as the only criterion to detect anomalies. All of the existing work is based on identical and conventional (non-injective) net- works for student and teacher, which causes undesired gen- eralization of the student as explained in Section 1.
v8uq | 2.2. Density Estimation
x972 | Anomaly detection can be viewed from a statistical per- spective: By estimating the density of normal samples, anomalies are identified through a low likelihood. The con- cept of density estimation for anomaly detection can be sim- ply realized by assuming a multivariate normal distribution. For example, the Mahalanobis distance of pre-extracted fea- tures can be applied as an anomaly score [12, 35] which is equivalent to computing the negative log likelihood of a multivariate Gaussian. However, this method is very inflex- ible to the training distributions, since the assumption of a Gaussian distribution is a strong simplification.
cwrw | To this end, many works try to estimate the density more flexibly with a Normalizing Flow (NF) [14, 22, 38, 39, 41, 44]. Normalizing Flows are a family of generative mod- els that map bijectively by construction [3, 15, 34, 52] as opposed to conventional neural networks. This property enables exact density estimation in contrast to other gen- erative models like GANs [21] or VAEs [27]. Rudolph et al. [38] make use of this concept by modeling the density of multi-scale feature vectors obtained by pre-trained net- works. Subsequently, they extend this to multi-scale feature maps instead of vectors to avoid information loss caused by averaging [39]. To handle differently sized feature maps so-called cross-convolutions are integrated. A similar ap- proach by Gudovskiy et al. [22] computes a density on fea- ture maps with a conditional normalizing flow, where like- lihoods are estimated on the level of local positions which act as a condition for the NF.
4i0e | A common problem of normalizing flows is unstable training, which has a tradeoff on the flexibility of density estimation [4]. However, even the ground truth density es- timation does not provide perfect anomaly detection, since the density strongly depends on the parameterization [28].
h2l3 | 2.3. Other Approaches
dfzb | Generative Models
u6ty | Many approaches try to tackle anomaly detection based on
awvu | other generative models than normalizing flows as autoen- coders [9, 18, 20, 37, 55, 57] or GANs [1, 11, 42]. This is motivated by the inability of these models to generate anomalous data. Usually, the reconstruction error is used for anomaly scoring. Since the magnitude of this error depends highly on the size and structure of the anomaly, these methods underperform in the industrial inspection set- ting. The disadvantage of these methods is that the synthetic anomalies cannot imitate many real anomalies.
on9b | Anomaly Synthesization
a8ge | Some work reformulates semi-supervised anomaly detec- tion as a supervised problem by synthetically generating anomalies. Either parts of training images [29, 43, 46] or random images [54] are patched into normal images. Syn- thetic masks are created to train a supervised segmentation. Traditional Approaches
h27a | In addition to deep-learning-based approaches, there are also classical approaches for anomaly detection. The one- class SVM [45] is a max-margin method optimizing a func- tion that assigns a higher value to high-density regions than to low-density regions. Isolation forests [30] are based on decision trees, where a sample is considered anomalous if it can be separated from the rest of the data by a few constraints. Local Outlier Factor [10] compares the den- sity of a point with that of its neighbors. A comparatively low density of a point identifies anomalies. Traditional ap- proaches usually fail in visual anomaly detection due to the high dimensionality and complexity of the data. This can be circumvented by combining them with other tech- niques: For example, the distance to the nearest neighbor, as first proposed by Amer and Goldstein [2], is used as an anomaly score after features are extracted by a pre-trained network [32, 36]. Alternatively point cloud features [24] or density-based clustering [16, 17] can be used to characterize a points neighborhood and label it accordingly. However, the runtime is linearly related to the dataset size.
pdjh | 3. Method
xmgs | Our goal is to train two models, a student model fs and a teacher model ft, such that the student learns to regress the teacher outputs on defect-free image data only. The train- ing process is divided into two phases: First, the teacher model is optimized to transform the training distribution px to a normal distribution N (0, I) bijectively with a normal- izing flow. Second, the student is optimized to match the teacher outputs by minimizing the distance between fs (x) and ft (x) of training samples x € X. We apply the distance for anomaly scoring at test-time, which is further described in Section 3.2.
bd9h | We follow [7, 22, 39] and use extracted features obtained by a pre-trained network on ImageNet [13] instead of RGB images as direct input for our models. Such networks have been shown to be universal feature extractors whose outputs
m047 | Figure 3. Overview of our pipeline: Teacher and student receive image features and/or depth maps as input which is conditioned by a positional encoding. First, the teacher represented by a normal- izing flow is optimized to reduce the negative log likelihood loss that may be masked by a foreground map from 3D. Second, the student is trained to match the teacher outputs by minimizing the (masked) distance between them.
gy8c | carry relevant semantics for industrial anomaly detection.
y4mj | In addition to RGB data, our approach is easily extend- able to multimodal inputs including 3D data. If 3D data is available, we concatenate depth maps to these features along the channels. Since the feature maps are reduced in height and width compared to the depth map resolution by a factor d, we apply pixel-unshuffling [56] by grouping a depth image patch of d × d pixels as one pixel with ď2 chan- nels to match the dimensions of the feature maps.