{
  "id": "chatcmpl-A3U7Q8numKvR6RKfdALCAkmXCMOmF",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "mbsp | Such a large architecture can be trained very efficiently, but doing so requires the use of a few techniques that are described in the appendixes. Appendix A describes details such as the particular sigmoid used and the weight initialization. Appendixes B and C describe the minimization procedure used, which is a stochastic version of a diagonal approximation to the Levenberg-Marquardt procedure.\nzynb | Three. Results and Comparison with Other Methods\n8h0u | While recognizing individual digits is only one of many problems involved in designing a practical recognition system, it is an excellent benchmark for comparing shape recognition methods. Though many existing methods combine a hand-crafted feature extractor and a trainable classifier, this study concentrates on adaptive methods that operate directly on size-normalized images.\nb09f | A. Database: The Modified NIST Set\nzutp | The database used to train and test the systems described in this paper was constructed from the NIST's Special Database Three and Special Database One containing binary images of handwritten digits. NIST originally designated S D Three as their training set and S D One as their test set. However, S D Three is much cleaner and easier to recognize than S D One. The reason for this can be found on the fact that S D Three was collected among Census Bureau employees, while S D One was collected among high school students. Drawing sensible conclusions from learning experiments requires that the result be independent of the choice of training set and test among the complete set of samples. Therefore it was necessary to build a new database by mixing NIST's datasets.\nqgmy | S D One contains fifty-eight thousand five hundred twenty-seven digit images written by five hundred different writers. In contrast to S D Three, where blocks of data from each writer appeared in sequence, the data in S D One is scrambled. Writer identities for S D One are available and we used this information to unscramble the writers. We then split S D One in two: characters written by the first two hundred fifty writers went into our new training set. The remaining two hundred fifty writers were placed in our test set. Thus we had two sets with nearly thirty thousand examples each. The new training set was completed with enough examples from S D Three, starting at pattern number zero, to make a full set of sixty thousand training patterns. Similarly, the new test set was completed with S D Three examples starting at pattern number thirty-five thousand to make a full set with sixty thousand test patterns. In the experiments described here, we only used a subset of ten thousand test images (five thousand from S D One and five thousand from S D Three), but we used the full sixty thousand training samples. The resulting database was called the modified NIST, or MNIST, dataset.\n40wi | The original black and white (bilevel) images were size normalized to fit in a twenty by twenty pixel box while preserving their aspect ratio. The resulting images contain grey levels as a result of the antialiasing (image interpolation) technique used by the normalization algorithm. Three versions of the database were used. In the first version, the images were centered in a twenty-eight by twenty-eight image by computing the center of mass of the pixels and translating the image so as to position this point at the center of the twenty-eight by twenty-eight field. In some instances, this twenty-eight by twenty-eight field was extended to thirty-two by thirty-two with background pixels. This version of the database will be referred to as the regular database. In the second version of the database, the character images were deslanted and cropped down to twenty by twenty pixel images. The deslanting computes the second moments of inertia of the pixels (counting a foreground pixel as one and a background pixel as zero) and shears the image by horizontally shifting the lines so that the principal axis is vertical. This version of the database will be referred to as the deslanted database. In the third version of the database, used in some early experiments, the images were reduced to sixteen by sixteen pixels. Fig. four shows examples randomly picked from the test set.\na418 | B. Results\ne9lu | Several versions of LeNet-5 were trained on the regular MNIST database. Twenty iterations through the entire training data were performed for each session. The values of the global learning rate n [see (twenty-one) in Appendix C for a definition] was decreased using the following schedule: zero point zero zero zero five for the first two passes; zero point zero zero zero two for the next three; zero point zero zero zero one for the next three; zero point zero zero zero zero five for the next four; and zero point zero zero zero zero one thereafter. Before each iteration, the diagonal\na278 | Hessian approximation was reevaluated on five hundred samples, as described in Appendix C, and was kept fixed during the entire iteration. The parameter was set to zero point zero two. The resulting effective learning rates during the first pass varied between approximately seven times ten to the negative five and zero point zero one six over the set of parameters. The test error rate stabilizes after around ten passes through the training set at zero point ninety-five percent. The error rate on the training set reaches zero point thirty-five percent after nineteen passes. Many authors have reported observing the common phenomenon of overtraining when training N N's or other adaptive algorithms on various tasks. When overtraining occurs, the training error keeps decreasing over time but the test error goes through a minimum and starts increasing after a certain number of iterations. While this phenomenon is very common, it was not observed in our case as the learning curves in Fig. five show. A possible reason is that the learning rate was kept relatively large. The effect of this is that the weights never settle down in the local minimum but keep oscillating randomly. Because of those fluctuations, the average cost will be lower in a broader minimum. Therefore, stochastic gradient will have a similar effect as a regularization term that favors broader minima. Broader minima correspond to solutions with large entropy of the parameter distribution, which is beneficial to the generalization error.\nffto | The influence of the training set size was measured by training the network with fifteen thousand, thirty thousand, and sixty thousand examples. The resulting training error and test error are shown in Fig. six. It is clear that, even with specialized architectures such as LeNet-5, more training data would improve the accuracy.\nak21 | To verify this hypothesis, we artificially generated more training examples by randomly distorting the original training images. The increased training set was composed of the sixty thousand original patterns plus five hundred forty thousand instances of distorted patterns with randomly picked distortion parameters.\ni7n1 | The distortions were combinations of the following planar affine transformations: horizontal and vertical translations; scaling; squeezing (simultaneous horizontal compression and vertical elongation, or the reverse); and horizontal shearing. Fig. seven shows examples of distorted patterns used for training. When distorted data were used for training, the test error rate dropped to zero point eight percent (from zero point ninety-five percent without deformation). The same training parameters were used as without deformations. The total length of the training session was left unchanged (twenty passes of sixty thousand patterns each). It is interesting to note that the network effectively sees each individual sample only twice over the course of these twenty passes.\nm9yn | C. Comparison with Other Classifiers",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725394124,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 1504,
    "prompt_tokens": 3224,
    "total_tokens": 4728
  }
}