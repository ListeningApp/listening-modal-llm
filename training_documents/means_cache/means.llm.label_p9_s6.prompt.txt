You will receive raw text from an OCR scan of a document or web page. Each line of input represents one textbox from the document.

Your task is to label each textbox with exactly one of the following labels:
  title: the primary document title, no more than one per document
  heading: the name of the following chapter or section (should never start in the middle of the previous paragraph)
  subheading: a secondary heading, often following another heading (should never start in the middle of the previous paragraph)
  body: main paragraph content: may start or end in the middle of a word, use context to follow paragraphs that are split across textboxes (if a textbox contains both heading and body text, label it "body")
  math: textbox primarily containing math
  imageDescription: standalone callout that only describes an image, chart, table, or similar (typically begins with "Table...", "Figure...", "Fig. ...", "Above...", "Left...", etc.)
  authors: names of contributors to this document
  institutions: list of universities, business, and other institutions affiliated with this document
  publisher: info about the publisher, or provided by the publisher, including copyright info
  pagenum: the page number
  headerOrFooter: boilerplate text page number, or redundant headings at the top/bottom of the page
  toc: the table of contents
  references: bibliographic references and resources
  acknowledgements: thanks and credits to contributors
  appendix: index, appendix, or any other end matter not part of the main thread (including related headings)
  table: tabular data, label, title, or subheading for a grid or list of data
  datum: numeric data or data label from a chart or list
  advertisement: promotional ads and marketing
  layout: non-content related to the layout of the page, e.g. sidebar navigation
  callout: pull quote or long block text that stands apart from the main text
  footnote: footnotes and endnotes near the bottom of the page
  keywords: list of keywords or tags
  url: web address or email
  other: any other text that does not belong to the main thread

Each line of input is prefixed with a textbox ID in the format `id | Text content`. The output must be in the format `id | label`.

Note that body text may be split across multiple textboxes, and textboxes may start or end in the middle of a sentence or word. Because the text was extracted from a web page or document, paragraphs of body text may be interrupted by multiple textboxes of redundant headers, footnotes, page numbers, tables, images, etc. For example, a real heading will never interrupt the middle of a sentence. Use context clues like this to carefully label each textbox.

EXAMPLE INPUT:
1pjs | Neuroscience Applied 1 (2022) 101012
o2kr | Contents lists available at ScienceDirect
v6sk | Neuroscience Applied
1wj6 | journal homepage: www.journals.elsevier.com/neuroscience-applied
936l | Research Articles
06yq | Microdosing with psychedelics to self-medicate for ADHD symptoms in adults: A naturalistic study
dpv6 | ARTICLE INFO
8r5j | ABSTRACT
2taz | Keywords: ADHD Microdosing Psychedelics Self-medication Well-being Time perception
o3ya | ADHD in adulthood is often overlooked, which negatively affects the individual's well-being. First-line phar- macological interventions are effective in many ADHD patients, relieving symptoms rapidly. However, there seems to be a proportion of individuals who discontinue, or fail to respond to these treatments. For these in- dividuals, alternative treatment options should be explored.
r5il | 1. Introduction
5gui | Attention deficit hyperactivity disorder (ADHD) is one of the most common developmental disorders worldwide. Prevalence research indicates that 2.6% of the adult population has persistent ADHD. ADHD in adults is often overlooked because of the high comorbidity rate and lack of knowledge about how ADHD is expressed in adulthood (Kooij et al.,
9d4o | Fig. 1. Flowchart of included participants for each time point.
82qc | 2019). In addition, ADHD is associated with deficits in various domains of cogni- tive functioning. Twenty-five percent of ADHD cases suffered from ADHD symptoms purely because of de- ficiencies in temporal processing (Sonuga-Barke et al., 2010).
ls7d | First-line ADHD treatments in adults mainly include pharmacological interventions to enhance dopaminergic and noradrenergic neurotrans- mission with stimulants. Overall, they have been proven to work effectively in adults with ADHD, inducing fast symptom relief and thereby enhancing the person's quality of life. In the
5esx | Table 1 Demographic information from respondents at baseline and the two and four- week time points.
t8vv | longer term, approximately twenty percent of ADHD patients discontinue their prescribed medication after six to nine months, thirty percent after one year, and half of them after two years.
xrd1 | 2. Material and methods
5746 | 2.1. Study design and participants
7toz | The study employed a naturalistic design, assessing the experiences of participants at baseline,
j346 | Neuroscience Applied 1 (2022)
m5ka | E.C.H.M. Haijen et al.
kx2c | REVIEW
v8uq | before they start MD on their own initiative, and at two and four weeks after MD initiation. The target population included adults diagnosed with ADHD and individuals who experienced ADHD symptoms to the extent that these interfered with their daily lives and who had not been diag- nosed with ADHD before. To be included in the analyses, participants needed to score above a cut-off point on at least one of the subscales of the Conner's Adult ADHD Rating Scale (CAARS-S:SV).
2zmw | Fig. 2. Mean scores of the CAARS-S:SV DSM-IV total symptoms T-scores at baseline (0W) and two (2W) and four weeks (4W) after MD (A) of the whole sample, and (B) per conventional ADHD medication use. Error bars represent mean error.
sdk3 | <LATEX>\lim _ { x ightarrow \infty } rac { 6 x ^ { 2 } + 1 0 0 } { 7 x ^ { 2 } - 1 0 } =</LATEX>
x972 | 2.2. Study procedure
k221 | Mean performance measures of the CAARS-

EXAMPLE OUTPUT:
1pjs | headerOrFooter
o2kr | publisher
v6sk | publisher
1wj6 | publisher
936l | headerOrFooter
06yq | title
dpv6 | other
8r5j | heading
2taz | keywords
o3ya | body
r5il | heading
5gui | body
9d4o | imageDescription
82qc | body
ls7d | body
5esx | imageDescription
t8vv | body
xrd1 | heading
5746 | subheading
7toz | body
j346 | headerOrFooter
m5ka | authors
kx2c | headerOrFooter
v8uq | body
2zmw | imageDescription
sdk3 | math
x972 | heading
k221 | body




dfzb | We can now see that the <LATEX>b</LATEX> b-values for the two dummy variables represent the differences between these adjusted means <LATEX>\left( 4 . 7 1 \quad - \quad 2 . 9 3 = 1 . 7 8 \right.</LATEX> for Dummy 1 and <LATEX>5 . 1 5 \quad - \quad 2 . 9 3 = 2 . 2 2</LATEX> for Dummy 2). These adjusted means are the average amount of happiness for each group at the mean level of love of puppies. Some people think of this kind of model (i.e., ANCOVA) as 'controlling' for the covariate, because it compares the predicted group means at the average value of the covariate, so the groups are being compared at a level of the covariate that is the same for each group. However, as we shall see, the 'controlling for the covariate' analogy is not a good one.
u6ty | Output 13.2
awvu | 13.4 Assumptions and issues in ANCOVA
on9b | Including covariates doesn't change the fact we're using the general linear model, so all the sources of potential bias (and counteractive measures) discussed in Chapter 6 apply. There are two additional considerations: (1) independence of the covariate and treatment effect; and (2) homogeneity of regression slopes.
a8ge | 13.4.1 Independence of the covariate and treatment
h27a | IIII
pdjh | effect
xmgs | I said in the previous section that covariates can be used to reduce within-group error variance if the covariate explains some of this error variance, which will be the case if the covariate is independent of the experimental effect (group means). Figure 13.2 shows three different scenarios. Part A shows a basic model that compares group means (it is like Figure 12.5). The variance in the outcome (in our example happiness) can be partitioned into two parts that represent the experimental or treatment effect (in this case the administration of puppy therapy) and the error or unexplained variance (i.e., factors that affect happiness that we haven't measured). Part B shows the ideal scenario when including a covariate, which is that the covariate shares its variance only with the bit of happiness that is currently unexplained. In other words, it is completely independent of the treatment effect (it does not overlap with the effect of puppy therapy at all). Some argue that this scenario is the only one in which ANCOVA is appropriate (Wildt & Ahtola, 1978). Part C shows a situation in which the effect of the covariate overlaps with the experimental effect. In other words, the experimental effect is confounded with the effect of the covariate. In this situation, the covariate will reduce (statistically speaking) the experimental effect because it explains some of the variance that would otherwise be attributable to the experiment. When the covariate and the experimental effect (independent variable) are not independent, the treatment effect is obscured, spurious treatment effects can arise, and at the very least the interpretation of the ANCOVA is seriously compromised (Wildt & Ahtola, 1978).
bd9h | The problem of the covariate and treatment sharing variance is common and is ignored or misunderstood by many people (Miller & Chapman, 2001). Miller and Chapman are not the only people to point this out, but their paper is very readable and they cite many examples of people misapplying ANCOVA. Their main point is that when treatment groups differ on the covariate, putting the covariate into the analysis will not 'control for' or 'balance out' those differences (Lord, 1967, 1969). This situation arises mostly when participants are not randomly assigned to experimental treatment conditions. For example, anxiety and depression are closely correlated (anxious people tend to be depressed), so if you wanted to compare an anxious group of people against a non-anxious group on some task, the chances are that the anxious group would also be more depressed than the non-anxious group. You might think that by adding depression as a covariate into the analysis you can look at the 'pure' effect of anxiety, but you can't. This situation matches part C of Figure 13.2 because the effect of the covariate (depression) would contain some of the variance from the effect of anxiety. Statistically speaking, all that we know is that anxiety and depression share variance; we cannot separate this shared
m047 | variance into 'anxiety variance' and 'depression variance', it will always be â€˜shared'. Another common example is if you happen to find that your experimental groups differ in their ages. Placing age into the analysis as a covariate will not solve this problem - it is still confounded with the experimental manipulation. The use of covariates cannot solve this problem (see Jane Superbrain Box 13.1).
gy8c | Figure 13.2 The role of the covariate in ANCOVA (see text for details)
y4mj | This problem can be avoided by randomizing participants to experimental groups, or by matching experimental groups on the covariate (in our anxiety example, you could try to find participants for the low-anxiety group who score high on depression). We can see whether this problem is likely to be an issue by checking whether experimental groups differ on the covariate before fitting the model. To use our anxiety example again, we could test whether our high-and low-anxiety groups differ on levels of depression. If the groups do not
vpgq | significantly differ then we might consider it reasonable to use depression as a covariate.
ousl | The treatment effect and covariate are simply predictor variables in a general linear model, yet despite several hundred pages discussing linear models, I haven't before mentioned that predictors should be completely independent. I've said that they shouldn't overlap too much (e.g., collinearity) but that's quite different from saying that they shouldn't overlap at all. If, in general, we don't care about predictors being independent in linear models, why should we care now? The short answer is we don't - there is no statistical requirement for the treatment variable and covariate to be independent.
m4ju | However, there are situations in which ANCOVA can be biased when the covariate is not independent of the treatment variable. One situation, common in medical research, has been discussed a lot: an outcome (e.g., hypertension) is measured at baseline, and after a treatment intervention (with participants assigned to a treatment or control group). This design can be analysed using an ANCOVA in which treatment effects on post-intervention hypertension are analysed while covarying baseline levels of hypertension. In this scenario the independence of treatment and covariate variables means that baseline levels of hypertension are equal in the different treatment groups. According to Senn (2006), the idea that ANCOVA is biased unless treatment groups are equal on the covariate applies only when there is temporal additivity. To use our hypertension example, temporal additivity is the assumption that both treatment groups would experience the same change in hypertension over time
a0mb | if the treatment had no effect. In other words, had we left the two groups alone, their hypertension would change by exactly the same amount. Given that the groups have different overall levels of hypertension to begin with, this assumption might not be reasonable, which undermines the argument for requiring group equality in baseline measures.
ki8m | To sum up, the independence of the covariate and treatment makes interpretation more straightforward but is not a statistical requirement. ANCOVA can be unbiased when groups differ on levels of the covariate, but as Miller and Chapman point out, it creates an interpretational problem that ANCOVA cannot magic away.
km8w | 13.4.2 Homogeneity of regression slopes
wzno | When a covariate is used we look at its overall relationship with the outcome variable: we ignore the group to which a person belongs. We assume that this relationship between covariate and outcome variable holds true for all groups of participants, which is known as the assumption of homogeneity of regression slopes. Think of the assumption like this: imagine a scatterplot for each group of participants with the covariate on one axis, the outcome on the other, and a regression line summarizing their relationship. If the assumption is met then the regression lines should look similar (i.e., the values of b in each group should be equal).
xagm | Figure 13.3 Scatterplot and regression lines of happiness against love of puppies for each of the experimental conditions
iwn5 | Let's make this concept a bit more concrete. Remember that the main example in this chapter looks at whether different doses of puppy therapy affect happiness when including love of puppies as a covariate. The homogeneity of regression slopes assumption means that the relationship between the outcome (dependent variable) and the covariate is the same in each of our treatment groups. Figure 13.3 shows a scatterplot with regression line that summarizes this relationship (i.e., the relationship between love of puppies, the covariate, and the outcome, participant's happiness) for the three experimental conditions (shown in different panels). There is a positive relationship (the regression line slopes upwards from left to right) between love of puppies and participant's happiness in both the control (left panel) and 15-minute conditions (middle panel). In fact, the slopes of the lines for these two groups are very similar, showing that the relationship between happiness and love of puppies is very similar in these two groups. This situation is an example of homogeneity of regression slopes. However, in the 30- minute condition (right panel) there is a slightly negative relationship between happiness and love of puppies. The slope of this line differs from the slopes in the other two groups, suggesting heterogeneity of regression slopes (because the relationship between happiness and love of puppies is different in the 30-minute group compared to the other two groups).
uooz | Although in a traditional ANCOVA heterogeneity of regression slopes is a bad thing (Jane Superbrain Box 13.2), there are situations where you might expect regression slopes to differ across groups and that variability may be interesting.
pxbq | For example, when research is conducted across different locations, you might expect the effects to vary across those locations. Imagine you had a new treatment for backache, and you recruit several physiotherapists to try it out in different hospitals. The effect of the treatment is likely to differ across these hospitals (because therapists will differ in expertise, the patients they see will have different problems and so on). As such, heterogeneity of regression slopes is not a bad thing per se. If you have violated the assumption of homogeneity of regression slopes, or if the variability in regression slopes is an interesting hypothesis in itself, then you can explicitly model this variation using multilevel linear models (see Chapter 21).
jfsr | 13.4.3 What to do when assumptions are violated
u15r | A bootstrap for the model parameters and post hoc tests can be used so that these, at least, are robust (see Chapter 6). The bootstrap won't help for the F- tests though. There is a robust variant of ANCOVA that can be implemented using R, and we'll discuss this in Section 13.8.
lq35 | 13.5 Conducting ANCOVA using SPSS Statistics IIII
3g9w | 13.5.1 General procedure
d09o | The general procedure is much the same as for any linear model, so remind yourself of the steps for fitting a linear model (Chapter 9). Figure 13.4 shows a simpler overview of the process that highlights some of the specific issues for ANCOVA-style models. As with any analysis, begin by graphing the data and looking for and correcting sources of bias.
ylhz | Figure 13.4 General procedure for analysis of covariance
a3td | We have already looked at the data (Table 13.1) and the data file (Puppy Love.sav). To remind you, the data file is set out like Table 13.1 and contains three columns: a coding variable called Dose <LATEX>\left( 1 = c o n t r o l , \right.</LATEX> <LATEX>2 = 1 5</LATEX> minutes, <LATEX>3 =</LATEX> 30 minutes), a variable called Happiness containing the scores for the person's happiness, and a variable called Puppy_love containing the scores for love of puppies from 0 to 7. The 30 rows correspond to each person's scores on these three variables.
ex5k | 13.5.3 Testing the independence of the treatment
9d4o | II
cjq2 | variable and covariate
5esx | In Section 13.4.1, I mentioned that if the covariate and group means (independent variable) are independent then the interpretation of ANCOVA models is a lot more straightforward. In this case, the covariate is love of puppies, so we'd want to check that the mean level of love of puppies is roughly equal across the three puppy therapy groups by fitting a linear model with Puppy_love as the outcome and Dose as the predictor.
m5ka | SELF TEST
byu8 | Fit a model to test whether love of puppies (our covariate) is independent of the dose of puppy therapy (our independent variable).
76b0 | Output 13.3 shows that the main effect of dose is not significant, <LATEX>F \left( 2 , 2 7 \right) = 1 . 9 8 ,</LATEX> <LATEX>p = 0 . 1 6 ,</LATEX> which shows that the average level of love of puppies was roughly the same in the three puppy therapy groups. In other words, the means for love of puppies in Table 13.2 are not significantly different across the control, 15-and 30-minute groups. This result is good news for using love of puppies as a covariate in the model.
zxz6 | Output 13.3
xnth | 13.5.4 The main analysis
vafm | Most of the General Linear Model (GLM) procedures in SPSS Statistics contain the facility to include one or more covariates. For designs that don't involve
2zmw | repeated measures it is easiest to include covariates by selecting Analyze General Linear Model GLM GEN Univariate ... to activate the dialog box in Figure 13.5. Drag the variable Happiness into the box labelled Dependent Variable (or click
bf9k | ), drag Dose into the box labelled Fixed Factor(s) and drag Puppy_love into the box labelled Covariate(s).
otkr | Figure 13.5 Main dialog box for GLM univariate
ibgs | 13.5.5 Contrasts
colo | There are various dialog boxes that can be accessed from the main dialog box. If a covariate is selected, the post hoc tests are disabled because the tests that we used in the previous chapter are not designed for models that include covariates.
d6r4 | However, comparisons can be done by clicking Contrasts ...
9mrz | to access the
pk06 | Contrasts dialog box in Figure 13.6. You cannot enter codes to specify user- defined contrasts (but see SPSS Tip 13.1); instead you can select one of the standard contrasts that we met in Table 12.6. In this example, there was a control condition (coded as the first group), so a sensible set of contrasts would be simple contrasts comparing each experimental group to the control (this results in the same contrasts as dummy coding). Click the drop-down list ( None and select a type of contrast (in this case Simple) from this list. For simple contrasts you need to specify the reference category (i.e., the category against which all other groups are compared). By default the last category is used, which for our data is the 30-minute group. We need to change the reference category to be the control group, which is the first category (assuming that you coded control as 1). We make this change by selecting
7wn9 | @ First . Having selected a contrast, click Change
wcd0 | to register the
caos | selection. Figure 13.6 shows the completed dialog box. Click Continue to return to the main dialog box.
qrvy | 13.5.6 Other options