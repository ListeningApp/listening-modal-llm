{
  "id": "chatcmpl-A3UDqxslYVvK2WXcantKe1u63hpOT",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "p2n3 | As in the variants of verbal N-back tasks, we also have \"spatial-with-noise,\" \"spatial-with-feedback,\" and \"spatial-with-CoT-reasoning\" versions of the task. The prompts for the the with-feedback and with-reasoning versions were basically the same as those for the\nohh7 | corresponding verbal tasks. For the spatial-with-noise version, we added a noise character (chosen from \"# dollar percent and at \") to one to three unoccupied locations in the three by three grid\nvbni | on every trial. This is a first step to examine the LLM's spatial working memory when it was not able to get the correct answer by simply doing string match.\ndgcw | To further confirm that the LLM can really reason in a spatial way rather than trivially performing some kind of string match under the hood, we further introduced two variants that specifically require abstract spatial reasoning; an model that would otherwise simply match strings would have failed. To achieve so, in these two tasks, a match is defined as when the location of the letter X is in the same row or column as the X N trials ago. The difference is whether identical locations also count as a match. We expect the version excluding identical locations to be harder for the LLM to perform.\ngvwz | We also explored whether the size of the grid three by three, four by four, five by five would influence the LLM's performance. To the best of our knowledge, there hasn't been human studies exploring how the number of all possible spatial locations would impact behavioral performance. In light of this, we didn't have specific assumptions for how the LLM would perform differently under these scenarios.\nsk5m | Four Results\nqws8 | To analyse the model's performance on our experiments, we used four widely accepted performance metrics reported in numerous human behavioral studies:\n877u | Hit Rate: It is a performance measure used in various fields, including computer science, statistics,\nrzhj | and information retrieval. It represents the proportion of correct or successful outcomes out of the\nbfax | total number of targets or true positives. Mathematically, it is calculated by\nf2y8 | Hit Rate equals Number of True Positives plus Number of False Negatives divided by Number of True Positives\no7qm | False Alarm Rate: It quantifies the frequency at which a system or algorithm incorrectly identifies a\nd52l | negative outcome as positive. Mathematically, it is calculated by\nqlbb | Number of False Positives\nnhsg | False Alarm Rate equals Number of False Positives plus Number of True Negatives\nhy0l | Accuracy: It is a commonly used performance metric that measures the correctness or reliability of a\n4jmz | system, model, or algorithm in making predictions or classifications. It represents the proportion of\nb7cq | correct predictions or classifications out of the total number of predictions or classifications made.\n0b39 | Mathematically, it is calculated by\nqecd | Accuracy equals Number of Correct Responses divided by Total Number of Trials\n1o7p | \ncp13 | Detection Sensitivity d prime: It is a statistical measure used to assess the ability of a diagnostic test\nbgre | or classification model to accurately distinguish between two groups or conditions. It quantifies the\n57lp | extent to which the test or model can correctly identify positive cases relative to negative cases while\nukth | minimizing false positives and false negatives. Mathematically, it is calculated by\nkpvt | d prime equals Z Hit Rate minus Z False Alarm Rate\njie2 | where Z Hit Rate and Z False Alarm Rate represent the z-score of Hit Rate and False Alarm Rate, respectively.\nd0ik | In the current study, we did fifty blocks of tests for N equals one, two, three in each experiment, which allows us to calculate the standard error of mean and draw error bars to visualize the reliability of our\n0x0s | findings.\n4exe | Four point one Verbal N-back experiments\n0myz | In all versions of the task, we observed a performance pattern strikingly consistent with human participants, with the LLM's performance declining significantly when N increased from one to three five,\nxh3i | as shown in hit rate, accuracy, and d prime. Compared to the base version, the verbal-with-noise variant\nsyiw | significantly made the LLM's performance worse. We observe that while chain-of-thought prompting has significantly improved the performance of the language models in verbal task variants, feedback on whether the model has performed correctly in the previous task failed to meaningfully improve performance.\nkg3t | Four point two Spatial N-back experiments\nlvae | In the four versions spatial tasks corresponding to the above verbal tasks, same patterns of performance were basically replicated. CoT reasoning significantly made the LLM perform better,\no9sc | adding noise made the model perform worse. But in all versions of the task, ChatGPT suffered significant declines in performance as N increases.\n121u | When further evaluated whether the LLM could conduct abstract spatial reasoning. The results confirmed so. In line with our prediction, the LLM performed worse when identical\nzh03 | locations are not defined a match, which means more abstract spatial reasoning would be required in\n1mvw | this scenario.\n6d4s | Our explorations on the effect of grid size on the model performance yielded interesting results, too.\nmqah | The LLM performed better when grid size was larger, especially as seen from the hit rate of d prime results\nxxr8 | in Figure eight.\nrl4j | Five Discussion\nvz32 | We argue that our experimental results firmly point to the conclusion that ChatGPT has limited working memory capacity similar to humans. Even various prompting techniques such as the provision of feedback and the use of state-of-the-art chain-of-thought prompting may be used to improve its performance, the trend of performance decline as a function of increasing N still bears striking resemblance to human performance shown in numerous previous work. The consistent pattern of performance declines thus might be reflecting a fundamental constraint emerged from the architecture of the model, suggesting a possibility that the low-level working memory mechanism of LLMs might be similar to human working memory at least in some aspects.\nncb0 | In neuroscience, there are many unsolved problems on working memory, especially where and how working memory is encoded and maintained in the brain and why working memory capacity is limited. We propose that, in light of the above observation, ChatGPT and or other large language models of similar calibre could be potentially used and tested as a modeling platform for studying human working memory, just as what neuroscientists have done in recent years using other artificial\n8mp0 | neural networks. Furthermore, future efforts aimed at interpreting activity of artificial neurons\nxjyk | in LLMs like ChatGPT would probably hold potential in informing the mechanisms of human working memory.\nmt0f | Our work also has some limitations. It would be important to test other LLMs on the same task we used here, to test whether they exhibit similar performance patterns and whether they have different\nawab | working memory capacity. It would also be helpful to test ChatGPT on other working memory span\nixcc | tasks used in cognitive sciences to address the generalizability of N-back tasks as measurement tools.",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725394522,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 1511,
    "prompt_tokens": 3488,
    "total_tokens": 4999
  }
}