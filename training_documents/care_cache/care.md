## Implementation science: What is it and why should I care?

ABSTRACT

Centuries of experience make it clear that establishing the effectiveness of a clinical innovation is not sufficient to guarantee its uptake into routine use. The relatively new field of implementation science has developed to enhance the uptake of evidence-based practices and thereby increase their public health impact.

Implementation science shares many characteristics, and the rigorous approach, of clinical research. However, it is distinct in that it attends to factors in addition to the effectiveness of the clinical innovation itself, to include identifying and addressing barriers and facilitators to the uptake of evidence-based clinical innovations.

This article reviews the definition, history, and scope of implementation science, and places the field within the broader enterprise of biomedical research. It also provides an overview of this Special Issue of Psychiatry Research, which introduces the principles and methods of implementation science to mental health researchers.


## One. Why worry about implementation?

The title of this article poses two questions. Let us answer the second question first, beginning with a story:

It was, by all estimations, a successful research effort. We had mounted a randomized, controlled clinical trial across eleven sites in the US Department of Veterans Affairs, testing an organization of care called the Collaborative Chronic Care Model for bipolar disorder versus treatment as usual. Over three years of follow-up, the Collaborative Chronic Care Model showed significant positive impact on weeks in mood episode, mental health quality of life, social role function, and satisfaction with care-all at no increased cost to the healthcare system. In parallel, a two-year, four-site randomized controlled clinical trial of the bipolar Collaborative Chronic Care Model in the Group Health Cooperative of Puget Sound, showed very similar outcomes at minimal cost, compared to treatment as usual. Both studies were published in the same year in mainstream psychiatric journals that are read and respected by mental health researchers, clinicians, and administrators. The Collaborative Chronic Care Model for bipolar disorders began to be endorsed by national clinical practice guidelines in the US Department of Veterans Affairs and in Canada, and the bipolar Collaborative Chronic Care Model was listed on the US Substance Abuse and Mental Health Services Administration's prestigious National Registry of Evidence-Based Programs and Practices.

A worthwhile innovation in care? It was a no-brainer: improved outcome at little to no cost.

And yet, within a year of the end of the studies, none of the fifteen sites had incorporated the Collaborative Chronic Care Model into their usual workflow. The clinicians who had participated in the Collaborative Chronic Care Model went back to their usual duties, and the individuals with bipolar disorder went back to receiving their usual form of care. For all practical purposes, bipolar Collaborative Chronic Care Models ceased to exist at these sites. Perhaps ironically, the only vestige of the Collaborative Chronic Care Model that remained one year after the study was that a group of patients at one of the sites continued to meet on their own to hold self-management skills groups, which were part of the Collaborative Chronic Care Model intervention.

Everything went right. What went wrong?

The experience in the story is not unique, as many clinical trialists have discovered. Succinctly put: Establishing effectiveness of an innovation does not guarantee its uptake into routine usage. Classic studies indicate that it takes seventeen to twenty years to get clinical innovations into practice; moreover, fewer than fifty percent of clinical innovations ever make it into general usage. Chalmers and Glasziou estimate that eighty percent of medical research dollars do not make a public health impact for various reasons. If only half of clinical innovations ever make it to general use, society's return on investment for each medical research dollar is diminished even further. Funders are beginning to take note.

Perhaps this lack of uptake of clinical innovations (among which we consider interventions, assessment and treatment technologies, and new organizations of care) is because the rapid pace of modern biomedical research has outstripped society's absorptive capacity. On the contrary: the problem is centuries old. For instance, the first observation that citrus cures scurvy in the British Navy occurred in sixteen zero one, with the first randomized controlled trial of citrus to treat scurvy conducted in seventeen forty-seven. Yet the British Navy did not adopt routine use of citrus to prevent scurvy until seventeen ninety-five, and the British merchant marine not until eighteen sixty-five.

Additional instances of the lag between evidence and widespread usage are provided by Colditz and Emmons. They contrast the slow uptake of the smallpox vaccine, penicillin, and insulin to the much more rapid uptake of anti-retroviral treatments for HIV and AIDS, making the point that contextual factors, not treatment effectiveness, play a dominating role in whether and how quickly a clinical innovation will become widely used. Similarly, Ferlie and coworkers studied non-adoption of eight hospital-based or primary care-based innovations in the UK. Their analysis demonstrated that, while strength of evidence contributed to the adoption of some clinical innovations, a more comprehensive explanation for non-adoption needed to also include contextual issues, such as the professional disciplines of the intended recipients.

Thus, the problem of non-uptake of effective clinical innovations is longstanding and persistent-and likely the rule rather than the exception. Moreover, the clues to this problem lie beyond simply the clinical innovation itself, and include contextual and factors related to use or non-use of the clinical innovation.


## Two. Implementation challenges in the biomedical research enterprise

The process of biomedical research can be conceptualized as a pipeline from concept development to public health impact. Those of us who came into research several decades ago worked from a simplistic model which assumed that developing evidence that a clinical innovation was efficacious was sufficient to generate public health impact.

In the late nineteen nineties researchers became aware of the need for studies, particularly clinical trials, which moved out of the rarified environment of the academic medical center into venues more typical of those in which the clinical innovation would be used. Thus, the focus of clinical research broadened beyond solely internal validity (which prioritizes isolating treatment effects from all extraneous influences in order to determine whether an innovation has an impact), to also include considerations of external validity (which prioritizes the relevance and transferability of innovation effects from highly controlled experimental conditions into conditions under which the eventual end-user of the innovation typically works).

Accordingly, a variety of types of clinical trials that prioritize external validity developed under the general concept of "practice-based research". Such trial designs are based on a variety of overlapping concepts-e.g., effectiveness studies, practical clinical trials, pragmatic clinical trials, comparative effectiveness trials, large simple clinical trials-but all have in common prioritizing evaluation of clinical innovations in environments in which the innovation will potentially be used. The biomedical research pipeline can thus be amended to illustrate an "efficacy-effectiveness" continuum, in which trials differ based on whether they seek primarily to favor study efficacy (internal validity, i.e., isolating and maximizing potential treatment impact) versus effectiveness (external validity, i.e., generalizability to the practice settings in which the clinical innovation will be used).

Upper Panel:

However, as the story above illustrates, even concerted effort at the effectiveness end of the trial design spectrum is not enough to guarantee routine usage of a clinical innovation. Moreover, education and monitoring are not sufficient to change provider behavior, as indicated by a meta-analysis by the Cochrane Collaboration which showed that audit and feedback only increased target provider behaviors by four point three percent. Thus, simply using traditional clinical trials approaches and pushing them further into real-world conditions is not sufficient to guarantee public health impact. Factors contributing to the non-adoption of evidence-based clinical innovations must be addressed directly through promising methods and strategies that can be subjected to rigorous investigation in the emerging field of implementation science.


## Three. What is implementation science?

Three point one. Some history

While researchers, clinicians, and healthcare leaders have been at least partially and intermittently aware of the problem of non-adoption of evidence-based clinical innovations, the area began to coalesce as a sustained field of study with the publication of Everett Rogers' Diffusion of Innovations. Drawing on examples from agriculture and industry as well as health, Rogers conceptualized the spread, or diffusion, of innovations as a social process with multiple determinants well beyond the evidence supporting the innovation itself. Concurrently, others also called attention to contextual aspects of innovation adoption or non-adoption, including the field of knowledge utilization, which addresses the tenuous link of evidence to policy decisions; and the field of technology transfer, which investigates reasons that knowledge and technology do not flow freely even within single organizations. These areas of study, and others, have provided a collective foundation upon which the field of implementation science emerged.


## Three point two. Defining implementation science

Implementation science has been defined in several ways that are consistent with the definition put forward in the inaugural issue of the journal, Implementation Science: "the scientific study of methods to promote the systematic uptake of research findings and other evidence-based practice into routine practice and, hence, to improve the quality and effectiveness of health services." Hence the goal of implementation science is not to establish the health impact of a clinical innovation, but rather to identify the factors that affect its uptake into routine use.

As outlined above, ample experience indicates that factors which affect the uptake of a clinical innovation differ from those factors that determine whether an innovation works. The crux of implementation science is thus two-fold:

Identify uptake barriers and facilitators across multiple levels of context (individuals in treatment, providers, organization, and other stakeholder groups), and

Develop and apply implementation strategies that overcome these barriers and enhance the facilitators to increase the uptake of evidence-based clinical innovations.

In its focus on changing individual and social unit behavior, implementation science is close kin to the wide spectrum of fields ranging, for example, from health behavior change to organizational psychology.


## Four. How implementation science differs from clinical research, quality improvement, and dissemination

Four point one. Clinical research versus implementation research

Perhaps the major difference between clinical research and implementation research is that the latter actively engages with the context into which a clinical innovation is introduced, rather than controlling, efficacy studies, or simply tolerating, effectiveness studies, that context. Consider a hypothetical trial of intranasal ketamine for depression.

Implementation trial designs differ from efficacy and effectiveness clinical trials at a fundamental level, that of the hypothesis. While clinical trials focus on contrasting the health effects of an innovation with a comparison group, implementation trials focus on testing strategies to increase uptake and sustainability of the innovation, in this example intranasal ketamine. Efficacy and effectiveness trials seek to evaluate a clinical innovation, a clinical treatment, test, organization of care, or other initiative to be introduced into a healthcare venue. In contrast, implementation trials seek to evaluate a strategy for increasing uptake of the evidence-based innovation into routine practice, typically an integrated set, bundle, or package of implementation innovations to enhance the usage of the clinical innovation.

While the subject population and setting in an implementation trial may resemble those used in effectiveness trials, alternatively the unit of observation in implementation trials may also be providers or clinics or even entire organizations depending on the focus of the implementation effort. Further, as the population becomes more heterogeneous in moving from efficacy to effectiveness designs, outcome measures must become briefer and simpler in order to minimize respondent burden and retain protocol subjects who are less research-tolerant; this is true in implementation trials as well. As one moves from left to right in Table One, intervention clinicians become progressively less highly specialized and skill development efforts more closely resemble training that would typically be feasible under general conditions; measurement of fidelity to the intervention varies similarly.

Consideration of the context for the research also differs substantively across the efficacy-effectiveness-implementation spectrum. Efficacy trials value conducting a trial that is invariant from beginning to end. This entails sometimes extraordinary efforts at training clinicians and tracking subjects, so that the research team's efforts amount at times to "crypto-case management" (i.e., aggressive subject engagement and follow-up outreach) to ensure close follow-up and minimal data loss-which is not a problem since the focus of the trial is solely on testing the intervention under optimal conditions. However, since effectiveness studies are more concerned with the performance of an efficacious intervention in real-world settings, careful attention is paid to limiting the amount of research-funded effort that is incorporated into the protocol, often "firewalling" research effort with carefully and explicitly defined parameters. Further, for implementation trials, trying to optimize the natural context in which uptake is being measured is a threat to the validity of the trial. Thus, researcher involvement at the sites is drastically circumscribed, with some implementation trials even limiting research support to training endogenous clinicians and utilizing remote, "light-touch" outcome assessments for patient and provider subjects.

These differences across the efficacy-effectiveness-implementation spectrum are reflected in radically different priorities regarding trial validity. As noted above, in efficacy studies internal validity prioritizes establishing a causal connection between the intervention and the outcome; in the service of this aim, the sample, outcome measurements, and intervention are all highly controlled without consideration of applicability beyond the specific trial itself. In effectiveness studies, external validity prioritizes the generalizability of trial results to other relevant populations and situations; accordingly, although the effectiveness trial must have sufficient internal validity to carry out the study successfully, explicit design decisions are made that value generalizability beyond the trial itself. Finally, implementation trials, in contrast to both types of clinical trials, aim at arriving at the optimal implementation strategy, and maximal innovation impact, by the end of the study. Note, however, that in subtle ways such "mid-course corrections" are the case for all types of trials, including especially efficacy trials, in which intervention fidelity data are assiduously collected throughout the study and adjustments in training and even personnel are made to ensure internal validity. The difference between clinical trials and implementation trials is that in the latter such modifications are planned for a priori and are often the focus of specific study hypotheses, under the rubric of "formative evaluation."

Thus, in overly simplistic terms, the clinical researcher establishes the efficacy and then effectiveness of an innovation, and then hands it off to the implementation scientist to test ways of getting people to use it. This process as stated is overly simplistic because it suggests a unidimensional flow of tasks (like our pipeline in Fig. one). In reality, the process is much more iterative as implementation experience may suggest changes in the clinical innovation to increase its external validity (while also taking steps to ensure fidelity to its core components, supporting internal validity).


## Four point two. Implementation science versus quality improvement and dissemination research

Implementation science also differs from quality improvement, which usually begins with a specific problem rather than a practice to promulgate. It typically focuses on an individual clinic, hospital, or system, and tends not to seek to contribute generalizable knowledge. It also differs from traditional dissemination research, which usually focuses on the spread of information using communication and education strategies. However, the purpose and methods of these three fields often overlap, as you will see in the articles that follow in this issue.


## Four point three. The scope of implementation science

Thus, not surprisingly, implementation science involves investigation at more levels than just the individual patient or subject. Targets of investigation may also include the provider, clinic, facility, organization, community, and even the policy environment.

Thus, also not surprisingly given the central role of context in implementation science, implementation researchers include not just clinicians and clinical scientists but also social scientists, economists, systems engineers and health services researchers. Additionally, special mention should be made of the relationship between implementation researchers and their "operational partners" with whom a clinical innovation is to be deployed: healthcare system leaders, administrators, and staff who run and staff the clinics, facilities, and organizations (and sometimes also including policymakers). Unlike clinical research, in which such individuals and structures play a primarily permissive or passively supportive role, in implementation research operational colleagues need to be full partners in the research from study design through analysis, since implementation research aims to assess and actively intervene in their structures, where they are the experts. While there are cultural gaps between researchers and healthcare system leaders and staff that must be overcome, their participation is essential since an innovation will be implemented because of them, not in spite of them.


## Four point four. Summary

In summary, implementation science seeks to "continue the job" of biomedical research, taking evidence-based clinical innovations and testing strategies to move them into wider practice. Given this goal, the principles and methods of implementation science differ somewhat from clinical research. Most centrally, implementation science protocols do not ignore or control for context but rather actively seek to intervene to change the context in which clinical innovations are used in order to enhance their uptake. Importantly, implementation scientists interface closely with healthcare leaders and staff as partners, breaking down the research-practice divide in order to achieve the ultimate goal of increasing the public health impact of evidence-based innovations.

A number of exciting challenges face the rapidly developing field of implementation science. First and foremost, further integration of implementation science methodologies into clinical trial designs will speed the development of evidence-based interventions that have demonstrable public health impact.

In addition, development of efficient methodologies for data collection with minimal disruption to the processes under study will be important in order to minimize the Hawthorne effect in both descriptive studies and controlled trials. Testing and validating the theories that underlie implementation efforts is needed to enhance the development of next-generation analytic methods and interventions. Finally, the development of an investigator workforce with diverse and complementary skills, and provision of sustainable academic careers for them, remains a central challenge to the field.


## Five. Conclusion: overview of this special issue

This Special Issue is designed to serve as an introduction to the principles, concepts, and methods of implementation science. The articles have been written particularly for clinical scientists interested in how to increase the public health impact of their work.

The Special issue provides articles on topics in two general areas, the first is those that are foundational to implementation science such as the conceptual background and specific methodologies that are core to this field of study. The second area are a set of articles that address special and emerging topics within implementation science including an expansion of case study analysis through a matrix process, the application of implementation science in global settings and capacity-building and training opportunities. Each of the manuscripts includes a review of underlying principles, introduction to relevant methods, and an example from a real-world, funded study that illustrates and amplifies the principles and methods discussed.


## Foundational topics:

Ms. Damschroder discusses the role of theories, frameworks, and models in implementation science, which are critical in identifying which, among all the "moving parts" of healthcare, are relevant to specific implementation efforts.

Dr. Kirchner and colleagues describe implementation strategies utilized to address barriers to and facilitators of clinical innovation uptake, which are the focus of implementation trials.

Dr. Smith and colleagues review quantitative evaluation methods for implementation studies, which span the multiple levels of relevant context, well beyond the subject-level focus of clinical trials research.

Dr. Hamilton and colleagues introduce qualitative methods, which do not regularly play a role in clinical research, yet are a key complement to quantitative methods in analyzing implementation efforts.

Dr. Elwy and colleagues introduce a novel aspect of implementation research methodology: formative evaluation, which seeks to integrate data from implementation trials in real-time to adjust implementation and intervention techniques in order to optimize outcome.

Dr. Miller and colleagues provide an overview of implementation trial designs, including their similarities and differences from clinical trials.

Dr. Landes and colleagues review hybrid effectiveness-implementation methods, novel designs that address both clinical impacts and implementation outcomes in the same protocol.


## Special and emerging topics:

Dr. Kim and colleagues introduce structured case study methodology, a methodology to go "beyond the mean" in results and learn lessons from variability in implementation in order to identify what works and for whom.

Drs. Kilbourne, Eisman, and colleagues describe economic analyses as applied to implementation studies, highlighting both the similarities and the differences between implementation science and clinical research approaches to this critical area.

Dr. Mckay and colleagues address considerations in moving implementation research from the developed world into more comprehensive global contexts.

Finally, Dr. Chambers and colleagues summarize the important issue of capacity-building and training opportunities in implementation science.


## Funding

This work was supported by Department of Veterans Affairs grants QUE fifteen-two eighty-nine and CIN thirteen-four zero three.


## Supplementary materials

Supplementary material associated with this article can be found, in the online version, at doi: ten point one zero one six/j.psychres.twenty nineteen point zero four point zero two five.