{
  "id": "chatcmpl-A3U6IRxyw0anX9kks0fqmsTYIO2F6",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "j79f | Modelling the scaling behavior. Understanding the scaling behaviour of language models and their transfer properties has been important in the development of recent large models. Kaplan et al. first showed a predictable relationship between model size and loss over many orders of magnitude. The authors investigate the question of choosing the optimal model size to train for a given compute budget. Similar to us, they address this question by training various models. Our work differs from Kaplan et al. in several important ways. First, the authors use a fixed number of training tokens and learning rate schedule for all models; this prevents them from modelling the impact of these hyperparameters on the loss. In contrast, we find that setting the learning rate schedule to approximately match the number of training tokens results in the best final loss regardless of model size-see Figure A1. For a fixed learning rate cosine schedule to one hundred thirty billion tokens, the intermediate loss estimates (for P less than one hundred thirty billion) are therefore overestimates of the loss of a model trained with a schedule length matching P. Using these intermediate losses results in underestimating the effectiveness of training models on less data than one hundred thirty billion tokens, and eventually contributes to the conclusion that model size should increase faster than training data size as compute budget increases. In contrast, our analysis predicts that both quantities should scale at roughly the same rate. Secondly, we include models with up to sixteen billion parameters, as we observe that there is slight curvature in the FLOP-loss frontier (see Appendix E)-in fact, the majority of the models used in our analysis have more than five hundred million parameters, in contrast the majority of runs in Kaplan et al. are significantly smaller-many being less than one hundred million parameters.\n6tbv | Recently, Clark et al. specifically looked in to the scaling properties of Mixture of Expert\nxb13 | Estimating hyperparameters for large models. The model size and the number of training tokens are not the only two parameters to choose when selecting a language model and a procedure to train it. Other important factors include learning rate, learning rate schedule, batch size, optimiser, and width-to-depth ratio. In this work, we focus on model size and the number of training steps, and we rely on existing work and provided experimental heuristics to determine the other necessary hyperparameters. Yang et al. investigates how to choose a variety of these parameters for training an autoregressive transformer, including the learning rate and batch size. McCandlish et al. finds only a weak dependence between optimal batch size and model size. Shallue et al.; Zhang et al. suggest that using larger batch-sizes than those we use is possible. Levine et al. investigates the optimal depth-to-width ratio for a variety of standard model sizes. We use slightly less deep models than proposed as this translates to better wall-clock performance on our hardware.\ncxqi | Improved model architectures. Recently, various promising alternatives to traditional dense transformers have been proposed. For example, through the use of conditional computation large MoE models like the one point seven trillion parameter Switch transformer, the one point two trillion parameter GLaM model, and others are able to provide a large effective model size despite using relatively fewer training and inference FLOPs. However, for very large models the computational benefits of routed models seems to diminish. An orthogonal approach to improving language models is to augment transformers with explicit retrieval mechanisms, as done by Borgeaud et al.; Guu et al.; Lewis et al. This approach effectively increases the number of data tokens seen during training (by a factor of approximately ten in Borgeaud et al.). This suggests that the performance of language models may be more dependent on the size of the training data than previously thought.\nqfmx | Three. Estimating the optimal parameter/training tokens allocation\nrut3 | We present three different approaches to answer the question driving our research: Given a fixed FLOPs budget, how should one trade-off model size and the number of training tokens? In all three cases we start by training a range of models varying both model size and the number of training tokens and use the resulting training curves to fit an empirical estimator of how they should scale. We assume a power-law relationship between compute and model size as done in Clark et al.; Kaplan et al., though future work may want to include potential curvature in this relationship for large model sizes. The resulting predictions are similar for all three methods and suggest that parameter count and number of training tokens should be increased equally with more compute—with proportions reported in Table Two. This is in clear contrast to previous work on this topic and warrants further investigation.\nm3cp | Three point one. Approach one: Fix model sizes and vary number of training tokens\nmq4s | In our first approach we vary the number of training steps for a fixed family of models (ranging from seventy million to over ten billion parameters), training each model for four different number of training sequences. From these runs, we are able to directly extract an estimate of the minimum loss achieved for a given number of training FLOPs. Training details for this approach can be found in Appendix D.\n95el | For each parameter count N we train four different models, decaying the learning rate by a factor of ten X over a horizon (measured in number of training tokens) that ranges by a factor of sixteen X. Then, for each run, we smooth and then interpolate the training loss curve. From this, we obtain a continuous mapping from FLOP count to training loss for each run. Then, for each FLOP count, we determine which run achieves the lowest loss. Using these interpolants, we obtain a mapping from any FLOP count C, to the most efficient choice of model size N and number of training tokens D such that FLOPs of N D equals C. At one thousand five hundred logarithmically spaced FLOP values, we find which model size achieves the lowest loss of all models along with the required number of training tokens. Finally, we fit power laws to estimate the optimal model size and number of training tokens for any given amount of compute (see the center and right panels of Figure Two), obtaining a relationship N-opt proportional to C to the power of A and D-opt of C to the power of B. We find that A equals zero point fifty and B equals zero point fifty—as summarized in Table Two. In Section D point four, we show a head-to-head comparison at ten to the twenty-one FLOPs, using the model size recommended by our analysis and by the analysis of Kaplan et al.—using the model size we predict has a clear advantage.",
        "role": "assistant",
        "function_call": null,
        "tool_calls": null,
        "refusal": null
      }
    }
  ],
  "created": 1725394054,
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_fde2829a40",
  "usage": {
    "completion_tokens": 1338,
    "prompt_tokens": 3188,
    "total_tokens": 4526
  }
}